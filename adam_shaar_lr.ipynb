{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va6zv-gKhIu8"
      },
      "source": [
        "Name: Adam Shaar"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tuaYthb0hXnU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submission:\n",
        "\n",
        "1- Run all cells (this is important, the results will remain there for us to look)\n",
        "\n",
        "2- Download .ipynb\n",
        "\n",
        "3- Download .py\n",
        "\n",
        "4- Use Save as or Print to create a PDF version of the notebook  \n",
        "\n",
        "5- Create a directory named: firstname_lastname_lr (e.g. pedram_rooshenas_lr)\n",
        "\n",
        "6- Put all three .ipynb, .py, and .pdf into the directory. (*** Don't forget the PDF and .py ***) \n",
        "\n",
        "7- Zip (don't use rar) and Submit on Gradescope"
      ],
      "metadata": {
        "id": "QZj93Jcmh8pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting Google Drive:\n",
        "#After running this cell a popup window will appear and requesting to select your  Google account and give the access permission.\n",
        "#You can either use your personal Google account or your UIC Google account.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "jkGXbH9kiQEK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3e51682b-f734-44df-83fa-22a5240ed523"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/gdrive/MyDrive/mlcourse/hw2/\""
      ],
      "metadata": {
        "id": "K5ljlocpiRxq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qB86CgDBhIu9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('ggplot')\n",
        "%matplotlib inline\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhFQTtLWhIu-"
      },
      "source": [
        "Numpy is library for scientific computing in Python. It has efficient implementation of n-dimensional array (tensor) manupulations, which is useful for machine learning applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RZceSJk8hIu-"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLJ1LE0RhIu_"
      },
      "source": [
        "We can convert a list into numpy array (tensor)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UIBuEViJhIu_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8f4f9074-8cba-41ef-921f-414b3b504d47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 4],\n",
              "       [2, 6, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "b = [[1, 2, 4], [2, 6, 9]]\n",
        "a = np.array(b)\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpaBPUPEhIu_"
      },
      "source": [
        "We can check the dimensions of the array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fUrF-jvPhIu_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "27cc0da6-65a1-4d54-8024-4a9e4cf88ba8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "a.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjUJI4vRhIu_"
      },
      "source": [
        "We can apply simple arithmetic operation on all element of a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VlQlxcGzhIu_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "94b5531c-4fa0-4995-b366-f87296c2588b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  6, 12],\n",
              "       [ 6, 18, 27]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "a * 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHyhHZbghIvA"
      },
      "source": [
        "You can transpose a tensor\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "88ItMKO8hIvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1c7b13c4-6774-4def-fdf4-41d1db8950f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [2, 6],\n",
              "       [4, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "print(a.T.shape)\n",
        "a.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5TsmIfZhIvA"
      },
      "source": [
        "You can apply aggregate functions on the whole tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0rHaLkUMhIvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d571c4e1-8323-45a8-fe0c-f466e3acd7ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "np.sum(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYoIqupihIvA"
      },
      "source": [
        "or on one dimension of it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2t74tXHOhIvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eba35deb-ce0b-4343-afce-df63385b12c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3,  8, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "np.sum(a, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9pLDwLh8hIvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b363c8ae-2c0b-412c-efc3-8befc1cbcf46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "np.sum(a, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQUIziphIvA"
      },
      "source": [
        "We can do element-wise arithmetic operation on two tensors (of the same size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hLcwq4nZhIvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "424ca758-a9ec-4e9d-f779-3353be536842"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2,  6, 20],\n",
              "       [ 2, 12,  9]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "c1 = np.array([[1, 2, 4], [2, 6, 9]])\n",
        "c2 = np.array([[2, 3, 5], [1, 2, 1]])\n",
        "c1 * c2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTbsvWJxhIvA"
      },
      "source": [
        "If you want to multiply all columns of a tensor by vector (for example if you want to multiply all data features by their lables) you need a trick. This multiplication shows up in calculating the gradients. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VDEbp8nzhIvB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5387a634-1477-4bbb-b47c-c7b351f25e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 4]\n",
            " [2 6 9]]\n",
            "[ 1 -1]\n"
          ]
        }
      ],
      "source": [
        "a = np.array([[1, 2, 4], [2, 6, 9]])\n",
        "b = np.array([1,-1])\n",
        "print(a)\n",
        "print(b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHqynmerhIvB"
      },
      "source": [
        "Here we want to multiply the first row of a by 1 and the second row of a by -1. Simply multiplying a by b does not work because a and b do not have the same dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMALphCUhIvB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "eb592046-36e2-4bbc-cfa1-59f4ab6aae63"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-9bc1a869709f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,3) (2,) "
          ]
        }
      ],
      "source": [
        "a * b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qkH4wCRhIvB"
      },
      "source": [
        "To do this multiplication we first have to assume b has one column and then repeat the column of b with the number of columns in a. We use tile function to do that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BK6SI4OqhIvB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8083c9ea-ffb1-4a22-90f2-8cbc211f3fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  1,  1],\n",
              "       [-1, -1, -1]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "b_repeat = np.tile(b,  (a.shape[1],1)).T\n",
        "print(b_repeat.shape)\n",
        "b_repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-37pUmo_hIvB"
      },
      "source": [
        "Now we can multiply each column of a by b:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Dj57Pxv1hIvB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e4cb46e1-88cb-4bd7-9d2c-d4a7a8e6ccdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  4],\n",
              "       [-2, -6, -9]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "a * b_repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njjOZYvHhIvB"
      },
      "source": [
        "You can create inital random vector using numpy (using N(0,1)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x8O7pRx5hIvB"
      },
      "outputs": [],
      "source": [
        "mu = 0 #mean\n",
        "sigma = 1 #standard deviation\n",
        "r = np.random.normal(mu,sigma, 1000) #draws 1000 samples from a normal distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6kyKWMnhIvB"
      },
      "source": [
        "We can apply functions on tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "13oMFM34hIvC"
      },
      "outputs": [],
      "source": [
        "#implementation of Normal distribution\n",
        "def normal(x, mu, sigma):\n",
        "    return np.exp( -0.5 * ((x-mu)/sigma)**2)/np.sqrt(2.0*np.pi*sigma**2)\n",
        "\n",
        "#probability of samples on the Normal distribution\n",
        "probabilities = normal(r, mu, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hby6OxM2hIvC"
      },
      "source": [
        "Numpy has useful APIs for analysis. Here we plot the histogram of samples and also plot the probabilies to see if the samples follow the normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ort_wanOhIvC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "49a24406-dc1d-4d17-ce3b-673d61399fb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fd9eda2d160>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "counts, bins = np.histogram(r,50,density=True)\n",
        "plt.hist(bins[:-1], bins, weights=counts)\n",
        "plt.scatter(r, probabilities, c='b', marker='.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1ixKIArxhIvC"
      },
      "outputs": [],
      "source": [
        "def read_data(filename):\n",
        "    f = open(filename, 'r')\n",
        "    p = re.compile(',')\n",
        "    xdata = []\n",
        "    ydata = []\n",
        "    header = f.readline().strip()\n",
        "    varnames = p.split(header)\n",
        "    namehash = {}\n",
        "    for l in f:\n",
        "        li = p.split(l.strip())\n",
        "        xdata.append([float(x) for x in li[:-1]])\n",
        "        ydata.append(float(li[-1]))\n",
        "    \n",
        "    return np.array(xdata), np.array(ydata)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES-3FSlmhIvC"
      },
      "source": [
        "Assuming our data is x is available in numpy we use numpy to implement logistic regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SN454nr0hIvC"
      },
      "outputs": [],
      "source": [
        "(xtrain_whole, ytrain_whole) = read_data(path + 'spambase-train.csv')\n",
        "(xtest, ytest) = read_data(path + 'spambase-test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ciUpv-cGhIvC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "aae3f8bd-bb96-4319-80dd-fce3d42eff4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of xtrain: (3601, 54)\n",
            "The shape of ytrain: (3601,)\n",
            "The shape of xtest: (1000, 54)\n",
            "The shape of ytest: (1000,)\n"
          ]
        }
      ],
      "source": [
        "print(\"The shape of xtrain:\", xtrain_whole.shape)\n",
        "print(\"The shape of ytrain:\", ytrain_whole.shape)\n",
        "print(\"The shape of xtest:\", xtest.shape)\n",
        "print(\"The shape of ytest:\", ytest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW3mxkkahIvC"
      },
      "source": [
        "before training make we normalize the input data (features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "whv7x-gthIvD"
      },
      "outputs": [],
      "source": [
        "xmean = np.mean(xtrain_whole, axis=0)\n",
        "xstd = np.std(xtrain_whole, axis=0)\n",
        "xtrain_normal_whole = (xtrain_whole-xmean) / xstd\n",
        "xtest_normal = (xtest-xmean) / xstd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu6SLWKFhIvD"
      },
      "source": [
        "We need to create a validation set. We create an array of indecies and permute it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "QcTRxtqohIvD"
      },
      "outputs": [],
      "source": [
        "premute_indicies = np.random.permutation(np.arange(xtrain_whole.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rols6OsxhIvD"
      },
      "source": [
        "We keep the first 2600 data points as the training data and rest as the validation data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DP87ZxqhhIvD"
      },
      "outputs": [],
      "source": [
        "xtrain_normal = xtrain_normal_whole[premute_indicies[:2600]]\n",
        "ytrain = ytrain_whole[premute_indicies[:2600]]\n",
        "xval_normal = xtrain_normal_whole[premute_indicies[2600:]]\n",
        "yval = ytrain_whole[premute_indicies[2600:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcY4MTtKhIvD"
      },
      "source": [
        "Initiallizing the weights and bias with random values from N(0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GSZd1VgGhIvD"
      },
      "outputs": [],
      "source": [
        "weights = np.random.normal(0, 1, xtrain_normal.shape[1]);\n",
        "bias = np.random.normal(0,1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2wT420TChIvD"
      },
      "outputs": [],
      "source": [
        "#the sigmoid function\n",
        "def sigmoid(v):\n",
        "    #return np.exp(-np.logaddexp(0, -v)) #numerically stable implementation of sigmoid function \n",
        "    return 1.0 / (1+np.exp(-v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe6TnxiBhIvD"
      },
      "source": [
        "We can use dot-product from numpy to calculate the margin and pass it to the sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YzLLEHfihIvD"
      },
      "outputs": [],
      "source": [
        "#w: weight vector (numpy array of size n)\n",
        "#b: numpy array of size 1\n",
        "#returns p(y=1|x, w, b)\n",
        "def prob(x, w, b):\n",
        "    return sigmoid(np.dot(x,w) + b);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HJuk25jhIvD"
      },
      "source": [
        "You can also calculate $l_2$ penalty using linalg library of numpy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TC70IRw9hIvD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6109cf08-378a-43f8-d0aa-75b349c5d1e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.22076369886441"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "np.linalg.norm(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVsllOx5hIvD"
      },
      "source": [
        "$$\\text{Cross Entropy Loss} = -\\frac{1}{|D|}[\\sum_{(y^i,\\mathbf{x}^i)\\in\\mathcal{D}} \n",
        " y^i \\log p(y=1|\\mathbf{x}^i;\\mathbf{w},b)  +  (1-y^i) \\log (1 - p(y=1|\\mathbf{x}^i;\\mathbf{w},b))]+\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "iI64I76vhIvE"
      },
      "outputs": [],
      "source": [
        "#w: weight vector (numpy array of size n)\n",
        "#x: training data points (only attributes)\n",
        "#y_prob: p(y|x, w, b)\n",
        "#y_true: class variable data\n",
        "#lambda_: l2 penalty coefficient\n",
        "#returns the cross entropy loss\n",
        "def loss(w, x, y_prob, y_true, lambda_):\n",
        "    return ((-1 / len(x)) * np.sum(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))) + (lambda_ / 2) * np.sum(np.square(w))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "hN3S3kGnhIvE"
      },
      "outputs": [],
      "source": [
        "#x: input variables (data of size m x n with m data point and n features)\n",
        "#w: weight vector (numpy array of size n)\n",
        "#y_prob: p(y|x, w, b)\n",
        "#y_true: class variable data\n",
        "#lambda_: l2 penalty coefficient\n",
        "#returns tuple of gradient w.r.t w and w.r.t to bias\n",
        "\n",
        "def grad_w_b(x, w, y_prob, y_true, lambda_):\n",
        "    grad_w = ((1/ x.shape[1]) * np.dot(x.T, y_prob - y_true)) + lambda_ * w\n",
        "    grad_b = (1 / len(y_true)) * np.sum(y_prob - y_true)\n",
        "    return (grad_w,grad_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "scrolled": false,
        "id": "4tGaJCDjhIvE"
      },
      "outputs": [],
      "source": [
        "\n",
        "#lambda_ is the coeffienct of l2 norm penalty\n",
        "#learning_rate is learning rate of gradient descent algorithm\n",
        "#max_iter determines the maximum number of iterations if the gradients descent does not converge.\n",
        "#continue the training while gradient > 0.1 or the number steps is less max_iter\n",
        "\n",
        "#returns model as tuple of (weights,bias)\n",
        "\n",
        "def fit(x, y_true, learning_rate, lambda_, max_iter, verbose=0):\n",
        "    weights = np.random.normal(0, 1, x.shape[1]);\n",
        "    bias = np.random.normal(0,1,1)\n",
        "    iter = 0\n",
        "    grad_w_norm = 1\n",
        "    #change the condition appropriately\n",
        "    while iter < max_iter and grad_w_norm > 0.1:\n",
        "        y_prob = prob(x, weights, bias)\n",
        "        grad_w, grad_b = grad_w_b(x, weights, y_prob, y_true, lambda_)\n",
        "        weights -= learning_rate * grad_w\n",
        "        bias -= learning_rate * grad_b\n",
        "        weights_norm = np.linalg.norm(weights)\n",
        "        grad_w_norm = np.linalg.norm(grad_w)\n",
        "        if verbose: #verbose is used for debugging purposes\n",
        "            loss_ = loss(weights, x, y_prob, y_true, lambda_)\n",
        "            print(f\"Iteration Number: {iter}, Loss: {loss_:5f}, 12 norm of gradients: {grad_w_norm:5f}, 12 norm of weights: {weights_norm:5f}\")\n",
        "            #print iteration number, loss, l2 norm of gradients, l2 norm of weights\n",
        "        iter += 1\n",
        "    return (weights, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "55TdlFeyhIvE"
      },
      "outputs": [],
      "source": [
        "def accuracy(x, y_true, model):\n",
        "    w, b = model\n",
        "    return np.sum((prob(x, w, b)>0.5).astype(np.float64) == y_true)  / y_true.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "DZlAxkr2hIvE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e9d6da8d-2154-4abc-ace9-a54426aa0047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration Number: 0, Loss: 27.412035, 12 norm of gradients: 57.510002, 12 norm of weights: 7.080815\n",
            "Iteration Number: 1, Loss: 27.201479, 12 norm of gradients: 56.078503, 12 norm of weights: 7.060199\n",
            "Iteration Number: 2, Loss: 26.997491, 12 norm of gradients: 54.638096, 12 norm of weights: 7.039988\n",
            "Iteration Number: 3, Loss: 26.799813, 12 norm of gradients: 53.213519, 12 norm of weights: 7.020160\n",
            "Iteration Number: 4, Loss: 26.608141, 12 norm of gradients: 51.827970, 12 norm of weights: 7.000692\n",
            "Iteration Number: 5, Loss: 26.422140, 12 norm of gradients: 50.498909, 12 norm of weights: 6.981566\n",
            "Iteration Number: 6, Loss: 26.241466, 12 norm of gradients: 49.236412, 12 norm of weights: 6.962762\n",
            "Iteration Number: 7, Loss: 26.065780, 12 norm of gradients: 48.043316, 12 norm of weights: 6.944264\n",
            "Iteration Number: 8, Loss: 25.894766, 12 norm of gradients: 46.916753, 12 norm of weights: 6.926058\n",
            "Iteration Number: 9, Loss: 25.728136, 12 norm of gradients: 45.850506, 12 norm of weights: 6.908130\n",
            "Iteration Number: 10, Loss: 25.565634, 12 norm of gradients: 44.837191, 12 norm of weights: 6.890468\n",
            "Iteration Number: 11, Loss: 25.407031, 12 norm of gradients: 43.869643, 12 norm of weights: 6.873062\n",
            "Iteration Number: 12, Loss: 25.252126, 12 norm of gradients: 42.941496, 12 norm of weights: 6.855902\n",
            "Iteration Number: 13, Loss: 25.100736, 12 norm of gradients: 42.047315, 12 norm of weights: 6.838977\n",
            "Iteration Number: 14, Loss: 24.952697, 12 norm of gradients: 41.182625, 12 norm of weights: 6.822279\n",
            "Iteration Number: 15, Loss: 24.807861, 12 norm of gradients: 40.343993, 12 norm of weights: 6.805800\n",
            "Iteration Number: 16, Loss: 24.666089, 12 norm of gradients: 39.529140, 12 norm of weights: 6.789530\n",
            "Iteration Number: 17, Loss: 24.527251, 12 norm of gradients: 38.737006, 12 norm of weights: 6.773463\n",
            "Iteration Number: 18, Loss: 24.391224, 12 norm of gradients: 37.967642, 12 norm of weights: 6.757590\n",
            "Iteration Number: 19, Loss: 24.257887, 12 norm of gradients: 37.221895, 12 norm of weights: 6.741904\n",
            "Iteration Number: 20, Loss: 24.127125, 12 norm of gradients: 36.500950, 12 norm of weights: 6.726398\n",
            "Iteration Number: 21, Loss: 23.998826, 12 norm of gradients: 35.805867, 12 norm of weights: 6.711065\n",
            "Iteration Number: 22, Loss: 23.872879, 12 norm of gradients: 35.137265, 12 norm of weights: 6.695900\n",
            "Iteration Number: 23, Loss: 23.749180, 12 norm of gradients: 34.495191, 12 norm of weights: 6.680895\n",
            "Iteration Number: 24, Loss: 23.627630, 12 norm of gradients: 33.879151, 12 norm of weights: 6.666045\n",
            "Iteration Number: 25, Loss: 23.508135, 12 norm of gradients: 33.288223, 12 norm of weights: 6.651346\n",
            "Iteration Number: 26, Loss: 23.390607, 12 norm of gradients: 32.721186, 12 norm of weights: 6.636791\n",
            "Iteration Number: 27, Loss: 23.274964, 12 norm of gradients: 32.176631, 12 norm of weights: 6.622377\n",
            "Iteration Number: 28, Loss: 23.161129, 12 norm of gradients: 31.653053, 12 norm of weights: 6.608100\n",
            "Iteration Number: 29, Loss: 23.049030, 12 norm of gradients: 31.148923, 12 norm of weights: 6.593954\n",
            "Iteration Number: 30, Loss: 22.938603, 12 norm of gradients: 30.662747, 12 norm of weights: 6.579937\n",
            "Iteration Number: 31, Loss: 22.829783, 12 norm of gradients: 30.193121, 12 norm of weights: 6.566044\n",
            "Iteration Number: 32, Loss: 22.722514, 12 norm of gradients: 29.738780, 12 norm of weights: 6.552271\n",
            "Iteration Number: 33, Loss: 22.616742, 12 norm of gradients: 29.298624, 12 norm of weights: 6.538617\n",
            "Iteration Number: 34, Loss: 22.512414, 12 norm of gradients: 28.871736, 12 norm of weights: 6.525076\n",
            "Iteration Number: 35, Loss: 22.409483, 12 norm of gradients: 28.457382, 12 norm of weights: 6.511646\n",
            "Iteration Number: 36, Loss: 22.307902, 12 norm of gradients: 28.054993, 12 norm of weights: 6.498324\n",
            "Iteration Number: 37, Loss: 22.207628, 12 norm of gradients: 27.664131, 12 norm of weights: 6.485106\n",
            "Iteration Number: 38, Loss: 22.108617, 12 norm of gradients: 27.284459, 12 norm of weights: 6.471991\n",
            "Iteration Number: 39, Loss: 22.010830, 12 norm of gradients: 26.915699, 12 norm of weights: 6.458974\n",
            "Iteration Number: 40, Loss: 21.914229, 12 norm of gradients: 26.557604, 12 norm of weights: 6.446054\n",
            "Iteration Number: 41, Loss: 21.818774, 12 norm of gradients: 26.209933, 12 norm of weights: 6.433228\n",
            "Iteration Number: 42, Loss: 21.724432, 12 norm of gradients: 25.872433, 12 norm of weights: 6.420492\n",
            "Iteration Number: 43, Loss: 21.631167, 12 norm of gradients: 25.544833, 12 norm of weights: 6.407846\n",
            "Iteration Number: 44, Loss: 21.538947, 12 norm of gradients: 25.226841, 12 norm of weights: 6.395286\n",
            "Iteration Number: 45, Loss: 21.447741, 12 norm of gradients: 24.918150, 12 norm of weights: 6.382811\n",
            "Iteration Number: 46, Loss: 21.357518, 12 norm of gradients: 24.618439, 12 norm of weights: 6.370418\n",
            "Iteration Number: 47, Loss: 21.268250, 12 norm of gradients: 24.327380, 12 norm of weights: 6.358105\n",
            "Iteration Number: 48, Loss: 21.179910, 12 norm of gradients: 24.044649, 12 norm of weights: 6.345870\n",
            "Iteration Number: 49, Loss: 21.092472, 12 norm of gradients: 23.769924, 12 norm of weights: 6.333711\n",
            "Iteration Number: 50, Loss: 21.005910, 12 norm of gradients: 23.502894, 12 norm of weights: 6.321628\n",
            "Iteration Number: 51, Loss: 20.920201, 12 norm of gradients: 23.243263, 12 norm of weights: 6.309617\n",
            "Iteration Number: 52, Loss: 20.835323, 12 norm of gradients: 22.990744, 12 norm of weights: 6.297677\n",
            "Iteration Number: 53, Loss: 20.751253, 12 norm of gradients: 22.745071, 12 norm of weights: 6.285807\n",
            "Iteration Number: 54, Loss: 20.667970, 12 norm of gradients: 22.505990, 12 norm of weights: 6.274005\n",
            "Iteration Number: 55, Loss: 20.585456, 12 norm of gradients: 22.273263, 12 norm of weights: 6.262269\n",
            "Iteration Number: 56, Loss: 20.503689, 12 norm of gradients: 22.046667, 12 norm of weights: 6.250599\n",
            "Iteration Number: 57, Loss: 20.422653, 12 norm of gradients: 21.825989, 12 norm of weights: 6.238992\n",
            "Iteration Number: 58, Loss: 20.342329, 12 norm of gradients: 21.611032, 12 norm of weights: 6.227448\n",
            "Iteration Number: 59, Loss: 20.262700, 12 norm of gradients: 21.401606, 12 norm of weights: 6.215965\n",
            "Iteration Number: 60, Loss: 20.183751, 12 norm of gradients: 21.197533, 12 norm of weights: 6.204543\n",
            "Iteration Number: 61, Loss: 20.105466, 12 norm of gradients: 20.998640, 12 norm of weights: 6.193179\n",
            "Iteration Number: 62, Loss: 20.027828, 12 norm of gradients: 20.804765, 12 norm of weights: 6.181872\n",
            "Iteration Number: 63, Loss: 19.950825, 12 norm of gradients: 20.615750, 12 norm of weights: 6.170623\n",
            "Iteration Number: 64, Loss: 19.874443, 12 norm of gradients: 20.431442, 12 norm of weights: 6.159428\n",
            "Iteration Number: 65, Loss: 19.798667, 12 norm of gradients: 20.251694, 12 norm of weights: 6.148289\n",
            "Iteration Number: 66, Loss: 19.723485, 12 norm of gradients: 20.076361, 12 norm of weights: 6.137203\n",
            "Iteration Number: 67, Loss: 19.648884, 12 norm of gradients: 19.905306, 12 norm of weights: 6.126169\n",
            "Iteration Number: 68, Loss: 19.574854, 12 norm of gradients: 19.738392, 12 norm of weights: 6.115187\n",
            "Iteration Number: 69, Loss: 19.501381, 12 norm of gradients: 19.575485, 12 norm of weights: 6.104256\n",
            "Iteration Number: 70, Loss: 19.428456, 12 norm of gradients: 19.416458, 12 norm of weights: 6.093375\n",
            "Iteration Number: 71, Loss: 19.356068, 12 norm of gradients: 19.261182, 12 norm of weights: 6.082543\n",
            "Iteration Number: 72, Loss: 19.284205, 12 norm of gradients: 19.109535, 12 norm of weights: 6.071759\n",
            "Iteration Number: 73, Loss: 19.212860, 12 norm of gradients: 18.961397, 12 norm of weights: 6.061023\n",
            "Iteration Number: 74, Loss: 19.142021, 12 norm of gradients: 18.816650, 12 norm of weights: 6.050334\n",
            "Iteration Number: 75, Loss: 19.071680, 12 norm of gradients: 18.675179, 12 norm of weights: 6.039691\n",
            "Iteration Number: 76, Loss: 19.001829, 12 norm of gradients: 18.536873, 12 norm of weights: 6.029093\n",
            "Iteration Number: 77, Loss: 18.932458, 12 norm of gradients: 18.401624, 12 norm of weights: 6.018541\n",
            "Iteration Number: 78, Loss: 18.863559, 12 norm of gradients: 18.269326, 12 norm of weights: 6.008032\n",
            "Iteration Number: 79, Loss: 18.795126, 12 norm of gradients: 18.139877, 12 norm of weights: 5.997568\n",
            "Iteration Number: 80, Loss: 18.727149, 12 norm of gradients: 18.013177, 12 norm of weights: 5.987146\n",
            "Iteration Number: 81, Loss: 18.659622, 12 norm of gradients: 17.889129, 12 norm of weights: 5.976767\n",
            "Iteration Number: 82, Loss: 18.592537, 12 norm of gradients: 17.767641, 12 norm of weights: 5.966430\n",
            "Iteration Number: 83, Loss: 18.525888, 12 norm of gradients: 17.648620, 12 norm of weights: 5.956134\n",
            "Iteration Number: 84, Loss: 18.459669, 12 norm of gradients: 17.531979, 12 norm of weights: 5.945879\n",
            "Iteration Number: 85, Loss: 18.393872, 12 norm of gradients: 17.417633, 12 norm of weights: 5.935664\n",
            "Iteration Number: 86, Loss: 18.328491, 12 norm of gradients: 17.305498, 12 norm of weights: 5.925490\n",
            "Iteration Number: 87, Loss: 18.263522, 12 norm of gradients: 17.195496, 12 norm of weights: 5.915355\n",
            "Iteration Number: 88, Loss: 18.198957, 12 norm of gradients: 17.087548, 12 norm of weights: 5.905259\n",
            "Iteration Number: 89, Loss: 18.134791, 12 norm of gradients: 16.981581, 12 norm of weights: 5.895202\n",
            "Iteration Number: 90, Loss: 18.071020, 12 norm of gradients: 16.877521, 12 norm of weights: 5.885183\n",
            "Iteration Number: 91, Loss: 18.007637, 12 norm of gradients: 16.775300, 12 norm of weights: 5.875201\n",
            "Iteration Number: 92, Loss: 17.944637, 12 norm of gradients: 16.674848, 12 norm of weights: 5.865257\n",
            "Iteration Number: 93, Loss: 17.882016, 12 norm of gradients: 16.576103, 12 norm of weights: 5.855350\n",
            "Iteration Number: 94, Loss: 17.819769, 12 norm of gradients: 16.479000, 12 norm of weights: 5.845480\n",
            "Iteration Number: 95, Loss: 17.757891, 12 norm of gradients: 16.383480, 12 norm of weights: 5.835646\n",
            "Iteration Number: 96, Loss: 17.696378, 12 norm of gradients: 16.289484, 12 norm of weights: 5.825848\n",
            "Iteration Number: 97, Loss: 17.635225, 12 norm of gradients: 16.196956, 12 norm of weights: 5.816085\n",
            "Iteration Number: 98, Loss: 17.574428, 12 norm of gradients: 16.105843, 12 norm of weights: 5.806358\n",
            "Iteration Number: 99, Loss: 17.513982, 12 norm of gradients: 16.016092, 12 norm of weights: 5.796666\n",
            "Iteration Number: 100, Loss: 17.453885, 12 norm of gradients: 15.927654, 12 norm of weights: 5.787008\n",
            "Iteration Number: 101, Loss: 17.394132, 12 norm of gradients: 15.840481, 12 norm of weights: 5.777385\n",
            "Iteration Number: 102, Loss: 17.334718, 12 norm of gradients: 15.754527, 12 norm of weights: 5.767796\n",
            "Iteration Number: 103, Loss: 17.275641, 12 norm of gradients: 15.669750, 12 norm of weights: 5.758240\n",
            "Iteration Number: 104, Loss: 17.216897, 12 norm of gradients: 15.586106, 12 norm of weights: 5.748718\n",
            "Iteration Number: 105, Loss: 17.158482, 12 norm of gradients: 15.503556, 12 norm of weights: 5.739229\n",
            "Iteration Number: 106, Loss: 17.100392, 12 norm of gradients: 15.422063, 12 norm of weights: 5.729772\n",
            "Iteration Number: 107, Loss: 17.042625, 12 norm of gradients: 15.341589, 12 norm of weights: 5.720349\n",
            "Iteration Number: 108, Loss: 16.985176, 12 norm of gradients: 15.262101, 12 norm of weights: 5.710957\n",
            "Iteration Number: 109, Loss: 16.928043, 12 norm of gradients: 15.183565, 12 norm of weights: 5.701598\n",
            "Iteration Number: 110, Loss: 16.871222, 12 norm of gradients: 15.105951, 12 norm of weights: 5.692270\n",
            "Iteration Number: 111, Loss: 16.814711, 12 norm of gradients: 15.029230, 12 norm of weights: 5.682974\n",
            "Iteration Number: 112, Loss: 16.758506, 12 norm of gradients: 14.953373, 12 norm of weights: 5.673709\n",
            "Iteration Number: 113, Loss: 16.702604, 12 norm of gradients: 14.878354, 12 norm of weights: 5.664475\n",
            "Iteration Number: 114, Loss: 16.647003, 12 norm of gradients: 14.804150, 12 norm of weights: 5.655272\n",
            "Iteration Number: 115, Loss: 16.591699, 12 norm of gradients: 14.730736, 12 norm of weights: 5.646100\n",
            "Iteration Number: 116, Loss: 16.536689, 12 norm of gradients: 14.658092, 12 norm of weights: 5.636958\n",
            "Iteration Number: 117, Loss: 16.481971, 12 norm of gradients: 14.586196, 12 norm of weights: 5.627845\n",
            "Iteration Number: 118, Loss: 16.427542, 12 norm of gradients: 14.515031, 12 norm of weights: 5.618763\n",
            "Iteration Number: 119, Loss: 16.373399, 12 norm of gradients: 14.444579, 12 norm of weights: 5.609710\n",
            "Iteration Number: 120, Loss: 16.319539, 12 norm of gradients: 14.374822, 12 norm of weights: 5.600687\n",
            "Iteration Number: 121, Loss: 16.265961, 12 norm of gradients: 14.305748, 12 norm of weights: 5.591692\n",
            "Iteration Number: 122, Loss: 16.212661, 12 norm of gradients: 14.237340, 12 norm of weights: 5.582727\n",
            "Iteration Number: 123, Loss: 16.159636, 12 norm of gradients: 14.169587, 12 norm of weights: 5.573790\n",
            "Iteration Number: 124, Loss: 16.106885, 12 norm of gradients: 14.102477, 12 norm of weights: 5.564882\n",
            "Iteration Number: 125, Loss: 16.054404, 12 norm of gradients: 14.035999, 12 norm of weights: 5.556002\n",
            "Iteration Number: 126, Loss: 16.002192, 12 norm of gradients: 13.970144, 12 norm of weights: 5.547150\n",
            "Iteration Number: 127, Loss: 15.950246, 12 norm of gradients: 13.904901, 12 norm of weights: 5.538326\n",
            "Iteration Number: 128, Loss: 15.898563, 12 norm of gradients: 13.840263, 12 norm of weights: 5.529529\n",
            "Iteration Number: 129, Loss: 15.847141, 12 norm of gradients: 13.776223, 12 norm of weights: 5.520760\n",
            "Iteration Number: 130, Loss: 15.795978, 12 norm of gradients: 13.712773, 12 norm of weights: 5.512018\n",
            "Iteration Number: 131, Loss: 15.745071, 12 norm of gradients: 13.649907, 12 norm of weights: 5.503303\n",
            "Iteration Number: 132, Loss: 15.694419, 12 norm of gradients: 13.587620, 12 norm of weights: 5.494615\n",
            "Iteration Number: 133, Loss: 15.644019, 12 norm of gradients: 13.525907, 12 norm of weights: 5.485953\n",
            "Iteration Number: 134, Loss: 15.593868, 12 norm of gradients: 13.464763, 12 norm of weights: 5.477318\n",
            "Iteration Number: 135, Loss: 15.543965, 12 norm of gradients: 13.404182, 12 norm of weights: 5.468709\n",
            "Iteration Number: 136, Loss: 15.494307, 12 norm of gradients: 13.344163, 12 norm of weights: 5.460126\n",
            "Iteration Number: 137, Loss: 15.444893, 12 norm of gradients: 13.284699, 12 norm of weights: 5.451569\n",
            "Iteration Number: 138, Loss: 15.395720, 12 norm of gradients: 13.225788, 12 norm of weights: 5.443038\n",
            "Iteration Number: 139, Loss: 15.346786, 12 norm of gradients: 13.167427, 12 norm of weights: 5.434531\n",
            "Iteration Number: 140, Loss: 15.298089, 12 norm of gradients: 13.109611, 12 norm of weights: 5.426051\n",
            "Iteration Number: 141, Loss: 15.249627, 12 norm of gradients: 13.052338, 12 norm of weights: 5.417595\n",
            "Iteration Number: 142, Loss: 15.201398, 12 norm of gradients: 12.995604, 12 norm of weights: 5.409164\n",
            "Iteration Number: 143, Loss: 15.153400, 12 norm of gradients: 12.939406, 12 norm of weights: 5.400758\n",
            "Iteration Number: 144, Loss: 15.105631, 12 norm of gradients: 12.883741, 12 norm of weights: 5.392377\n",
            "Iteration Number: 145, Loss: 15.058089, 12 norm of gradients: 12.828604, 12 norm of weights: 5.384020\n",
            "Iteration Number: 146, Loss: 15.010773, 12 norm of gradients: 12.773993, 12 norm of weights: 5.375687\n",
            "Iteration Number: 147, Loss: 14.963680, 12 norm of gradients: 12.719903, 12 norm of weights: 5.367378\n",
            "Iteration Number: 148, Loss: 14.916809, 12 norm of gradients: 12.666331, 12 norm of weights: 5.359093\n",
            "Iteration Number: 149, Loss: 14.870158, 12 norm of gradients: 12.613273, 12 norm of weights: 5.350832\n",
            "Iteration Number: 150, Loss: 14.823724, 12 norm of gradients: 12.560724, 12 norm of weights: 5.342594\n",
            "Iteration Number: 151, Loss: 14.777507, 12 norm of gradients: 12.508680, 12 norm of weights: 5.334380\n",
            "Iteration Number: 152, Loss: 14.731504, 12 norm of gradients: 12.457137, 12 norm of weights: 5.326189\n",
            "Iteration Number: 153, Loss: 14.685714, 12 norm of gradients: 12.406089, 12 norm of weights: 5.318021\n",
            "Iteration Number: 154, Loss: 14.640135, 12 norm of gradients: 12.355533, 12 norm of weights: 5.309876\n",
            "Iteration Number: 155, Loss: 14.594766, 12 norm of gradients: 12.305462, 12 norm of weights: 5.301754\n",
            "Iteration Number: 156, Loss: 14.549604, 12 norm of gradients: 12.255871, 12 norm of weights: 5.293655\n",
            "Iteration Number: 157, Loss: 14.504648, 12 norm of gradients: 12.206755, 12 norm of weights: 5.285578\n",
            "Iteration Number: 158, Loss: 14.459897, 12 norm of gradients: 12.158108, 12 norm of weights: 5.277524\n",
            "Iteration Number: 159, Loss: 14.415349, 12 norm of gradients: 12.109926, 12 norm of weights: 5.269492\n",
            "Iteration Number: 160, Loss: 14.371002, 12 norm of gradients: 12.062200, 12 norm of weights: 5.261482\n",
            "Iteration Number: 161, Loss: 14.326855, 12 norm of gradients: 12.014927, 12 norm of weights: 5.253494\n",
            "Iteration Number: 162, Loss: 14.282907, 12 norm of gradients: 11.968099, 12 norm of weights: 5.245528\n",
            "Iteration Number: 163, Loss: 14.239155, 12 norm of gradients: 11.921710, 12 norm of weights: 5.237583\n",
            "Iteration Number: 164, Loss: 14.195599, 12 norm of gradients: 11.875755, 12 norm of weights: 5.229660\n",
            "Iteration Number: 165, Loss: 14.152237, 12 norm of gradients: 11.830226, 12 norm of weights: 5.221759\n",
            "Iteration Number: 166, Loss: 14.109068, 12 norm of gradients: 11.785118, 12 norm of weights: 5.213879\n",
            "Iteration Number: 167, Loss: 14.066090, 12 norm of gradients: 11.740424, 12 norm of weights: 5.206020\n",
            "Iteration Number: 168, Loss: 14.023301, 12 norm of gradients: 11.696138, 12 norm of weights: 5.198183\n",
            "Iteration Number: 169, Loss: 13.980701, 12 norm of gradients: 11.652253, 12 norm of weights: 5.190366\n",
            "Iteration Number: 170, Loss: 13.938288, 12 norm of gradients: 11.608764, 12 norm of weights: 5.182570\n",
            "Iteration Number: 171, Loss: 13.896061, 12 norm of gradients: 11.565662, 12 norm of weights: 5.174795\n",
            "Iteration Number: 172, Loss: 13.854019, 12 norm of gradients: 11.522944, 12 norm of weights: 5.167041\n",
            "Iteration Number: 173, Loss: 13.812159, 12 norm of gradients: 11.480601, 12 norm of weights: 5.159308\n",
            "Iteration Number: 174, Loss: 13.770482, 12 norm of gradients: 11.438628, 12 norm of weights: 5.151594\n",
            "Iteration Number: 175, Loss: 13.728986, 12 norm of gradients: 11.397018, 12 norm of weights: 5.143902\n",
            "Iteration Number: 176, Loss: 13.687668, 12 norm of gradients: 11.355766, 12 norm of weights: 5.136229\n",
            "Iteration Number: 177, Loss: 13.646530, 12 norm of gradients: 11.314866, 12 norm of weights: 5.128576\n",
            "Iteration Number: 178, Loss: 13.605568, 12 norm of gradients: 11.274311, 12 norm of weights: 5.120944\n",
            "Iteration Number: 179, Loss: 13.564782, 12 norm of gradients: 11.234096, 12 norm of weights: 5.113332\n",
            "Iteration Number: 180, Loss: 13.524171, 12 norm of gradients: 11.194215, 12 norm of weights: 5.105739\n",
            "Iteration Number: 181, Loss: 13.483733, 12 norm of gradients: 11.154662, 12 norm of weights: 5.098166\n",
            "Iteration Number: 182, Loss: 13.443468, 12 norm of gradients: 11.115432, 12 norm of weights: 5.090613\n",
            "Iteration Number: 183, Loss: 13.403374, 12 norm of gradients: 11.076520, 12 norm of weights: 5.083079\n",
            "Iteration Number: 184, Loss: 13.363451, 12 norm of gradients: 11.037919, 12 norm of weights: 5.075565\n",
            "Iteration Number: 185, Loss: 13.323696, 12 norm of gradients: 10.999625, 12 norm of weights: 5.068070\n",
            "Iteration Number: 186, Loss: 13.284110, 12 norm of gradients: 10.961633, 12 norm of weights: 5.060594\n",
            "Iteration Number: 187, Loss: 13.244690, 12 norm of gradients: 10.923937, 12 norm of weights: 5.053138\n",
            "Iteration Number: 188, Loss: 13.205437, 12 norm of gradients: 10.886534, 12 norm of weights: 5.045701\n",
            "Iteration Number: 189, Loss: 13.166348, 12 norm of gradients: 10.849417, 12 norm of weights: 5.038282\n",
            "Iteration Number: 190, Loss: 13.127424, 12 norm of gradients: 10.812582, 12 norm of weights: 5.030883\n",
            "Iteration Number: 191, Loss: 13.088662, 12 norm of gradients: 10.776025, 12 norm of weights: 5.023502\n",
            "Iteration Number: 192, Loss: 13.050062, 12 norm of gradients: 10.739741, 12 norm of weights: 5.016141\n",
            "Iteration Number: 193, Loss: 13.011622, 12 norm of gradients: 10.703727, 12 norm of weights: 5.008797\n",
            "Iteration Number: 194, Loss: 12.973343, 12 norm of gradients: 10.667976, 12 norm of weights: 5.001473\n",
            "Iteration Number: 195, Loss: 12.935222, 12 norm of gradients: 10.632486, 12 norm of weights: 4.994167\n",
            "Iteration Number: 196, Loss: 12.897260, 12 norm of gradients: 10.597253, 12 norm of weights: 4.986879\n",
            "Iteration Number: 197, Loss: 12.859454, 12 norm of gradients: 10.562272, 12 norm of weights: 4.979610\n",
            "Iteration Number: 198, Loss: 12.821805, 12 norm of gradients: 10.527540, 12 norm of weights: 4.972359\n",
            "Iteration Number: 199, Loss: 12.784310, 12 norm of gradients: 10.493052, 12 norm of weights: 4.965126\n",
            "Iteration Number: 200, Loss: 12.746970, 12 norm of gradients: 10.458806, 12 norm of weights: 4.957912\n",
            "Iteration Number: 201, Loss: 12.709783, 12 norm of gradients: 10.424797, 12 norm of weights: 4.950715\n",
            "Iteration Number: 202, Loss: 12.672748, 12 norm of gradients: 10.391023, 12 norm of weights: 4.943536\n",
            "Iteration Number: 203, Loss: 12.635865, 12 norm of gradients: 10.357479, 12 norm of weights: 4.936375\n",
            "Iteration Number: 204, Loss: 12.599133, 12 norm of gradients: 10.324162, 12 norm of weights: 4.929232\n",
            "Iteration Number: 205, Loss: 12.562550, 12 norm of gradients: 10.291070, 12 norm of weights: 4.922107\n",
            "Iteration Number: 206, Loss: 12.526116, 12 norm of gradients: 10.258199, 12 norm of weights: 4.914999\n",
            "Iteration Number: 207, Loss: 12.489830, 12 norm of gradients: 10.225546, 12 norm of weights: 4.907909\n",
            "Iteration Number: 208, Loss: 12.453691, 12 norm of gradients: 10.193107, 12 norm of weights: 4.900837\n",
            "Iteration Number: 209, Loss: 12.417699, 12 norm of gradients: 10.160881, 12 norm of weights: 4.893782\n",
            "Iteration Number: 210, Loss: 12.381852, 12 norm of gradients: 10.128863, 12 norm of weights: 4.886744\n",
            "Iteration Number: 211, Loss: 12.346150, 12 norm of gradients: 10.097053, 12 norm of weights: 4.879724\n",
            "Iteration Number: 212, Loss: 12.310591, 12 norm of gradients: 10.065445, 12 norm of weights: 4.872720\n",
            "Iteration Number: 213, Loss: 12.275176, 12 norm of gradients: 10.034039, 12 norm of weights: 4.865734\n",
            "Iteration Number: 214, Loss: 12.239903, 12 norm of gradients: 10.002831, 12 norm of weights: 4.858766\n",
            "Iteration Number: 215, Loss: 12.204771, 12 norm of gradients: 9.971818, 12 norm of weights: 4.851814\n",
            "Iteration Number: 216, Loss: 12.169780, 12 norm of gradients: 9.940999, 12 norm of weights: 4.844879\n",
            "Iteration Number: 217, Loss: 12.134929, 12 norm of gradients: 9.910371, 12 norm of weights: 4.837961\n",
            "Iteration Number: 218, Loss: 12.100217, 12 norm of gradients: 9.879931, 12 norm of weights: 4.831060\n",
            "Iteration Number: 219, Loss: 12.065644, 12 norm of gradients: 9.849677, 12 norm of weights: 4.824175\n",
            "Iteration Number: 220, Loss: 12.031208, 12 norm of gradients: 9.819607, 12 norm of weights: 4.817307\n",
            "Iteration Number: 221, Loss: 11.996909, 12 norm of gradients: 9.789718, 12 norm of weights: 4.810456\n",
            "Iteration Number: 222, Loss: 11.962746, 12 norm of gradients: 9.760009, 12 norm of weights: 4.803622\n",
            "Iteration Number: 223, Loss: 11.928718, 12 norm of gradients: 9.730477, 12 norm of weights: 4.796804\n",
            "Iteration Number: 224, Loss: 11.894825, 12 norm of gradients: 9.701120, 12 norm of weights: 4.790002\n",
            "Iteration Number: 225, Loss: 11.861066, 12 norm of gradients: 9.671936, 12 norm of weights: 4.783217\n",
            "Iteration Number: 226, Loss: 11.827440, 12 norm of gradients: 9.642923, 12 norm of weights: 4.776449\n",
            "Iteration Number: 227, Loss: 11.793946, 12 norm of gradients: 9.614080, 12 norm of weights: 4.769696\n",
            "Iteration Number: 228, Loss: 11.760585, 12 norm of gradients: 9.585403, 12 norm of weights: 4.762960\n",
            "Iteration Number: 229, Loss: 11.727354, 12 norm of gradients: 9.556891, 12 norm of weights: 4.756240\n",
            "Iteration Number: 230, Loss: 11.694254, 12 norm of gradients: 9.528543, 12 norm of weights: 4.749535\n",
            "Iteration Number: 231, Loss: 11.661284, 12 norm of gradients: 9.500356, 12 norm of weights: 4.742847\n",
            "Iteration Number: 232, Loss: 11.628443, 12 norm of gradients: 9.472328, 12 norm of weights: 4.736175\n",
            "Iteration Number: 233, Loss: 11.595729, 12 norm of gradients: 9.444459, 12 norm of weights: 4.729519\n",
            "Iteration Number: 234, Loss: 11.563144, 12 norm of gradients: 9.416745, 12 norm of weights: 4.722879\n",
            "Iteration Number: 235, Loss: 11.530686, 12 norm of gradients: 9.389186, 12 norm of weights: 4.716254\n",
            "Iteration Number: 236, Loss: 11.498354, 12 norm of gradients: 9.361780, 12 norm of weights: 4.709646\n",
            "Iteration Number: 237, Loss: 11.466148, 12 norm of gradients: 9.334525, 12 norm of weights: 4.703053\n",
            "Iteration Number: 238, Loss: 11.434066, 12 norm of gradients: 9.307419, 12 norm of weights: 4.696475\n",
            "Iteration Number: 239, Loss: 11.402110, 12 norm of gradients: 9.280461, 12 norm of weights: 4.689913\n",
            "Iteration Number: 240, Loss: 11.370277, 12 norm of gradients: 9.253649, 12 norm of weights: 4.683367\n",
            "Iteration Number: 241, Loss: 11.338567, 12 norm of gradients: 9.226982, 12 norm of weights: 4.676836\n",
            "Iteration Number: 242, Loss: 11.306979, 12 norm of gradients: 9.200458, 12 norm of weights: 4.670321\n",
            "Iteration Number: 243, Loss: 11.275514, 12 norm of gradients: 9.174075, 12 norm of weights: 4.663821\n",
            "Iteration Number: 244, Loss: 11.244170, 12 norm of gradients: 9.147833, 12 norm of weights: 4.657336\n",
            "Iteration Number: 245, Loss: 11.212946, 12 norm of gradients: 9.121730, 12 norm of weights: 4.650867\n",
            "Iteration Number: 246, Loss: 11.181843, 12 norm of gradients: 9.095764, 12 norm of weights: 4.644413\n",
            "Iteration Number: 247, Loss: 11.150859, 12 norm of gradients: 9.069934, 12 norm of weights: 4.637973\n",
            "Iteration Number: 248, Loss: 11.119994, 12 norm of gradients: 9.044238, 12 norm of weights: 4.631549\n",
            "Iteration Number: 249, Loss: 11.089247, 12 norm of gradients: 9.018676, 12 norm of weights: 4.625141\n",
            "Iteration Number: 250, Loss: 11.058618, 12 norm of gradients: 8.993245, 12 norm of weights: 4.618747\n",
            "Iteration Number: 251, Loss: 11.028106, 12 norm of gradients: 8.967945, 12 norm of weights: 4.612368\n",
            "Iteration Number: 252, Loss: 10.997710, 12 norm of gradients: 8.942774, 12 norm of weights: 4.606004\n",
            "Iteration Number: 253, Loss: 10.967431, 12 norm of gradients: 8.917731, 12 norm of weights: 4.599654\n",
            "Iteration Number: 254, Loss: 10.937266, 12 norm of gradients: 8.892814, 12 norm of weights: 4.593320\n",
            "Iteration Number: 255, Loss: 10.907217, 12 norm of gradients: 8.868023, 12 norm of weights: 4.587000\n",
            "Iteration Number: 256, Loss: 10.877282, 12 norm of gradients: 8.843356, 12 norm of weights: 4.580695\n",
            "Iteration Number: 257, Loss: 10.847460, 12 norm of gradients: 8.818812, 12 norm of weights: 4.574405\n",
            "Iteration Number: 258, Loss: 10.817752, 12 norm of gradients: 8.794390, 12 norm of weights: 4.568130\n",
            "Iteration Number: 259, Loss: 10.788156, 12 norm of gradients: 8.770089, 12 norm of weights: 4.561868\n",
            "Iteration Number: 260, Loss: 10.758672, 12 norm of gradients: 8.745906, 12 norm of weights: 4.555622\n",
            "Iteration Number: 261, Loss: 10.729299, 12 norm of gradients: 8.721842, 12 norm of weights: 4.549390\n",
            "Iteration Number: 262, Loss: 10.700038, 12 norm of gradients: 8.697896, 12 norm of weights: 4.543172\n",
            "Iteration Number: 263, Loss: 10.670887, 12 norm of gradients: 8.674065, 12 norm of weights: 4.536969\n",
            "Iteration Number: 264, Loss: 10.641846, 12 norm of gradients: 8.650349, 12 norm of weights: 4.530780\n",
            "Iteration Number: 265, Loss: 10.612914, 12 norm of gradients: 8.626747, 12 norm of weights: 4.524605\n",
            "Iteration Number: 266, Loss: 10.584091, 12 norm of gradients: 8.603257, 12 norm of weights: 4.518445\n",
            "Iteration Number: 267, Loss: 10.555376, 12 norm of gradients: 8.579880, 12 norm of weights: 4.512299\n",
            "Iteration Number: 268, Loss: 10.526770, 12 norm of gradients: 8.556613, 12 norm of weights: 4.506167\n",
            "Iteration Number: 269, Loss: 10.498270, 12 norm of gradients: 8.533455, 12 norm of weights: 4.500049\n",
            "Iteration Number: 270, Loss: 10.469877, 12 norm of gradients: 8.510407, 12 norm of weights: 4.493945\n",
            "Iteration Number: 271, Loss: 10.441591, 12 norm of gradients: 8.487465, 12 norm of weights: 4.487855\n",
            "Iteration Number: 272, Loss: 10.413410, 12 norm of gradients: 8.464631, 12 norm of weights: 4.481779\n",
            "Iteration Number: 273, Loss: 10.385335, 12 norm of gradients: 8.441902, 12 norm of weights: 4.475717\n",
            "Iteration Number: 274, Loss: 10.357364, 12 norm of gradients: 8.419277, 12 norm of weights: 4.469669\n",
            "Iteration Number: 275, Loss: 10.329497, 12 norm of gradients: 8.396757, 12 norm of weights: 4.463635\n",
            "Iteration Number: 276, Loss: 10.301735, 12 norm of gradients: 8.374339, 12 norm of weights: 4.457614\n",
            "Iteration Number: 277, Loss: 10.274076, 12 norm of gradients: 8.352022, 12 norm of weights: 4.451608\n",
            "Iteration Number: 278, Loss: 10.246519, 12 norm of gradients: 8.329807, 12 norm of weights: 4.445615\n",
            "Iteration Number: 279, Loss: 10.219065, 12 norm of gradients: 8.307692, 12 norm of weights: 4.439636\n",
            "Iteration Number: 280, Loss: 10.191713, 12 norm of gradients: 8.285676, 12 norm of weights: 4.433670\n",
            "Iteration Number: 281, Loss: 10.164462, 12 norm of gradients: 8.263757, 12 norm of weights: 4.427718\n",
            "Iteration Number: 282, Loss: 10.137312, 12 norm of gradients: 8.241937, 12 norm of weights: 4.421780\n",
            "Iteration Number: 283, Loss: 10.110263, 12 norm of gradients: 8.220212, 12 norm of weights: 4.415855\n",
            "Iteration Number: 284, Loss: 10.083314, 12 norm of gradients: 8.198584, 12 norm of weights: 4.409944\n",
            "Iteration Number: 285, Loss: 10.056464, 12 norm of gradients: 8.177050, 12 norm of weights: 4.404046\n",
            "Iteration Number: 286, Loss: 10.029714, 12 norm of gradients: 8.155609, 12 norm of weights: 4.398161\n",
            "Iteration Number: 287, Loss: 10.003062, 12 norm of gradients: 8.134262, 12 norm of weights: 4.392290\n",
            "Iteration Number: 288, Loss: 9.976508, 12 norm of gradients: 8.113008, 12 norm of weights: 4.386432\n",
            "Iteration Number: 289, Loss: 9.950052, 12 norm of gradients: 8.091844, 12 norm of weights: 4.380588\n",
            "Iteration Number: 290, Loss: 9.923694, 12 norm of gradients: 8.070771, 12 norm of weights: 4.374756\n",
            "Iteration Number: 291, Loss: 9.897432, 12 norm of gradients: 8.049789, 12 norm of weights: 4.368938\n",
            "Iteration Number: 292, Loss: 9.871267, 12 norm of gradients: 8.028895, 12 norm of weights: 4.363133\n",
            "Iteration Number: 293, Loss: 9.845198, 12 norm of gradients: 8.008089, 12 norm of weights: 4.357342\n",
            "Iteration Number: 294, Loss: 9.819224, 12 norm of gradients: 7.987371, 12 norm of weights: 4.351563\n",
            "Iteration Number: 295, Loss: 9.793345, 12 norm of gradients: 7.966740, 12 norm of weights: 4.345797\n",
            "Iteration Number: 296, Loss: 9.767562, 12 norm of gradients: 7.946195, 12 norm of weights: 4.340045\n",
            "Iteration Number: 297, Loss: 9.741872, 12 norm of gradients: 7.925735, 12 norm of weights: 4.334305\n",
            "Iteration Number: 298, Loss: 9.716277, 12 norm of gradients: 7.905360, 12 norm of weights: 4.328578\n",
            "Iteration Number: 299, Loss: 9.690775, 12 norm of gradients: 7.885069, 12 norm of weights: 4.322865\n",
            "Iteration Number: 300, Loss: 9.665366, 12 norm of gradients: 7.864861, 12 norm of weights: 4.317164\n",
            "Iteration Number: 301, Loss: 9.640050, 12 norm of gradients: 7.844735, 12 norm of weights: 4.311476\n",
            "Iteration Number: 302, Loss: 9.614825, 12 norm of gradients: 7.824691, 12 norm of weights: 4.305801\n",
            "Iteration Number: 303, Loss: 9.589693, 12 norm of gradients: 7.804728, 12 norm of weights: 4.300138\n",
            "Iteration Number: 304, Loss: 9.564652, 12 norm of gradients: 7.784846, 12 norm of weights: 4.294488\n",
            "Iteration Number: 305, Loss: 9.539703, 12 norm of gradients: 7.765043, 12 norm of weights: 4.288851\n",
            "Iteration Number: 306, Loss: 9.514843, 12 norm of gradients: 7.745319, 12 norm of weights: 4.283227\n",
            "Iteration Number: 307, Loss: 9.490075, 12 norm of gradients: 7.725674, 12 norm of weights: 4.277615\n",
            "Iteration Number: 308, Loss: 9.465395, 12 norm of gradients: 7.706107, 12 norm of weights: 4.272016\n",
            "Iteration Number: 309, Loss: 9.440806, 12 norm of gradients: 7.686617, 12 norm of weights: 4.266430\n",
            "Iteration Number: 310, Loss: 9.416305, 12 norm of gradients: 7.667203, 12 norm of weights: 4.260856\n",
            "Iteration Number: 311, Loss: 9.391893, 12 norm of gradients: 7.647865, 12 norm of weights: 4.255294\n",
            "Iteration Number: 312, Loss: 9.367570, 12 norm of gradients: 7.628602, 12 norm of weights: 4.249745\n",
            "Iteration Number: 313, Loss: 9.343334, 12 norm of gradients: 7.609415, 12 norm of weights: 4.244208\n",
            "Iteration Number: 314, Loss: 9.319186, 12 norm of gradients: 7.590301, 12 norm of weights: 4.238684\n",
            "Iteration Number: 315, Loss: 9.295125, 12 norm of gradients: 7.571260, 12 norm of weights: 4.233172\n",
            "Iteration Number: 316, Loss: 9.271150, 12 norm of gradients: 7.552293, 12 norm of weights: 4.227672\n",
            "Iteration Number: 317, Loss: 9.247263, 12 norm of gradients: 7.533398, 12 norm of weights: 4.222185\n",
            "Iteration Number: 318, Loss: 9.223461, 12 norm of gradients: 7.514574, 12 norm of weights: 4.216710\n",
            "Iteration Number: 319, Loss: 9.199745, 12 norm of gradients: 7.495822, 12 norm of weights: 4.211247\n",
            "Iteration Number: 320, Loss: 9.176114, 12 norm of gradients: 7.477141, 12 norm of weights: 4.205797\n",
            "Iteration Number: 321, Loss: 9.152568, 12 norm of gradients: 7.458529, 12 norm of weights: 4.200358\n",
            "Iteration Number: 322, Loss: 9.129106, 12 norm of gradients: 7.439987, 12 norm of weights: 4.194932\n",
            "Iteration Number: 323, Loss: 9.105729, 12 norm of gradients: 7.421514, 12 norm of weights: 4.189518\n",
            "Iteration Number: 324, Loss: 9.082436, 12 norm of gradients: 7.403110, 12 norm of weights: 4.184115\n",
            "Iteration Number: 325, Loss: 9.059226, 12 norm of gradients: 7.384773, 12 norm of weights: 4.178725\n",
            "Iteration Number: 326, Loss: 9.036099, 12 norm of gradients: 7.366504, 12 norm of weights: 4.173347\n",
            "Iteration Number: 327, Loss: 9.013055, 12 norm of gradients: 7.348301, 12 norm of weights: 4.167981\n",
            "Iteration Number: 328, Loss: 8.990094, 12 norm of gradients: 7.330165, 12 norm of weights: 4.162627\n",
            "Iteration Number: 329, Loss: 8.967214, 12 norm of gradients: 7.312095, 12 norm of weights: 4.157285\n",
            "Iteration Number: 330, Loss: 8.944417, 12 norm of gradients: 7.294090, 12 norm of weights: 4.151954\n",
            "Iteration Number: 331, Loss: 8.921701, 12 norm of gradients: 7.276151, 12 norm of weights: 4.146636\n",
            "Iteration Number: 332, Loss: 8.899065, 12 norm of gradients: 7.258275, 12 norm of weights: 4.141329\n",
            "Iteration Number: 333, Loss: 8.876511, 12 norm of gradients: 7.240464, 12 norm of weights: 4.136034\n",
            "Iteration Number: 334, Loss: 8.854037, 12 norm of gradients: 7.222715, 12 norm of weights: 4.130751\n",
            "Iteration Number: 335, Loss: 8.831643, 12 norm of gradients: 7.205030, 12 norm of weights: 4.125479\n",
            "Iteration Number: 336, Loss: 8.809328, 12 norm of gradients: 7.187408, 12 norm of weights: 4.120219\n",
            "Iteration Number: 337, Loss: 8.787093, 12 norm of gradients: 7.169847, 12 norm of weights: 4.114971\n",
            "Iteration Number: 338, Loss: 8.764937, 12 norm of gradients: 7.152348, 12 norm of weights: 4.109735\n",
            "Iteration Number: 339, Loss: 8.742860, 12 norm of gradients: 7.134911, 12 norm of weights: 4.104510\n",
            "Iteration Number: 340, Loss: 8.720862, 12 norm of gradients: 7.117534, 12 norm of weights: 4.099297\n",
            "Iteration Number: 341, Loss: 8.698941, 12 norm of gradients: 7.100217, 12 norm of weights: 4.094095\n",
            "Iteration Number: 342, Loss: 8.677098, 12 norm of gradients: 7.082960, 12 norm of weights: 4.088905\n",
            "Iteration Number: 343, Loss: 8.655332, 12 norm of gradients: 7.065763, 12 norm of weights: 4.083726\n",
            "Iteration Number: 344, Loss: 8.633644, 12 norm of gradients: 7.048624, 12 norm of weights: 4.078559\n",
            "Iteration Number: 345, Loss: 8.612032, 12 norm of gradients: 7.031544, 12 norm of weights: 4.073403\n",
            "Iteration Number: 346, Loss: 8.590497, 12 norm of gradients: 7.014523, 12 norm of weights: 4.068259\n",
            "Iteration Number: 347, Loss: 8.569038, 12 norm of gradients: 6.997559, 12 norm of weights: 4.063126\n",
            "Iteration Number: 348, Loss: 8.547655, 12 norm of gradients: 6.980652, 12 norm of weights: 4.058004\n",
            "Iteration Number: 349, Loss: 8.526347, 12 norm of gradients: 6.963803, 12 norm of weights: 4.052894\n",
            "Iteration Number: 350, Loss: 8.505115, 12 norm of gradients: 6.947010, 12 norm of weights: 4.047795\n",
            "Iteration Number: 351, Loss: 8.483957, 12 norm of gradients: 6.930273, 12 norm of weights: 4.042707\n",
            "Iteration Number: 352, Loss: 8.462874, 12 norm of gradients: 6.913593, 12 norm of weights: 4.037630\n",
            "Iteration Number: 353, Loss: 8.441866, 12 norm of gradients: 6.896967, 12 norm of weights: 4.032565\n",
            "Iteration Number: 354, Loss: 8.420931, 12 norm of gradients: 6.880397, 12 norm of weights: 4.027511\n",
            "Iteration Number: 355, Loss: 8.400070, 12 norm of gradients: 6.863881, 12 norm of weights: 4.022468\n",
            "Iteration Number: 356, Loss: 8.379283, 12 norm of gradients: 6.847420, 12 norm of weights: 4.017436\n",
            "Iteration Number: 357, Loss: 8.358568, 12 norm of gradients: 6.831012, 12 norm of weights: 4.012415\n",
            "Iteration Number: 358, Loss: 8.337927, 12 norm of gradients: 6.814659, 12 norm of weights: 4.007405\n",
            "Iteration Number: 359, Loss: 8.317358, 12 norm of gradients: 6.798358, 12 norm of weights: 4.002407\n",
            "Iteration Number: 360, Loss: 8.296861, 12 norm of gradients: 6.782110, 12 norm of weights: 3.997419\n",
            "Iteration Number: 361, Loss: 8.276436, 12 norm of gradients: 6.765915, 12 norm of weights: 3.992443\n",
            "Iteration Number: 362, Loss: 8.256083, 12 norm of gradients: 6.749773, 12 norm of weights: 3.987477\n",
            "Iteration Number: 363, Loss: 8.235801, 12 norm of gradients: 6.733682, 12 norm of weights: 3.982522\n",
            "Iteration Number: 364, Loss: 8.215590, 12 norm of gradients: 6.717642, 12 norm of weights: 3.977578\n",
            "Iteration Number: 365, Loss: 8.195450, 12 norm of gradients: 6.701654, 12 norm of weights: 3.972645\n",
            "Iteration Number: 366, Loss: 8.175380, 12 norm of gradients: 6.685717, 12 norm of weights: 3.967723\n",
            "Iteration Number: 367, Loss: 8.155381, 12 norm of gradients: 6.669830, 12 norm of weights: 3.962812\n",
            "Iteration Number: 368, Loss: 8.135451, 12 norm of gradients: 6.653994, 12 norm of weights: 3.957912\n",
            "Iteration Number: 369, Loss: 8.115591, 12 norm of gradients: 6.638208, 12 norm of weights: 3.953022\n",
            "Iteration Number: 370, Loss: 8.095801, 12 norm of gradients: 6.622471, 12 norm of weights: 3.948143\n",
            "Iteration Number: 371, Loss: 8.076080, 12 norm of gradients: 6.606783, 12 norm of weights: 3.943275\n",
            "Iteration Number: 372, Loss: 8.056427, 12 norm of gradients: 6.591145, 12 norm of weights: 3.938418\n",
            "Iteration Number: 373, Loss: 8.036843, 12 norm of gradients: 6.575555, 12 norm of weights: 3.933571\n",
            "Iteration Number: 374, Loss: 8.017327, 12 norm of gradients: 6.560014, 12 norm of weights: 3.928735\n",
            "Iteration Number: 375, Loss: 7.997880, 12 norm of gradients: 6.544521, 12 norm of weights: 3.923910\n",
            "Iteration Number: 376, Loss: 7.978500, 12 norm of gradients: 6.529076, 12 norm of weights: 3.919095\n",
            "Iteration Number: 377, Loss: 7.959187, 12 norm of gradients: 6.513678, 12 norm of weights: 3.914291\n",
            "Iteration Number: 378, Loss: 7.939942, 12 norm of gradients: 6.498328, 12 norm of weights: 3.909497\n",
            "Iteration Number: 379, Loss: 7.920763, 12 norm of gradients: 6.483025, 12 norm of weights: 3.904714\n",
            "Iteration Number: 380, Loss: 7.901652, 12 norm of gradients: 6.467768, 12 norm of weights: 3.899941\n",
            "Iteration Number: 381, Loss: 7.882606, 12 norm of gradients: 6.452558, 12 norm of weights: 3.895179\n",
            "Iteration Number: 382, Loss: 7.863627, 12 norm of gradients: 6.437394, 12 norm of weights: 3.890427\n",
            "Iteration Number: 383, Loss: 7.844714, 12 norm of gradients: 6.422276, 12 norm of weights: 3.885686\n",
            "Iteration Number: 384, Loss: 7.825866, 12 norm of gradients: 6.407204, 12 norm of weights: 3.880955\n",
            "Iteration Number: 385, Loss: 7.807083, 12 norm of gradients: 6.392178, 12 norm of weights: 3.876235\n",
            "Iteration Number: 386, Loss: 7.788366, 12 norm of gradients: 6.377196, 12 norm of weights: 3.871524\n",
            "Iteration Number: 387, Loss: 7.769714, 12 norm of gradients: 6.362259, 12 norm of weights: 3.866825\n",
            "Iteration Number: 388, Loss: 7.751126, 12 norm of gradients: 6.347367, 12 norm of weights: 3.862135\n",
            "Iteration Number: 389, Loss: 7.732602, 12 norm of gradients: 6.332520, 12 norm of weights: 3.857456\n",
            "Iteration Number: 390, Loss: 7.714143, 12 norm of gradients: 6.317717, 12 norm of weights: 3.852787\n",
            "Iteration Number: 391, Loss: 7.695747, 12 norm of gradients: 6.302957, 12 norm of weights: 3.848128\n",
            "Iteration Number: 392, Loss: 7.677415, 12 norm of gradients: 6.288242, 12 norm of weights: 3.843480\n",
            "Iteration Number: 393, Loss: 7.659146, 12 norm of gradients: 6.273570, 12 norm of weights: 3.838842\n",
            "Iteration Number: 394, Loss: 7.640940, 12 norm of gradients: 6.258941, 12 norm of weights: 3.834214\n",
            "Iteration Number: 395, Loss: 7.622798, 12 norm of gradients: 6.244355, 12 norm of weights: 3.829596\n",
            "Iteration Number: 396, Loss: 7.604717, 12 norm of gradients: 6.229812, 12 norm of weights: 3.824988\n",
            "Iteration Number: 397, Loss: 7.586699, 12 norm of gradients: 6.215312, 12 norm of weights: 3.820390\n",
            "Iteration Number: 398, Loss: 7.568744, 12 norm of gradients: 6.200854, 12 norm of weights: 3.815802\n",
            "Iteration Number: 399, Loss: 7.550850, 12 norm of gradients: 6.186438, 12 norm of weights: 3.811225\n",
            "Iteration Number: 400, Loss: 7.533017, 12 norm of gradients: 6.172064, 12 norm of weights: 3.806657\n",
            "Iteration Number: 401, Loss: 7.515246, 12 norm of gradients: 6.157732, 12 norm of weights: 3.802100\n",
            "Iteration Number: 402, Loss: 7.497537, 12 norm of gradients: 6.143442, 12 norm of weights: 3.797552\n",
            "Iteration Number: 403, Loss: 7.479888, 12 norm of gradients: 6.129193, 12 norm of weights: 3.793015\n",
            "Iteration Number: 404, Loss: 7.462299, 12 norm of gradients: 6.114985, 12 norm of weights: 3.788487\n",
            "Iteration Number: 405, Loss: 7.444772, 12 norm of gradients: 6.100818, 12 norm of weights: 3.783969\n",
            "Iteration Number: 406, Loss: 7.427304, 12 norm of gradients: 6.086692, 12 norm of weights: 3.779461\n",
            "Iteration Number: 407, Loss: 7.409896, 12 norm of gradients: 6.072606, 12 norm of weights: 3.774963\n",
            "Iteration Number: 408, Loss: 7.392548, 12 norm of gradients: 6.058561, 12 norm of weights: 3.770475\n",
            "Iteration Number: 409, Loss: 7.375260, 12 norm of gradients: 6.044556, 12 norm of weights: 3.765997\n",
            "Iteration Number: 410, Loss: 7.358031, 12 norm of gradients: 6.030591, 12 norm of weights: 3.761528\n",
            "Iteration Number: 411, Loss: 7.340861, 12 norm of gradients: 6.016666, 12 norm of weights: 3.757070\n",
            "Iteration Number: 412, Loss: 7.323749, 12 norm of gradients: 6.002781, 12 norm of weights: 3.752621\n",
            "Iteration Number: 413, Loss: 7.306697, 12 norm of gradients: 5.988935, 12 norm of weights: 3.748181\n",
            "Iteration Number: 414, Loss: 7.289702, 12 norm of gradients: 5.975128, 12 norm of weights: 3.743752\n",
            "Iteration Number: 415, Loss: 7.272766, 12 norm of gradients: 5.961361, 12 norm of weights: 3.739332\n",
            "Iteration Number: 416, Loss: 7.255888, 12 norm of gradients: 5.947633, 12 norm of weights: 3.734922\n",
            "Iteration Number: 417, Loss: 7.239067, 12 norm of gradients: 5.933943, 12 norm of weights: 3.730521\n",
            "Iteration Number: 418, Loss: 7.222304, 12 norm of gradients: 5.920292, 12 norm of weights: 3.726130\n",
            "Iteration Number: 419, Loss: 7.205598, 12 norm of gradients: 5.906680, 12 norm of weights: 3.721749\n",
            "Iteration Number: 420, Loss: 7.188949, 12 norm of gradients: 5.893106, 12 norm of weights: 3.717377\n",
            "Iteration Number: 421, Loss: 7.172357, 12 norm of gradients: 5.879570, 12 norm of weights: 3.713015\n",
            "Iteration Number: 422, Loss: 7.155821, 12 norm of gradients: 5.866073, 12 norm of weights: 3.708663\n",
            "Iteration Number: 423, Loss: 7.139342, 12 norm of gradients: 5.852613, 12 norm of weights: 3.704320\n",
            "Iteration Number: 424, Loss: 7.122919, 12 norm of gradients: 5.839191, 12 norm of weights: 3.699986\n",
            "Iteration Number: 425, Loss: 7.106552, 12 norm of gradients: 5.825806, 12 norm of weights: 3.695662\n",
            "Iteration Number: 426, Loss: 7.090241, 12 norm of gradients: 5.812459, 12 norm of weights: 3.691347\n",
            "Iteration Number: 427, Loss: 7.073985, 12 norm of gradients: 5.799150, 12 norm of weights: 3.687042\n",
            "Iteration Number: 428, Loss: 7.057785, 12 norm of gradients: 5.785877, 12 norm of weights: 3.682746\n",
            "Iteration Number: 429, Loss: 7.041639, 12 norm of gradients: 5.772642, 12 norm of weights: 3.678460\n",
            "Iteration Number: 430, Loss: 7.025549, 12 norm of gradients: 5.759444, 12 norm of weights: 3.674183\n",
            "Iteration Number: 431, Loss: 7.009513, 12 norm of gradients: 5.746282, 12 norm of weights: 3.669915\n",
            "Iteration Number: 432, Loss: 6.993531, 12 norm of gradients: 5.733157, 12 norm of weights: 3.665657\n",
            "Iteration Number: 433, Loss: 6.977604, 12 norm of gradients: 5.720069, 12 norm of weights: 3.661408\n",
            "Iteration Number: 434, Loss: 6.961731, 12 norm of gradients: 5.707017, 12 norm of weights: 3.657168\n",
            "Iteration Number: 435, Loss: 6.945912, 12 norm of gradients: 5.694001, 12 norm of weights: 3.652937\n",
            "Iteration Number: 436, Loss: 6.930146, 12 norm of gradients: 5.681021, 12 norm of weights: 3.648716\n",
            "Iteration Number: 437, Loss: 6.914434, 12 norm of gradients: 5.668078, 12 norm of weights: 3.644504\n",
            "Iteration Number: 438, Loss: 6.898775, 12 norm of gradients: 5.655170, 12 norm of weights: 3.640301\n",
            "Iteration Number: 439, Loss: 6.883169, 12 norm of gradients: 5.642298, 12 norm of weights: 3.636107\n",
            "Iteration Number: 440, Loss: 6.867616, 12 norm of gradients: 5.629462, 12 norm of weights: 3.631923\n",
            "Iteration Number: 441, Loss: 6.852116, 12 norm of gradients: 5.616662, 12 norm of weights: 3.627748\n",
            "Iteration Number: 442, Loss: 6.836668, 12 norm of gradients: 5.603897, 12 norm of weights: 3.623581\n",
            "Iteration Number: 443, Loss: 6.821272, 12 norm of gradients: 5.591167, 12 norm of weights: 3.619424\n",
            "Iteration Number: 444, Loss: 6.805928, 12 norm of gradients: 5.578472, 12 norm of weights: 3.615276\n",
            "Iteration Number: 445, Loss: 6.790636, 12 norm of gradients: 5.565813, 12 norm of weights: 3.611137\n",
            "Iteration Number: 446, Loss: 6.775396, 12 norm of gradients: 5.553188, 12 norm of weights: 3.607007\n",
            "Iteration Number: 447, Loss: 6.760207, 12 norm of gradients: 5.540599, 12 norm of weights: 3.602886\n",
            "Iteration Number: 448, Loss: 6.745070, 12 norm of gradients: 5.528044, 12 norm of weights: 3.598774\n",
            "Iteration Number: 449, Loss: 6.729983, 12 norm of gradients: 5.515524, 12 norm of weights: 3.594671\n",
            "Iteration Number: 450, Loss: 6.714948, 12 norm of gradients: 5.503039, 12 norm of weights: 3.590577\n",
            "Iteration Number: 451, Loss: 6.699963, 12 norm of gradients: 5.490588, 12 norm of weights: 3.586492\n",
            "Iteration Number: 452, Loss: 6.685028, 12 norm of gradients: 5.478172, 12 norm of weights: 3.582416\n",
            "Iteration Number: 453, Loss: 6.670144, 12 norm of gradients: 5.465790, 12 norm of weights: 3.578349\n",
            "Iteration Number: 454, Loss: 6.655310, 12 norm of gradients: 5.453442, 12 norm of weights: 3.574291\n",
            "Iteration Number: 455, Loss: 6.640526, 12 norm of gradients: 5.441128, 12 norm of weights: 3.570241\n",
            "Iteration Number: 456, Loss: 6.625791, 12 norm of gradients: 5.428848, 12 norm of weights: 3.566201\n",
            "Iteration Number: 457, Loss: 6.611106, 12 norm of gradients: 5.416602, 12 norm of weights: 3.562169\n",
            "Iteration Number: 458, Loss: 6.596471, 12 norm of gradients: 5.404390, 12 norm of weights: 3.558146\n",
            "Iteration Number: 459, Loss: 6.581884, 12 norm of gradients: 5.392211, 12 norm of weights: 3.554132\n",
            "Iteration Number: 460, Loss: 6.567347, 12 norm of gradients: 5.380067, 12 norm of weights: 3.550126\n",
            "Iteration Number: 461, Loss: 6.552858, 12 norm of gradients: 5.367956, 12 norm of weights: 3.546129\n",
            "Iteration Number: 462, Loss: 6.538418, 12 norm of gradients: 5.355878, 12 norm of weights: 3.542141\n",
            "Iteration Number: 463, Loss: 6.524027, 12 norm of gradients: 5.343833, 12 norm of weights: 3.538162\n",
            "Iteration Number: 464, Loss: 6.509683, 12 norm of gradients: 5.331822, 12 norm of weights: 3.534192\n",
            "Iteration Number: 465, Loss: 6.495388, 12 norm of gradients: 5.319844, 12 norm of weights: 3.530230\n",
            "Iteration Number: 466, Loss: 6.481141, 12 norm of gradients: 5.307899, 12 norm of weights: 3.526276\n",
            "Iteration Number: 467, Loss: 6.466941, 12 norm of gradients: 5.295988, 12 norm of weights: 3.522332\n",
            "Iteration Number: 468, Loss: 6.452789, 12 norm of gradients: 5.284109, 12 norm of weights: 3.518396\n",
            "Iteration Number: 469, Loss: 6.438684, 12 norm of gradients: 5.272263, 12 norm of weights: 3.514468\n",
            "Iteration Number: 470, Loss: 6.424627, 12 norm of gradients: 5.260449, 12 norm of weights: 3.510550\n",
            "Iteration Number: 471, Loss: 6.410616, 12 norm of gradients: 5.248669, 12 norm of weights: 3.506639\n",
            "Iteration Number: 472, Loss: 6.396653, 12 norm of gradients: 5.236921, 12 norm of weights: 3.502738\n",
            "Iteration Number: 473, Loss: 6.382736, 12 norm of gradients: 5.225205, 12 norm of weights: 3.498844\n",
            "Iteration Number: 474, Loss: 6.368865, 12 norm of gradients: 5.213522, 12 norm of weights: 3.494960\n",
            "Iteration Number: 475, Loss: 6.355041, 12 norm of gradients: 5.201871, 12 norm of weights: 3.491083\n",
            "Iteration Number: 476, Loss: 6.341263, 12 norm of gradients: 5.190252, 12 norm of weights: 3.487216\n",
            "Iteration Number: 477, Loss: 6.327531, 12 norm of gradients: 5.178666, 12 norm of weights: 3.483356\n",
            "Iteration Number: 478, Loss: 6.313845, 12 norm of gradients: 5.167111, 12 norm of weights: 3.479506\n",
            "Iteration Number: 479, Loss: 6.300204, 12 norm of gradients: 5.155589, 12 norm of weights: 3.475663\n",
            "Iteration Number: 480, Loss: 6.286609, 12 norm of gradients: 5.144098, 12 norm of weights: 3.471829\n",
            "Iteration Number: 481, Loss: 6.273059, 12 norm of gradients: 5.132639, 12 norm of weights: 3.468003\n",
            "Iteration Number: 482, Loss: 6.259554, 12 norm of gradients: 5.121212, 12 norm of weights: 3.464186\n",
            "Iteration Number: 483, Loss: 6.246094, 12 norm of gradients: 5.109817, 12 norm of weights: 3.460377\n",
            "Iteration Number: 484, Loss: 6.232679, 12 norm of gradients: 5.098453, 12 norm of weights: 3.456576\n",
            "Iteration Number: 485, Loss: 6.219309, 12 norm of gradients: 5.087121, 12 norm of weights: 3.452784\n",
            "Iteration Number: 486, Loss: 6.205983, 12 norm of gradients: 5.075820, 12 norm of weights: 3.449000\n",
            "Iteration Number: 487, Loss: 6.192702, 12 norm of gradients: 5.064550, 12 norm of weights: 3.445224\n",
            "Iteration Number: 488, Loss: 6.179464, 12 norm of gradients: 5.053312, 12 norm of weights: 3.441456\n",
            "Iteration Number: 489, Loss: 6.166271, 12 norm of gradients: 5.042105, 12 norm of weights: 3.437697\n",
            "Iteration Number: 490, Loss: 6.153121, 12 norm of gradients: 5.030928, 12 norm of weights: 3.433946\n",
            "Iteration Number: 491, Loss: 6.140015, 12 norm of gradients: 5.019783, 12 norm of weights: 3.430203\n",
            "Iteration Number: 492, Loss: 6.126952, 12 norm of gradients: 5.008669, 12 norm of weights: 3.426468\n",
            "Iteration Number: 493, Loss: 6.113933, 12 norm of gradients: 4.997585, 12 norm of weights: 3.422742\n",
            "Iteration Number: 494, Loss: 6.100957, 12 norm of gradients: 4.986532, 12 norm of weights: 3.419023\n",
            "Iteration Number: 495, Loss: 6.088024, 12 norm of gradients: 4.975510, 12 norm of weights: 3.415313\n",
            "Iteration Number: 496, Loss: 6.075134, 12 norm of gradients: 4.964518, 12 norm of weights: 3.411611\n",
            "Iteration Number: 497, Loss: 6.062287, 12 norm of gradients: 4.953557, 12 norm of weights: 3.407917\n",
            "Iteration Number: 498, Loss: 6.049482, 12 norm of gradients: 4.942626, 12 norm of weights: 3.404231\n",
            "Iteration Number: 499, Loss: 6.036720, 12 norm of gradients: 4.931726, 12 norm of weights: 3.400553\n",
            "Iteration Number: 500, Loss: 6.024000, 12 norm of gradients: 4.920855, 12 norm of weights: 3.396883\n",
            "Iteration Number: 501, Loss: 6.011321, 12 norm of gradients: 4.910015, 12 norm of weights: 3.393221\n",
            "Iteration Number: 502, Loss: 5.998685, 12 norm of gradients: 4.899205, 12 norm of weights: 3.389567\n",
            "Iteration Number: 503, Loss: 5.986091, 12 norm of gradients: 4.888425, 12 norm of weights: 3.385921\n",
            "Iteration Number: 504, Loss: 5.973538, 12 norm of gradients: 4.877674, 12 norm of weights: 3.382284\n",
            "Iteration Number: 505, Loss: 5.961027, 12 norm of gradients: 4.866954, 12 norm of weights: 3.378654\n",
            "Iteration Number: 506, Loss: 5.948557, 12 norm of gradients: 4.856263, 12 norm of weights: 3.375032\n",
            "Iteration Number: 507, Loss: 5.936128, 12 norm of gradients: 4.845601, 12 norm of weights: 3.371418\n",
            "Iteration Number: 508, Loss: 5.923741, 12 norm of gradients: 4.834970, 12 norm of weights: 3.367811\n",
            "Iteration Number: 509, Loss: 5.911394, 12 norm of gradients: 4.824367, 12 norm of weights: 3.364213\n",
            "Iteration Number: 510, Loss: 5.899088, 12 norm of gradients: 4.813794, 12 norm of weights: 3.360623\n",
            "Iteration Number: 511, Loss: 5.886822, 12 norm of gradients: 4.803250, 12 norm of weights: 3.357040\n",
            "Iteration Number: 512, Loss: 5.874597, 12 norm of gradients: 4.792735, 12 norm of weights: 3.353466\n",
            "Iteration Number: 513, Loss: 5.862412, 12 norm of gradients: 4.782250, 12 norm of weights: 3.349899\n",
            "Iteration Number: 514, Loss: 5.850268, 12 norm of gradients: 4.771793, 12 norm of weights: 3.346340\n",
            "Iteration Number: 515, Loss: 5.838163, 12 norm of gradients: 4.761365, 12 norm of weights: 3.342789\n",
            "Iteration Number: 516, Loss: 5.826098, 12 norm of gradients: 4.750966, 12 norm of weights: 3.339245\n",
            "Iteration Number: 517, Loss: 5.814074, 12 norm of gradients: 4.740596, 12 norm of weights: 3.335710\n",
            "Iteration Number: 518, Loss: 5.802088, 12 norm of gradients: 4.730254, 12 norm of weights: 3.332182\n",
            "Iteration Number: 519, Loss: 5.790142, 12 norm of gradients: 4.719940, 12 norm of weights: 3.328662\n",
            "Iteration Number: 520, Loss: 5.778235, 12 norm of gradients: 4.709656, 12 norm of weights: 3.325149\n",
            "Iteration Number: 521, Loss: 5.766368, 12 norm of gradients: 4.699399, 12 norm of weights: 3.321645\n",
            "Iteration Number: 522, Loss: 5.754539, 12 norm of gradients: 4.689171, 12 norm of weights: 3.318147\n",
            "Iteration Number: 523, Loss: 5.742750, 12 norm of gradients: 4.678970, 12 norm of weights: 3.314658\n",
            "Iteration Number: 524, Loss: 5.730999, 12 norm of gradients: 4.668798, 12 norm of weights: 3.311176\n",
            "Iteration Number: 525, Loss: 5.719287, 12 norm of gradients: 4.658654, 12 norm of weights: 3.307702\n",
            "Iteration Number: 526, Loss: 5.707613, 12 norm of gradients: 4.648538, 12 norm of weights: 3.304236\n",
            "Iteration Number: 527, Loss: 5.695977, 12 norm of gradients: 4.638449, 12 norm of weights: 3.300777\n",
            "Iteration Number: 528, Loss: 5.684380, 12 norm of gradients: 4.628388, 12 norm of weights: 3.297326\n",
            "Iteration Number: 529, Loss: 5.672821, 12 norm of gradients: 4.618354, 12 norm of weights: 3.293882\n",
            "Iteration Number: 530, Loss: 5.661300, 12 norm of gradients: 4.608349, 12 norm of weights: 3.290446\n",
            "Iteration Number: 531, Loss: 5.649816, 12 norm of gradients: 4.598370, 12 norm of weights: 3.287018\n",
            "Iteration Number: 532, Loss: 5.638370, 12 norm of gradients: 4.588419, 12 norm of weights: 3.283597\n",
            "Iteration Number: 533, Loss: 5.626962, 12 norm of gradients: 4.578494, 12 norm of weights: 3.280183\n",
            "Iteration Number: 534, Loss: 5.615591, 12 norm of gradients: 4.568597, 12 norm of weights: 3.276777\n",
            "Iteration Number: 535, Loss: 5.604257, 12 norm of gradients: 4.558727, 12 norm of weights: 3.273379\n",
            "Iteration Number: 536, Loss: 5.592961, 12 norm of gradients: 4.548884, 12 norm of weights: 3.269988\n",
            "Iteration Number: 537, Loss: 5.581701, 12 norm of gradients: 4.539067, 12 norm of weights: 3.266604\n",
            "Iteration Number: 538, Loss: 5.570479, 12 norm of gradients: 4.529278, 12 norm of weights: 3.263228\n",
            "Iteration Number: 539, Loss: 5.559293, 12 norm of gradients: 4.519514, 12 norm of weights: 3.259860\n",
            "Iteration Number: 540, Loss: 5.548143, 12 norm of gradients: 4.509778, 12 norm of weights: 3.256499\n",
            "Iteration Number: 541, Loss: 5.537031, 12 norm of gradients: 4.500067, 12 norm of weights: 3.253145\n",
            "Iteration Number: 542, Loss: 5.525954, 12 norm of gradients: 4.490383, 12 norm of weights: 3.249798\n",
            "Iteration Number: 543, Loss: 5.514914, 12 norm of gradients: 4.480725, 12 norm of weights: 3.246459\n",
            "Iteration Number: 544, Loss: 5.503910, 12 norm of gradients: 4.471094, 12 norm of weights: 3.243128\n",
            "Iteration Number: 545, Loss: 5.492941, 12 norm of gradients: 4.461488, 12 norm of weights: 3.239803\n",
            "Iteration Number: 546, Loss: 5.482009, 12 norm of gradients: 4.451908, 12 norm of weights: 3.236486\n",
            "Iteration Number: 547, Loss: 5.471113, 12 norm of gradients: 4.442354, 12 norm of weights: 3.233177\n",
            "Iteration Number: 548, Loss: 5.460252, 12 norm of gradients: 4.432826, 12 norm of weights: 3.229874\n",
            "Iteration Number: 549, Loss: 5.449426, 12 norm of gradients: 4.423323, 12 norm of weights: 3.226579\n",
            "Iteration Number: 550, Loss: 5.438636, 12 norm of gradients: 4.413845, 12 norm of weights: 3.223292\n",
            "Iteration Number: 551, Loss: 5.427881, 12 norm of gradients: 4.404393, 12 norm of weights: 3.220011\n",
            "Iteration Number: 552, Loss: 5.417161, 12 norm of gradients: 4.394967, 12 norm of weights: 3.216738\n",
            "Iteration Number: 553, Loss: 5.406476, 12 norm of gradients: 4.385565, 12 norm of weights: 3.213472\n",
            "Iteration Number: 554, Loss: 5.395826, 12 norm of gradients: 4.376189, 12 norm of weights: 3.210213\n",
            "Iteration Number: 555, Loss: 5.385211, 12 norm of gradients: 4.366838, 12 norm of weights: 3.206961\n",
            "Iteration Number: 556, Loss: 5.374630, 12 norm of gradients: 4.357511, 12 norm of weights: 3.203717\n",
            "Iteration Number: 557, Loss: 5.364084, 12 norm of gradients: 4.348209, 12 norm of weights: 3.200480\n",
            "Iteration Number: 558, Loss: 5.353572, 12 norm of gradients: 4.338932, 12 norm of weights: 3.197250\n",
            "Iteration Number: 559, Loss: 5.343095, 12 norm of gradients: 4.329680, 12 norm of weights: 3.194027\n",
            "Iteration Number: 560, Loss: 5.332652, 12 norm of gradients: 4.320452, 12 norm of weights: 3.190811\n",
            "Iteration Number: 561, Loss: 5.322242, 12 norm of gradients: 4.311248, 12 norm of weights: 3.187602\n",
            "Iteration Number: 562, Loss: 5.311867, 12 norm of gradients: 4.302069, 12 norm of weights: 3.184401\n",
            "Iteration Number: 563, Loss: 5.301526, 12 norm of gradients: 4.292914, 12 norm of weights: 3.181207\n",
            "Iteration Number: 564, Loss: 5.291218, 12 norm of gradients: 4.283783, 12 norm of weights: 3.178019\n",
            "Iteration Number: 565, Loss: 5.280943, 12 norm of gradients: 4.274676, 12 norm of weights: 3.174839\n",
            "Iteration Number: 566, Loss: 5.270703, 12 norm of gradients: 4.265593, 12 norm of weights: 3.171666\n",
            "Iteration Number: 567, Loss: 5.260495, 12 norm of gradients: 4.256533, 12 norm of weights: 3.168500\n",
            "Iteration Number: 568, Loss: 5.250321, 12 norm of gradients: 4.247498, 12 norm of weights: 3.165341\n",
            "Iteration Number: 569, Loss: 5.240180, 12 norm of gradients: 4.238486, 12 norm of weights: 3.162189\n",
            "Iteration Number: 570, Loss: 5.230072, 12 norm of gradients: 4.229497, 12 norm of weights: 3.159044\n",
            "Iteration Number: 571, Loss: 5.219996, 12 norm of gradients: 4.220532, 12 norm of weights: 3.155905\n",
            "Iteration Number: 572, Loss: 5.209954, 12 norm of gradients: 4.211590, 12 norm of weights: 3.152774\n",
            "Iteration Number: 573, Loss: 5.199944, 12 norm of gradients: 4.202671, 12 norm of weights: 3.149650\n",
            "Iteration Number: 574, Loss: 5.189967, 12 norm of gradients: 4.193775, 12 norm of weights: 3.146533\n",
            "Iteration Number: 575, Loss: 5.180022, 12 norm of gradients: 4.184903, 12 norm of weights: 3.143423\n",
            "Iteration Number: 576, Loss: 5.170110, 12 norm of gradients: 4.176053, 12 norm of weights: 3.140320\n",
            "Iteration Number: 577, Loss: 5.160229, 12 norm of gradients: 4.167226, 12 norm of weights: 3.137223\n",
            "Iteration Number: 578, Loss: 5.150381, 12 norm of gradients: 4.158422, 12 norm of weights: 3.134134\n",
            "Iteration Number: 579, Loss: 5.140565, 12 norm of gradients: 4.149640, 12 norm of weights: 3.131051\n",
            "Iteration Number: 580, Loss: 5.130781, 12 norm of gradients: 4.140881, 12 norm of weights: 3.127976\n",
            "Iteration Number: 581, Loss: 5.121029, 12 norm of gradients: 4.132144, 12 norm of weights: 3.124907\n",
            "Iteration Number: 582, Loss: 5.111308, 12 norm of gradients: 4.123429, 12 norm of weights: 3.121845\n",
            "Iteration Number: 583, Loss: 5.101619, 12 norm of gradients: 4.114737, 12 norm of weights: 3.118790\n",
            "Iteration Number: 584, Loss: 5.091961, 12 norm of gradients: 4.106067, 12 norm of weights: 3.115742\n",
            "Iteration Number: 585, Loss: 5.082335, 12 norm of gradients: 4.097418, 12 norm of weights: 3.112700\n",
            "Iteration Number: 586, Loss: 5.072740, 12 norm of gradients: 4.088792, 12 norm of weights: 3.109666\n",
            "Iteration Number: 587, Loss: 5.063176, 12 norm of gradients: 4.080188, 12 norm of weights: 3.106638\n",
            "Iteration Number: 588, Loss: 5.053643, 12 norm of gradients: 4.071605, 12 norm of weights: 3.103617\n",
            "Iteration Number: 589, Loss: 5.044142, 12 norm of gradients: 4.063044, 12 norm of weights: 3.100603\n",
            "Iteration Number: 590, Loss: 5.034671, 12 norm of gradients: 4.054504, 12 norm of weights: 3.097595\n",
            "Iteration Number: 591, Loss: 5.025230, 12 norm of gradients: 4.045986, 12 norm of weights: 3.094594\n",
            "Iteration Number: 592, Loss: 5.015821, 12 norm of gradients: 4.037489, 12 norm of weights: 3.091600\n",
            "Iteration Number: 593, Loss: 5.006442, 12 norm of gradients: 4.029013, 12 norm of weights: 3.088613\n",
            "Iteration Number: 594, Loss: 4.997093, 12 norm of gradients: 4.020559, 12 norm of weights: 3.085633\n",
            "Iteration Number: 595, Loss: 4.987775, 12 norm of gradients: 4.012125, 12 norm of weights: 3.082659\n",
            "Iteration Number: 596, Loss: 4.978487, 12 norm of gradients: 4.003713, 12 norm of weights: 3.079692\n",
            "Iteration Number: 597, Loss: 4.969229, 12 norm of gradients: 3.995321, 12 norm of weights: 3.076731\n",
            "Iteration Number: 598, Loss: 4.960001, 12 norm of gradients: 3.986950, 12 norm of weights: 3.073777\n",
            "Iteration Number: 599, Loss: 4.950803, 12 norm of gradients: 3.978600, 12 norm of weights: 3.070830\n",
            "Iteration Number: 600, Loss: 4.941635, 12 norm of gradients: 3.970271, 12 norm of weights: 3.067890\n",
            "Iteration Number: 601, Loss: 4.932497, 12 norm of gradients: 3.961962, 12 norm of weights: 3.064956\n",
            "Iteration Number: 602, Loss: 4.923389, 12 norm of gradients: 3.953673, 12 norm of weights: 3.062028\n",
            "Iteration Number: 603, Loss: 4.914309, 12 norm of gradients: 3.945405, 12 norm of weights: 3.059108\n",
            "Iteration Number: 604, Loss: 4.905260, 12 norm of gradients: 3.937156, 12 norm of weights: 3.056194\n",
            "Iteration Number: 605, Loss: 4.896239, 12 norm of gradients: 3.928929, 12 norm of weights: 3.053286\n",
            "Iteration Number: 606, Loss: 4.887248, 12 norm of gradients: 3.920721, 12 norm of weights: 3.050385\n",
            "Iteration Number: 607, Loss: 4.878287, 12 norm of gradients: 3.912533, 12 norm of weights: 3.047491\n",
            "Iteration Number: 608, Loss: 4.869354, 12 norm of gradients: 3.904364, 12 norm of weights: 3.044603\n",
            "Iteration Number: 609, Loss: 4.860450, 12 norm of gradients: 3.896216, 12 norm of weights: 3.041722\n",
            "Iteration Number: 610, Loss: 4.851575, 12 norm of gradients: 3.888088, 12 norm of weights: 3.038847\n",
            "Iteration Number: 611, Loss: 4.842729, 12 norm of gradients: 3.879978, 12 norm of weights: 3.035979\n",
            "Iteration Number: 612, Loss: 4.833911, 12 norm of gradients: 3.871889, 12 norm of weights: 3.033117\n",
            "Iteration Number: 613, Loss: 4.825122, 12 norm of gradients: 3.863819, 12 norm of weights: 3.030262\n",
            "Iteration Number: 614, Loss: 4.816362, 12 norm of gradients: 3.855768, 12 norm of weights: 3.027413\n",
            "Iteration Number: 615, Loss: 4.807630, 12 norm of gradients: 3.847737, 12 norm of weights: 3.024571\n",
            "Iteration Number: 616, Loss: 4.798926, 12 norm of gradients: 3.839724, 12 norm of weights: 3.021735\n",
            "Iteration Number: 617, Loss: 4.790251, 12 norm of gradients: 3.831731, 12 norm of weights: 3.018906\n",
            "Iteration Number: 618, Loss: 4.781603, 12 norm of gradients: 3.823757, 12 norm of weights: 3.016083\n",
            "Iteration Number: 619, Loss: 4.772984, 12 norm of gradients: 3.815801, 12 norm of weights: 3.013266\n",
            "Iteration Number: 620, Loss: 4.764393, 12 norm of gradients: 3.807865, 12 norm of weights: 3.010456\n",
            "Iteration Number: 621, Loss: 4.755829, 12 norm of gradients: 3.799947, 12 norm of weights: 3.007653\n",
            "Iteration Number: 622, Loss: 4.747293, 12 norm of gradients: 3.792048, 12 norm of weights: 3.004855\n",
            "Iteration Number: 623, Loss: 4.738785, 12 norm of gradients: 3.784168, 12 norm of weights: 3.002064\n",
            "Iteration Number: 624, Loss: 4.730305, 12 norm of gradients: 3.776306, 12 norm of weights: 2.999280\n",
            "Iteration Number: 625, Loss: 4.721852, 12 norm of gradients: 3.768462, 12 norm of weights: 2.996502\n",
            "Iteration Number: 626, Loss: 4.713426, 12 norm of gradients: 3.760637, 12 norm of weights: 2.993730\n",
            "Iteration Number: 627, Loss: 4.705028, 12 norm of gradients: 3.752830, 12 norm of weights: 2.990965\n",
            "Iteration Number: 628, Loss: 4.696657, 12 norm of gradients: 3.745041, 12 norm of weights: 2.988205\n",
            "Iteration Number: 629, Loss: 4.688313, 12 norm of gradients: 3.737270, 12 norm of weights: 2.985453\n",
            "Iteration Number: 630, Loss: 4.679996, 12 norm of gradients: 3.729518, 12 norm of weights: 2.982706\n",
            "Iteration Number: 631, Loss: 4.671706, 12 norm of gradients: 3.721783, 12 norm of weights: 2.979966\n",
            "Iteration Number: 632, Loss: 4.663443, 12 norm of gradients: 3.714066, 12 norm of weights: 2.977232\n",
            "Iteration Number: 633, Loss: 4.655207, 12 norm of gradients: 3.706367, 12 norm of weights: 2.974504\n",
            "Iteration Number: 634, Loss: 4.646998, 12 norm of gradients: 3.698685, 12 norm of weights: 2.971783\n",
            "Iteration Number: 635, Loss: 4.638815, 12 norm of gradients: 3.691022, 12 norm of weights: 2.969068\n",
            "Iteration Number: 636, Loss: 4.630658, 12 norm of gradients: 3.683375, 12 norm of weights: 2.966359\n",
            "Iteration Number: 637, Loss: 4.622528, 12 norm of gradients: 3.675747, 12 norm of weights: 2.963656\n",
            "Iteration Number: 638, Loss: 4.614425, 12 norm of gradients: 3.668135, 12 norm of weights: 2.960960\n",
            "Iteration Number: 639, Loss: 4.606348, 12 norm of gradients: 3.660541, 12 norm of weights: 2.958270\n",
            "Iteration Number: 640, Loss: 4.598297, 12 norm of gradients: 3.652965, 12 norm of weights: 2.955586\n",
            "Iteration Number: 641, Loss: 4.590272, 12 norm of gradients: 3.645405, 12 norm of weights: 2.952908\n",
            "Iteration Number: 642, Loss: 4.582273, 12 norm of gradients: 3.637863, 12 norm of weights: 2.950236\n",
            "Iteration Number: 643, Loss: 4.574300, 12 norm of gradients: 3.630337, 12 norm of weights: 2.947571\n",
            "Iteration Number: 644, Loss: 4.566353, 12 norm of gradients: 3.622829, 12 norm of weights: 2.944912\n",
            "Iteration Number: 645, Loss: 4.558431, 12 norm of gradients: 3.615337, 12 norm of weights: 2.942258\n",
            "Iteration Number: 646, Loss: 4.550536, 12 norm of gradients: 3.607862, 12 norm of weights: 2.939611\n",
            "Iteration Number: 647, Loss: 4.542666, 12 norm of gradients: 3.600404, 12 norm of weights: 2.936971\n",
            "Iteration Number: 648, Loss: 4.534821, 12 norm of gradients: 3.592963, 12 norm of weights: 2.934336\n",
            "Iteration Number: 649, Loss: 4.527002, 12 norm of gradients: 3.585538, 12 norm of weights: 2.931707\n",
            "Iteration Number: 650, Loss: 4.519208, 12 norm of gradients: 3.578130, 12 norm of weights: 2.929085\n",
            "Iteration Number: 651, Loss: 4.511440, 12 norm of gradients: 3.570738, 12 norm of weights: 2.926468\n",
            "Iteration Number: 652, Loss: 4.503697, 12 norm of gradients: 3.563363, 12 norm of weights: 2.923858\n",
            "Iteration Number: 653, Loss: 4.495979, 12 norm of gradients: 3.556004, 12 norm of weights: 2.921253\n",
            "Iteration Number: 654, Loss: 4.488285, 12 norm of gradients: 3.548661, 12 norm of weights: 2.918655\n",
            "Iteration Number: 655, Loss: 4.480617, 12 norm of gradients: 3.541334, 12 norm of weights: 2.916063\n",
            "Iteration Number: 656, Loss: 4.472974, 12 norm of gradients: 3.534024, 12 norm of weights: 2.913477\n",
            "Iteration Number: 657, Loss: 4.465356, 12 norm of gradients: 3.526729, 12 norm of weights: 2.910897\n",
            "Iteration Number: 658, Loss: 4.457762, 12 norm of gradients: 3.519451, 12 norm of weights: 2.908323\n",
            "Iteration Number: 659, Loss: 4.450193, 12 norm of gradients: 3.512188, 12 norm of weights: 2.905755\n",
            "Iteration Number: 660, Loss: 4.442649, 12 norm of gradients: 3.504942, 12 norm of weights: 2.903192\n",
            "Iteration Number: 661, Loss: 4.435129, 12 norm of gradients: 3.497711, 12 norm of weights: 2.900636\n",
            "Iteration Number: 662, Loss: 4.427633, 12 norm of gradients: 3.490496, 12 norm of weights: 2.898086\n",
            "Iteration Number: 663, Loss: 4.420162, 12 norm of gradients: 3.483297, 12 norm of weights: 2.895542\n",
            "Iteration Number: 664, Loss: 4.412715, 12 norm of gradients: 3.476113, 12 norm of weights: 2.893004\n",
            "Iteration Number: 665, Loss: 4.405293, 12 norm of gradients: 3.468944, 12 norm of weights: 2.890472\n",
            "Iteration Number: 666, Loss: 4.397894, 12 norm of gradients: 3.461792, 12 norm of weights: 2.887945\n",
            "Iteration Number: 667, Loss: 4.390519, 12 norm of gradients: 3.454654, 12 norm of weights: 2.885425\n",
            "Iteration Number: 668, Loss: 4.383169, 12 norm of gradients: 3.447532, 12 norm of weights: 2.882910\n",
            "Iteration Number: 669, Loss: 4.375842, 12 norm of gradients: 3.440426, 12 norm of weights: 2.880402\n",
            "Iteration Number: 670, Loss: 4.368539, 12 norm of gradients: 3.433334, 12 norm of weights: 2.877899\n",
            "Iteration Number: 671, Loss: 4.361260, 12 norm of gradients: 3.426258, 12 norm of weights: 2.875402\n",
            "Iteration Number: 672, Loss: 4.354004, 12 norm of gradients: 3.419197, 12 norm of weights: 2.872912\n",
            "Iteration Number: 673, Loss: 4.346772, 12 norm of gradients: 3.412151, 12 norm of weights: 2.870427\n",
            "Iteration Number: 674, Loss: 4.339564, 12 norm of gradients: 3.405119, 12 norm of weights: 2.867947\n",
            "Iteration Number: 675, Loss: 4.332379, 12 norm of gradients: 3.398103, 12 norm of weights: 2.865474\n",
            "Iteration Number: 676, Loss: 4.325217, 12 norm of gradients: 3.391102, 12 norm of weights: 2.863007\n",
            "Iteration Number: 677, Loss: 4.318079, 12 norm of gradients: 3.384116, 12 norm of weights: 2.860545\n",
            "Iteration Number: 678, Loss: 4.310963, 12 norm of gradients: 3.377144, 12 norm of weights: 2.858089\n",
            "Iteration Number: 679, Loss: 4.303871, 12 norm of gradients: 3.370187, 12 norm of weights: 2.855639\n",
            "Iteration Number: 680, Loss: 4.296802, 12 norm of gradients: 3.363245, 12 norm of weights: 2.853195\n",
            "Iteration Number: 681, Loss: 4.289756, 12 norm of gradients: 3.356317, 12 norm of weights: 2.850757\n",
            "Iteration Number: 682, Loss: 4.282733, 12 norm of gradients: 3.349404, 12 norm of weights: 2.848324\n",
            "Iteration Number: 683, Loss: 4.275733, 12 norm of gradients: 3.342505, 12 norm of weights: 2.845897\n",
            "Iteration Number: 684, Loss: 4.268756, 12 norm of gradients: 3.335621, 12 norm of weights: 2.843476\n",
            "Iteration Number: 685, Loss: 4.261801, 12 norm of gradients: 3.328751, 12 norm of weights: 2.841061\n",
            "Iteration Number: 686, Loss: 4.254869, 12 norm of gradients: 3.321896, 12 norm of weights: 2.838651\n",
            "Iteration Number: 687, Loss: 4.247959, 12 norm of gradients: 3.315054, 12 norm of weights: 2.836247\n",
            "Iteration Number: 688, Loss: 4.241072, 12 norm of gradients: 3.308227, 12 norm of weights: 2.833849\n",
            "Iteration Number: 689, Loss: 4.234207, 12 norm of gradients: 3.301415, 12 norm of weights: 2.831457\n",
            "Iteration Number: 690, Loss: 4.227365, 12 norm of gradients: 3.294616, 12 norm of weights: 2.829070\n",
            "Iteration Number: 691, Loss: 4.220545, 12 norm of gradients: 3.287831, 12 norm of weights: 2.826689\n",
            "Iteration Number: 692, Loss: 4.213747, 12 norm of gradients: 3.281061, 12 norm of weights: 2.824314\n",
            "Iteration Number: 693, Loss: 4.206972, 12 norm of gradients: 3.274304, 12 norm of weights: 2.821944\n",
            "Iteration Number: 694, Loss: 4.200218, 12 norm of gradients: 3.267561, 12 norm of weights: 2.819580\n",
            "Iteration Number: 695, Loss: 4.193486, 12 norm of gradients: 3.260833, 12 norm of weights: 2.817222\n",
            "Iteration Number: 696, Loss: 4.186777, 12 norm of gradients: 3.254118, 12 norm of weights: 2.814869\n",
            "Iteration Number: 697, Loss: 4.180089, 12 norm of gradients: 3.247416, 12 norm of weights: 2.812522\n",
            "Iteration Number: 698, Loss: 4.173423, 12 norm of gradients: 3.240729, 12 norm of weights: 2.810181\n",
            "Iteration Number: 699, Loss: 4.166779, 12 norm of gradients: 3.234055, 12 norm of weights: 2.807845\n",
            "Iteration Number: 700, Loss: 4.160156, 12 norm of gradients: 3.227395, 12 norm of weights: 2.805515\n",
            "Iteration Number: 701, Loss: 4.153555, 12 norm of gradients: 3.220748, 12 norm of weights: 2.803190\n",
            "Iteration Number: 702, Loss: 4.146975, 12 norm of gradients: 3.214115, 12 norm of weights: 2.800871\n",
            "Iteration Number: 703, Loss: 4.140417, 12 norm of gradients: 3.207496, 12 norm of weights: 2.798558\n",
            "Iteration Number: 704, Loss: 4.133881, 12 norm of gradients: 3.200890, 12 norm of weights: 2.796250\n",
            "Iteration Number: 705, Loss: 4.127365, 12 norm of gradients: 3.194297, 12 norm of weights: 2.793948\n",
            "Iteration Number: 706, Loss: 4.120871, 12 norm of gradients: 3.187718, 12 norm of weights: 2.791651\n",
            "Iteration Number: 707, Loss: 4.114398, 12 norm of gradients: 3.181152, 12 norm of weights: 2.789360\n",
            "Iteration Number: 708, Loss: 4.107946, 12 norm of gradients: 3.174599, 12 norm of weights: 2.787075\n",
            "Iteration Number: 709, Loss: 4.101516, 12 norm of gradients: 3.168059, 12 norm of weights: 2.784795\n",
            "Iteration Number: 710, Loss: 4.095106, 12 norm of gradients: 3.161533, 12 norm of weights: 2.782520\n",
            "Iteration Number: 711, Loss: 4.088717, 12 norm of gradients: 3.155020, 12 norm of weights: 2.780251\n",
            "Iteration Number: 712, Loss: 4.082349, 12 norm of gradients: 3.148520, 12 norm of weights: 2.777988\n",
            "Iteration Number: 713, Loss: 4.076002, 12 norm of gradients: 3.142032, 12 norm of weights: 2.775729\n",
            "Iteration Number: 714, Loss: 4.069676, 12 norm of gradients: 3.135558, 12 norm of weights: 2.773477\n",
            "Iteration Number: 715, Loss: 4.063370, 12 norm of gradients: 3.129097, 12 norm of weights: 2.771230\n",
            "Iteration Number: 716, Loss: 4.057085, 12 norm of gradients: 3.122649, 12 norm of weights: 2.768988\n",
            "Iteration Number: 717, Loss: 4.050820, 12 norm of gradients: 3.116214, 12 norm of weights: 2.766752\n",
            "Iteration Number: 718, Loss: 4.044576, 12 norm of gradients: 3.109791, 12 norm of weights: 2.764521\n",
            "Iteration Number: 719, Loss: 4.038352, 12 norm of gradients: 3.103382, 12 norm of weights: 2.762296\n",
            "Iteration Number: 720, Loss: 4.032149, 12 norm of gradients: 3.096985, 12 norm of weights: 2.760076\n",
            "Iteration Number: 721, Loss: 4.025965, 12 norm of gradients: 3.090601, 12 norm of weights: 2.757862\n",
            "Iteration Number: 722, Loss: 4.019802, 12 norm of gradients: 3.084229, 12 norm of weights: 2.755653\n",
            "Iteration Number: 723, Loss: 4.013660, 12 norm of gradients: 3.077870, 12 norm of weights: 2.753449\n",
            "Iteration Number: 724, Loss: 4.007537, 12 norm of gradients: 3.071524, 12 norm of weights: 2.751251\n",
            "Iteration Number: 725, Loss: 4.001434, 12 norm of gradients: 3.065190, 12 norm of weights: 2.749058\n",
            "Iteration Number: 726, Loss: 3.995352, 12 norm of gradients: 3.058869, 12 norm of weights: 2.746871\n",
            "Iteration Number: 727, Loss: 3.989289, 12 norm of gradients: 3.052560, 12 norm of weights: 2.744688\n",
            "Iteration Number: 728, Loss: 3.983246, 12 norm of gradients: 3.046264, 12 norm of weights: 2.742512\n",
            "Iteration Number: 729, Loss: 3.977223, 12 norm of gradients: 3.039980, 12 norm of weights: 2.740340\n",
            "Iteration Number: 730, Loss: 3.971219, 12 norm of gradients: 3.033709, 12 norm of weights: 2.738174\n",
            "Iteration Number: 731, Loss: 3.965235, 12 norm of gradients: 3.027450, 12 norm of weights: 2.736013\n",
            "Iteration Number: 732, Loss: 3.959271, 12 norm of gradients: 3.021203, 12 norm of weights: 2.733858\n",
            "Iteration Number: 733, Loss: 3.953326, 12 norm of gradients: 3.014969, 12 norm of weights: 2.731708\n",
            "Iteration Number: 734, Loss: 3.947401, 12 norm of gradients: 3.008747, 12 norm of weights: 2.729563\n",
            "Iteration Number: 735, Loss: 3.941495, 12 norm of gradients: 3.002536, 12 norm of weights: 2.727423\n",
            "Iteration Number: 736, Loss: 3.935609, 12 norm of gradients: 2.996339, 12 norm of weights: 2.725289\n",
            "Iteration Number: 737, Loss: 3.929742, 12 norm of gradients: 2.990153, 12 norm of weights: 2.723160\n",
            "Iteration Number: 738, Loss: 3.923894, 12 norm of gradients: 2.983979, 12 norm of weights: 2.721036\n",
            "Iteration Number: 739, Loss: 3.918065, 12 norm of gradients: 2.977818, 12 norm of weights: 2.718918\n",
            "Iteration Number: 740, Loss: 3.912255, 12 norm of gradients: 2.971668, 12 norm of weights: 2.716805\n",
            "Iteration Number: 741, Loss: 3.906465, 12 norm of gradients: 2.965531, 12 norm of weights: 2.714697\n",
            "Iteration Number: 742, Loss: 3.900693, 12 norm of gradients: 2.959405, 12 norm of weights: 2.712594\n",
            "Iteration Number: 743, Loss: 3.894940, 12 norm of gradients: 2.953291, 12 norm of weights: 2.710496\n",
            "Iteration Number: 744, Loss: 3.889207, 12 norm of gradients: 2.947190, 12 norm of weights: 2.708404\n",
            "Iteration Number: 745, Loss: 3.883491, 12 norm of gradients: 2.941100, 12 norm of weights: 2.706317\n",
            "Iteration Number: 746, Loss: 3.877795, 12 norm of gradients: 2.935022, 12 norm of weights: 2.704235\n",
            "Iteration Number: 747, Loss: 3.872118, 12 norm of gradients: 2.928955, 12 norm of weights: 2.702158\n",
            "Iteration Number: 748, Loss: 3.866459, 12 norm of gradients: 2.922901, 12 norm of weights: 2.700087\n",
            "Iteration Number: 749, Loss: 3.860818, 12 norm of gradients: 2.916858, 12 norm of weights: 2.698020\n",
            "Iteration Number: 750, Loss: 3.855197, 12 norm of gradients: 2.910827, 12 norm of weights: 2.695959\n",
            "Iteration Number: 751, Loss: 3.849593, 12 norm of gradients: 2.904808, 12 norm of weights: 2.693903\n",
            "Iteration Number: 752, Loss: 3.844008, 12 norm of gradients: 2.898800, 12 norm of weights: 2.691852\n",
            "Iteration Number: 753, Loss: 3.838442, 12 norm of gradients: 2.892804, 12 norm of weights: 2.689806\n",
            "Iteration Number: 754, Loss: 3.832893, 12 norm of gradients: 2.886819, 12 norm of weights: 2.687765\n",
            "Iteration Number: 755, Loss: 3.827363, 12 norm of gradients: 2.880847, 12 norm of weights: 2.685729\n",
            "Iteration Number: 756, Loss: 3.821851, 12 norm of gradients: 2.874885, 12 norm of weights: 2.683699\n",
            "Iteration Number: 757, Loss: 3.816357, 12 norm of gradients: 2.868935, 12 norm of weights: 2.681673\n",
            "Iteration Number: 758, Loss: 3.810882, 12 norm of gradients: 2.862997, 12 norm of weights: 2.679653\n",
            "Iteration Number: 759, Loss: 3.805424, 12 norm of gradients: 2.857070, 12 norm of weights: 2.677638\n",
            "Iteration Number: 760, Loss: 3.799984, 12 norm of gradients: 2.851154, 12 norm of weights: 2.675627\n",
            "Iteration Number: 761, Loss: 3.794562, 12 norm of gradients: 2.845250, 12 norm of weights: 2.673622\n",
            "Iteration Number: 762, Loss: 3.789158, 12 norm of gradients: 2.839357, 12 norm of weights: 2.671622\n",
            "Iteration Number: 763, Loss: 3.783772, 12 norm of gradients: 2.833476, 12 norm of weights: 2.669627\n",
            "Iteration Number: 764, Loss: 3.778404, 12 norm of gradients: 2.827606, 12 norm of weights: 2.667637\n",
            "Iteration Number: 765, Loss: 3.773053, 12 norm of gradients: 2.821747, 12 norm of weights: 2.665652\n",
            "Iteration Number: 766, Loss: 3.767719, 12 norm of gradients: 2.815899, 12 norm of weights: 2.663672\n",
            "Iteration Number: 767, Loss: 3.762404, 12 norm of gradients: 2.810063, 12 norm of weights: 2.661697\n",
            "Iteration Number: 768, Loss: 3.757106, 12 norm of gradients: 2.804237, 12 norm of weights: 2.659727\n",
            "Iteration Number: 769, Loss: 3.751825, 12 norm of gradients: 2.798423, 12 norm of weights: 2.657762\n",
            "Iteration Number: 770, Loss: 3.746561, 12 norm of gradients: 2.792620, 12 norm of weights: 2.655802\n",
            "Iteration Number: 771, Loss: 3.741315, 12 norm of gradients: 2.786828, 12 norm of weights: 2.653847\n",
            "Iteration Number: 772, Loss: 3.736087, 12 norm of gradients: 2.781048, 12 norm of weights: 2.651896\n",
            "Iteration Number: 773, Loss: 3.730875, 12 norm of gradients: 2.775278, 12 norm of weights: 2.649951\n",
            "Iteration Number: 774, Loss: 3.725681, 12 norm of gradients: 2.769520, 12 norm of weights: 2.648011\n",
            "Iteration Number: 775, Loss: 3.720504, 12 norm of gradients: 2.763772, 12 norm of weights: 2.646076\n",
            "Iteration Number: 776, Loss: 3.715344, 12 norm of gradients: 2.758035, 12 norm of weights: 2.644145\n",
            "Iteration Number: 777, Loss: 3.710201, 12 norm of gradients: 2.752310, 12 norm of weights: 2.642220\n",
            "Iteration Number: 778, Loss: 3.705075, 12 norm of gradients: 2.746595, 12 norm of weights: 2.640299\n",
            "Iteration Number: 779, Loss: 3.699965, 12 norm of gradients: 2.740891, 12 norm of weights: 2.638383\n",
            "Iteration Number: 780, Loss: 3.694873, 12 norm of gradients: 2.735199, 12 norm of weights: 2.636473\n",
            "Iteration Number: 781, Loss: 3.689797, 12 norm of gradients: 2.729517, 12 norm of weights: 2.634567\n",
            "Iteration Number: 782, Loss: 3.684739, 12 norm of gradients: 2.723845, 12 norm of weights: 2.632666\n",
            "Iteration Number: 783, Loss: 3.679697, 12 norm of gradients: 2.718185, 12 norm of weights: 2.630769\n",
            "Iteration Number: 784, Loss: 3.674671, 12 norm of gradients: 2.712536, 12 norm of weights: 2.628878\n",
            "Iteration Number: 785, Loss: 3.669662, 12 norm of gradients: 2.706897, 12 norm of weights: 2.626992\n",
            "Iteration Number: 786, Loss: 3.664670, 12 norm of gradients: 2.701269, 12 norm of weights: 2.625110\n",
            "Iteration Number: 787, Loss: 3.659694, 12 norm of gradients: 2.695652, 12 norm of weights: 2.623233\n",
            "Iteration Number: 788, Loss: 3.654735, 12 norm of gradients: 2.690045, 12 norm of weights: 2.621361\n",
            "Iteration Number: 789, Loss: 3.649792, 12 norm of gradients: 2.684450, 12 norm of weights: 2.619494\n",
            "Iteration Number: 790, Loss: 3.644865, 12 norm of gradients: 2.678864, 12 norm of weights: 2.617631\n",
            "Iteration Number: 791, Loss: 3.639955, 12 norm of gradients: 2.673290, 12 norm of weights: 2.615774\n",
            "Iteration Number: 792, Loss: 3.635061, 12 norm of gradients: 2.667726, 12 norm of weights: 2.613921\n",
            "Iteration Number: 793, Loss: 3.630183, 12 norm of gradients: 2.662173, 12 norm of weights: 2.612073\n",
            "Iteration Number: 794, Loss: 3.625321, 12 norm of gradients: 2.656630, 12 norm of weights: 2.610229\n",
            "Iteration Number: 795, Loss: 3.620475, 12 norm of gradients: 2.651098, 12 norm of weights: 2.608391\n",
            "Iteration Number: 796, Loss: 3.615646, 12 norm of gradients: 2.645577, 12 norm of weights: 2.606557\n",
            "Iteration Number: 797, Loss: 3.610832, 12 norm of gradients: 2.640066, 12 norm of weights: 2.604728\n",
            "Iteration Number: 798, Loss: 3.606034, 12 norm of gradients: 2.634565, 12 norm of weights: 2.602904\n",
            "Iteration Number: 799, Loss: 3.601252, 12 norm of gradients: 2.629075, 12 norm of weights: 2.601084\n",
            "Iteration Number: 800, Loss: 3.596486, 12 norm of gradients: 2.623595, 12 norm of weights: 2.599269\n",
            "Iteration Number: 801, Loss: 3.591736, 12 norm of gradients: 2.618126, 12 norm of weights: 2.597459\n",
            "Iteration Number: 802, Loss: 3.587001, 12 norm of gradients: 2.612667, 12 norm of weights: 2.595653\n",
            "Iteration Number: 803, Loss: 3.582283, 12 norm of gradients: 2.607219, 12 norm of weights: 2.593852\n",
            "Iteration Number: 804, Loss: 3.577579, 12 norm of gradients: 2.601781, 12 norm of weights: 2.592056\n",
            "Iteration Number: 805, Loss: 3.572892, 12 norm of gradients: 2.596354, 12 norm of weights: 2.590265\n",
            "Iteration Number: 806, Loss: 3.568219, 12 norm of gradients: 2.590936, 12 norm of weights: 2.588478\n",
            "Iteration Number: 807, Loss: 3.563563, 12 norm of gradients: 2.585529, 12 norm of weights: 2.586696\n",
            "Iteration Number: 808, Loss: 3.558922, 12 norm of gradients: 2.580133, 12 norm of weights: 2.584918\n",
            "Iteration Number: 809, Loss: 3.554296, 12 norm of gradients: 2.574746, 12 norm of weights: 2.583145\n",
            "Iteration Number: 810, Loss: 3.549685, 12 norm of gradients: 2.569370, 12 norm of weights: 2.581377\n",
            "Iteration Number: 811, Loss: 3.545090, 12 norm of gradients: 2.564004, 12 norm of weights: 2.579613\n",
            "Iteration Number: 812, Loss: 3.540510, 12 norm of gradients: 2.558649, 12 norm of weights: 2.577854\n",
            "Iteration Number: 813, Loss: 3.535945, 12 norm of gradients: 2.553303, 12 norm of weights: 2.576099\n",
            "Iteration Number: 814, Loss: 3.531396, 12 norm of gradients: 2.547968, 12 norm of weights: 2.574350\n",
            "Iteration Number: 815, Loss: 3.526861, 12 norm of gradients: 2.542643, 12 norm of weights: 2.572604\n",
            "Iteration Number: 816, Loss: 3.522342, 12 norm of gradients: 2.537328, 12 norm of weights: 2.570863\n",
            "Iteration Number: 817, Loss: 3.517837, 12 norm of gradients: 2.532023, 12 norm of weights: 2.569127\n",
            "Iteration Number: 818, Loss: 3.513348, 12 norm of gradients: 2.526729, 12 norm of weights: 2.567396\n",
            "Iteration Number: 819, Loss: 3.508873, 12 norm of gradients: 2.521444, 12 norm of weights: 2.565669\n",
            "Iteration Number: 820, Loss: 3.504413, 12 norm of gradients: 2.516170, 12 norm of weights: 2.563946\n",
            "Iteration Number: 821, Loss: 3.499969, 12 norm of gradients: 2.510905, 12 norm of weights: 2.562228\n",
            "Iteration Number: 822, Loss: 3.495538, 12 norm of gradients: 2.505651, 12 norm of weights: 2.560514\n",
            "Iteration Number: 823, Loss: 3.491123, 12 norm of gradients: 2.500407, 12 norm of weights: 2.558805\n",
            "Iteration Number: 824, Loss: 3.486722, 12 norm of gradients: 2.495172, 12 norm of weights: 2.557101\n",
            "Iteration Number: 825, Loss: 3.482336, 12 norm of gradients: 2.489948, 12 norm of weights: 2.555401\n",
            "Iteration Number: 826, Loss: 3.477964, 12 norm of gradients: 2.484734, 12 norm of weights: 2.553705\n",
            "Iteration Number: 827, Loss: 3.473607, 12 norm of gradients: 2.479530, 12 norm of weights: 2.552014\n",
            "Iteration Number: 828, Loss: 3.469265, 12 norm of gradients: 2.474335, 12 norm of weights: 2.550328\n",
            "Iteration Number: 829, Loss: 3.464937, 12 norm of gradients: 2.469151, 12 norm of weights: 2.548645\n",
            "Iteration Number: 830, Loss: 3.460623, 12 norm of gradients: 2.463976, 12 norm of weights: 2.546968\n",
            "Iteration Number: 831, Loss: 3.456324, 12 norm of gradients: 2.458811, 12 norm of weights: 2.545294\n",
            "Iteration Number: 832, Loss: 3.452039, 12 norm of gradients: 2.453657, 12 norm of weights: 2.543626\n",
            "Iteration Number: 833, Loss: 3.447768, 12 norm of gradients: 2.448512, 12 norm of weights: 2.541961\n",
            "Iteration Number: 834, Loss: 3.443511, 12 norm of gradients: 2.443377, 12 norm of weights: 2.540301\n",
            "Iteration Number: 835, Loss: 3.439269, 12 norm of gradients: 2.438251, 12 norm of weights: 2.538646\n",
            "Iteration Number: 836, Loss: 3.435040, 12 norm of gradients: 2.433136, 12 norm of weights: 2.536994\n",
            "Iteration Number: 837, Loss: 3.430826, 12 norm of gradients: 2.428030, 12 norm of weights: 2.535348\n",
            "Iteration Number: 838, Loss: 3.426626, 12 norm of gradients: 2.422934, 12 norm of weights: 2.533705\n",
            "Iteration Number: 839, Loss: 3.422440, 12 norm of gradients: 2.417848, 12 norm of weights: 2.532067\n",
            "Iteration Number: 840, Loss: 3.418267, 12 norm of gradients: 2.412772, 12 norm of weights: 2.530433\n",
            "Iteration Number: 841, Loss: 3.414109, 12 norm of gradients: 2.407705, 12 norm of weights: 2.528804\n",
            "Iteration Number: 842, Loss: 3.409965, 12 norm of gradients: 2.402648, 12 norm of weights: 2.527179\n",
            "Iteration Number: 843, Loss: 3.405834, 12 norm of gradients: 2.397601, 12 norm of weights: 2.525558\n",
            "Iteration Number: 844, Loss: 3.401717, 12 norm of gradients: 2.392564, 12 norm of weights: 2.523942\n",
            "Iteration Number: 845, Loss: 3.397614, 12 norm of gradients: 2.387536, 12 norm of weights: 2.522330\n",
            "Iteration Number: 846, Loss: 3.393524, 12 norm of gradients: 2.382518, 12 norm of weights: 2.520722\n",
            "Iteration Number: 847, Loss: 3.389448, 12 norm of gradients: 2.377509, 12 norm of weights: 2.519119\n",
            "Iteration Number: 848, Loss: 3.385386, 12 norm of gradients: 2.372511, 12 norm of weights: 2.517520\n",
            "Iteration Number: 849, Loss: 3.381337, 12 norm of gradients: 2.367521, 12 norm of weights: 2.515925\n",
            "Iteration Number: 850, Loss: 3.377302, 12 norm of gradients: 2.362542, 12 norm of weights: 2.514334\n",
            "Iteration Number: 851, Loss: 3.373280, 12 norm of gradients: 2.357572, 12 norm of weights: 2.512748\n",
            "Iteration Number: 852, Loss: 3.369272, 12 norm of gradients: 2.352611, 12 norm of weights: 2.511166\n",
            "Iteration Number: 853, Loss: 3.365277, 12 norm of gradients: 2.347660, 12 norm of weights: 2.509588\n",
            "Iteration Number: 854, Loss: 3.361295, 12 norm of gradients: 2.342719, 12 norm of weights: 2.508015\n",
            "Iteration Number: 855, Loss: 3.357327, 12 norm of gradients: 2.337787, 12 norm of weights: 2.506445\n",
            "Iteration Number: 856, Loss: 3.353371, 12 norm of gradients: 2.332865, 12 norm of weights: 2.504880\n",
            "Iteration Number: 857, Loss: 3.349430, 12 norm of gradients: 2.327952, 12 norm of weights: 2.503319\n",
            "Iteration Number: 858, Loss: 3.345501, 12 norm of gradients: 2.323048, 12 norm of weights: 2.501763\n",
            "Iteration Number: 859, Loss: 3.341585, 12 norm of gradients: 2.318155, 12 norm of weights: 2.500210\n",
            "Iteration Number: 860, Loss: 3.337683, 12 norm of gradients: 2.313270, 12 norm of weights: 2.498662\n",
            "Iteration Number: 861, Loss: 3.333793, 12 norm of gradients: 2.308395, 12 norm of weights: 2.497118\n",
            "Iteration Number: 862, Loss: 3.329916, 12 norm of gradients: 2.303530, 12 norm of weights: 2.495578\n",
            "Iteration Number: 863, Loss: 3.326053, 12 norm of gradients: 2.298673, 12 norm of weights: 2.494042\n",
            "Iteration Number: 864, Loss: 3.322202, 12 norm of gradients: 2.293827, 12 norm of weights: 2.492510\n",
            "Iteration Number: 865, Loss: 3.318364, 12 norm of gradients: 2.288989, 12 norm of weights: 2.490983\n",
            "Iteration Number: 866, Loss: 3.314539, 12 norm of gradients: 2.284161, 12 norm of weights: 2.489459\n",
            "Iteration Number: 867, Loss: 3.310727, 12 norm of gradients: 2.279343, 12 norm of weights: 2.487940\n",
            "Iteration Number: 868, Loss: 3.306928, 12 norm of gradients: 2.274533, 12 norm of weights: 2.486425\n",
            "Iteration Number: 869, Loss: 3.303141, 12 norm of gradients: 2.269734, 12 norm of weights: 2.484914\n",
            "Iteration Number: 870, Loss: 3.299367, 12 norm of gradients: 2.264943, 12 norm of weights: 2.483407\n",
            "Iteration Number: 871, Loss: 3.295605, 12 norm of gradients: 2.260162, 12 norm of weights: 2.481904\n",
            "Iteration Number: 872, Loss: 3.291856, 12 norm of gradients: 2.255390, 12 norm of weights: 2.480406\n",
            "Iteration Number: 873, Loss: 3.288120, 12 norm of gradients: 2.250627, 12 norm of weights: 2.478911\n",
            "Iteration Number: 874, Loss: 3.284396, 12 norm of gradients: 2.245873, 12 norm of weights: 2.477420\n",
            "Iteration Number: 875, Loss: 3.280685, 12 norm of gradients: 2.241129, 12 norm of weights: 2.475934\n",
            "Iteration Number: 876, Loss: 3.276986, 12 norm of gradients: 2.236394, 12 norm of weights: 2.474451\n",
            "Iteration Number: 877, Loss: 3.273299, 12 norm of gradients: 2.231668, 12 norm of weights: 2.472973\n",
            "Iteration Number: 878, Loss: 3.269625, 12 norm of gradients: 2.226952, 12 norm of weights: 2.471499\n",
            "Iteration Number: 879, Loss: 3.265963, 12 norm of gradients: 2.222245, 12 norm of weights: 2.470028\n",
            "Iteration Number: 880, Loss: 3.262313, 12 norm of gradients: 2.217546, 12 norm of weights: 2.468562\n",
            "Iteration Number: 881, Loss: 3.258676, 12 norm of gradients: 2.212857, 12 norm of weights: 2.467100\n",
            "Iteration Number: 882, Loss: 3.255050, 12 norm of gradients: 2.208178, 12 norm of weights: 2.465641\n",
            "Iteration Number: 883, Loss: 3.251437, 12 norm of gradients: 2.203507, 12 norm of weights: 2.464187\n",
            "Iteration Number: 884, Loss: 3.247836, 12 norm of gradients: 2.198845, 12 norm of weights: 2.462736\n",
            "Iteration Number: 885, Loss: 3.244247, 12 norm of gradients: 2.194193, 12 norm of weights: 2.461290\n",
            "Iteration Number: 886, Loss: 3.240670, 12 norm of gradients: 2.189550, 12 norm of weights: 2.459848\n",
            "Iteration Number: 887, Loss: 3.237105, 12 norm of gradients: 2.184915, 12 norm of weights: 2.458409\n",
            "Iteration Number: 888, Loss: 3.233552, 12 norm of gradients: 2.180290, 12 norm of weights: 2.456975\n",
            "Iteration Number: 889, Loss: 3.230011, 12 norm of gradients: 2.175674, 12 norm of weights: 2.455544\n",
            "Iteration Number: 890, Loss: 3.226482, 12 norm of gradients: 2.171067, 12 norm of weights: 2.454117\n",
            "Iteration Number: 891, Loss: 3.222964, 12 norm of gradients: 2.166469, 12 norm of weights: 2.452695\n",
            "Iteration Number: 892, Loss: 3.219459, 12 norm of gradients: 2.161880, 12 norm of weights: 2.451276\n",
            "Iteration Number: 893, Loss: 3.215965, 12 norm of gradients: 2.157300, 12 norm of weights: 2.449861\n",
            "Iteration Number: 894, Loss: 3.212483, 12 norm of gradients: 2.152729, 12 norm of weights: 2.448450\n",
            "Iteration Number: 895, Loss: 3.209012, 12 norm of gradients: 2.148168, 12 norm of weights: 2.447043\n",
            "Iteration Number: 896, Loss: 3.205553, 12 norm of gradients: 2.143615, 12 norm of weights: 2.445640\n",
            "Iteration Number: 897, Loss: 3.202106, 12 norm of gradients: 2.139071, 12 norm of weights: 2.444240\n",
            "Iteration Number: 898, Loss: 3.198670, 12 norm of gradients: 2.134536, 12 norm of weights: 2.442845\n",
            "Iteration Number: 899, Loss: 3.195246, 12 norm of gradients: 2.130010, 12 norm of weights: 2.441453\n",
            "Iteration Number: 900, Loss: 3.191833, 12 norm of gradients: 2.125492, 12 norm of weights: 2.440065\n",
            "Iteration Number: 901, Loss: 3.188432, 12 norm of gradients: 2.120984, 12 norm of weights: 2.438681\n",
            "Iteration Number: 902, Loss: 3.185042, 12 norm of gradients: 2.116485, 12 norm of weights: 2.437301\n",
            "Iteration Number: 903, Loss: 3.181664, 12 norm of gradients: 2.111995, 12 norm of weights: 2.435925\n",
            "Iteration Number: 904, Loss: 3.178297, 12 norm of gradients: 2.107513, 12 norm of weights: 2.434552\n",
            "Iteration Number: 905, Loss: 3.174941, 12 norm of gradients: 2.103040, 12 norm of weights: 2.433183\n",
            "Iteration Number: 906, Loss: 3.171596, 12 norm of gradients: 2.098577, 12 norm of weights: 2.431819\n",
            "Iteration Number: 907, Loss: 3.168263, 12 norm of gradients: 2.094122, 12 norm of weights: 2.430457\n",
            "Iteration Number: 908, Loss: 3.164941, 12 norm of gradients: 2.089675, 12 norm of weights: 2.429100\n",
            "Iteration Number: 909, Loss: 3.161630, 12 norm of gradients: 2.085238, 12 norm of weights: 2.427747\n",
            "Iteration Number: 910, Loss: 3.158330, 12 norm of gradients: 2.080810, 12 norm of weights: 2.426397\n",
            "Iteration Number: 911, Loss: 3.155041, 12 norm of gradients: 2.076390, 12 norm of weights: 2.425051\n",
            "Iteration Number: 912, Loss: 3.151763, 12 norm of gradients: 2.071979, 12 norm of weights: 2.423708\n",
            "Iteration Number: 913, Loss: 3.148496, 12 norm of gradients: 2.067577, 12 norm of weights: 2.422370\n",
            "Iteration Number: 914, Loss: 3.145241, 12 norm of gradients: 2.063184, 12 norm of weights: 2.421035\n",
            "Iteration Number: 915, Loss: 3.141996, 12 norm of gradients: 2.058799, 12 norm of weights: 2.419704\n",
            "Iteration Number: 916, Loss: 3.138762, 12 norm of gradients: 2.054423, 12 norm of weights: 2.418376\n",
            "Iteration Number: 917, Loss: 3.135539, 12 norm of gradients: 2.050056, 12 norm of weights: 2.417053\n",
            "Iteration Number: 918, Loss: 3.132327, 12 norm of gradients: 2.045697, 12 norm of weights: 2.415733\n",
            "Iteration Number: 919, Loss: 3.129125, 12 norm of gradients: 2.041348, 12 norm of weights: 2.414416\n",
            "Iteration Number: 920, Loss: 3.125934, 12 norm of gradients: 2.037006, 12 norm of weights: 2.413104\n",
            "Iteration Number: 921, Loss: 3.122754, 12 norm of gradients: 2.032674, 12 norm of weights: 2.411795\n",
            "Iteration Number: 922, Loss: 3.119585, 12 norm of gradients: 2.028350, 12 norm of weights: 2.410489\n",
            "Iteration Number: 923, Loss: 3.116427, 12 norm of gradients: 2.024035, 12 norm of weights: 2.409188\n",
            "Iteration Number: 924, Loss: 3.113279, 12 norm of gradients: 2.019729, 12 norm of weights: 2.407890\n",
            "Iteration Number: 925, Loss: 3.110141, 12 norm of gradients: 2.015431, 12 norm of weights: 2.406595\n",
            "Iteration Number: 926, Loss: 3.107014, 12 norm of gradients: 2.011142, 12 norm of weights: 2.405305\n",
            "Iteration Number: 927, Loss: 3.103898, 12 norm of gradients: 2.006861, 12 norm of weights: 2.404017\n",
            "Iteration Number: 928, Loss: 3.100792, 12 norm of gradients: 2.002589, 12 norm of weights: 2.402734\n",
            "Iteration Number: 929, Loss: 3.097696, 12 norm of gradients: 1.998326, 12 norm of weights: 2.401454\n",
            "Iteration Number: 930, Loss: 3.094611, 12 norm of gradients: 1.994071, 12 norm of weights: 2.400178\n",
            "Iteration Number: 931, Loss: 3.091537, 12 norm of gradients: 1.989824, 12 norm of weights: 2.398905\n",
            "Iteration Number: 932, Loss: 3.088472, 12 norm of gradients: 1.985587, 12 norm of weights: 2.397636\n",
            "Iteration Number: 933, Loss: 3.085418, 12 norm of gradients: 1.981357, 12 norm of weights: 2.396370\n",
            "Iteration Number: 934, Loss: 3.082375, 12 norm of gradients: 1.977137, 12 norm of weights: 2.395108\n",
            "Iteration Number: 935, Loss: 3.079341, 12 norm of gradients: 1.972924, 12 norm of weights: 2.393850\n",
            "Iteration Number: 936, Loss: 3.076318, 12 norm of gradients: 1.968721, 12 norm of weights: 2.392595\n",
            "Iteration Number: 937, Loss: 3.073305, 12 norm of gradients: 1.964525, 12 norm of weights: 2.391344\n",
            "Iteration Number: 938, Loss: 3.070302, 12 norm of gradients: 1.960339, 12 norm of weights: 2.390096\n",
            "Iteration Number: 939, Loss: 3.067309, 12 norm of gradients: 1.956160, 12 norm of weights: 2.388851\n",
            "Iteration Number: 940, Loss: 3.064326, 12 norm of gradients: 1.951990, 12 norm of weights: 2.387611\n",
            "Iteration Number: 941, Loss: 3.061353, 12 norm of gradients: 1.947829, 12 norm of weights: 2.386373\n",
            "Iteration Number: 942, Loss: 3.058390, 12 norm of gradients: 1.943676, 12 norm of weights: 2.385139\n",
            "Iteration Number: 943, Loss: 3.055438, 12 norm of gradients: 1.939531, 12 norm of weights: 2.383909\n",
            "Iteration Number: 944, Loss: 3.052495, 12 norm of gradients: 1.935395, 12 norm of weights: 2.382682\n",
            "Iteration Number: 945, Loss: 3.049562, 12 norm of gradients: 1.931267, 12 norm of weights: 2.381459\n",
            "Iteration Number: 946, Loss: 3.046639, 12 norm of gradients: 1.927148, 12 norm of weights: 2.380239\n",
            "Iteration Number: 947, Loss: 3.043725, 12 norm of gradients: 1.923037, 12 norm of weights: 2.379023\n",
            "Iteration Number: 948, Loss: 3.040822, 12 norm of gradients: 1.918934, 12 norm of weights: 2.377810\n",
            "Iteration Number: 949, Loss: 3.037928, 12 norm of gradients: 1.914840, 12 norm of weights: 2.376600\n",
            "Iteration Number: 950, Loss: 3.035044, 12 norm of gradients: 1.910754, 12 norm of weights: 2.375394\n",
            "Iteration Number: 951, Loss: 3.032170, 12 norm of gradients: 1.906676, 12 norm of weights: 2.374191\n",
            "Iteration Number: 952, Loss: 3.029306, 12 norm of gradients: 1.902607, 12 norm of weights: 2.372992\n",
            "Iteration Number: 953, Loss: 3.026451, 12 norm of gradients: 1.898545, 12 norm of weights: 2.371796\n",
            "Iteration Number: 954, Loss: 3.023605, 12 norm of gradients: 1.894493, 12 norm of weights: 2.370604\n",
            "Iteration Number: 955, Loss: 3.020770, 12 norm of gradients: 1.890448, 12 norm of weights: 2.369415\n",
            "Iteration Number: 956, Loss: 3.017943, 12 norm of gradients: 1.886412, 12 norm of weights: 2.368229\n",
            "Iteration Number: 957, Loss: 3.015127, 12 norm of gradients: 1.882384, 12 norm of weights: 2.367047\n",
            "Iteration Number: 958, Loss: 3.012320, 12 norm of gradients: 1.878364, 12 norm of weights: 2.365868\n",
            "Iteration Number: 959, Loss: 3.009522, 12 norm of gradients: 1.874352, 12 norm of weights: 2.364692\n",
            "Iteration Number: 960, Loss: 3.006734, 12 norm of gradients: 1.870349, 12 norm of weights: 2.363520\n",
            "Iteration Number: 961, Loss: 3.003955, 12 norm of gradients: 1.866354, 12 norm of weights: 2.362351\n",
            "Iteration Number: 962, Loss: 3.001185, 12 norm of gradients: 1.862367, 12 norm of weights: 2.361185\n",
            "Iteration Number: 963, Loss: 2.998425, 12 norm of gradients: 1.858388, 12 norm of weights: 2.360023\n",
            "Iteration Number: 964, Loss: 2.995674, 12 norm of gradients: 1.854417, 12 norm of weights: 2.358864\n",
            "Iteration Number: 965, Loss: 2.992932, 12 norm of gradients: 1.850455, 12 norm of weights: 2.357709\n",
            "Iteration Number: 966, Loss: 2.990200, 12 norm of gradients: 1.846500, 12 norm of weights: 2.356556\n",
            "Iteration Number: 967, Loss: 2.987477, 12 norm of gradients: 1.842554, 12 norm of weights: 2.355407\n",
            "Iteration Number: 968, Loss: 2.984763, 12 norm of gradients: 1.838616, 12 norm of weights: 2.354262\n",
            "Iteration Number: 969, Loss: 2.982058, 12 norm of gradients: 1.834686, 12 norm of weights: 2.353119\n",
            "Iteration Number: 970, Loss: 2.979362, 12 norm of gradients: 1.830764, 12 norm of weights: 2.351980\n",
            "Iteration Number: 971, Loss: 2.976675, 12 norm of gradients: 1.826850, 12 norm of weights: 2.350844\n",
            "Iteration Number: 972, Loss: 2.973998, 12 norm of gradients: 1.822944, 12 norm of weights: 2.349711\n",
            "Iteration Number: 973, Loss: 2.971329, 12 norm of gradients: 1.819046, 12 norm of weights: 2.348582\n",
            "Iteration Number: 974, Loss: 2.968669, 12 norm of gradients: 1.815157, 12 norm of weights: 2.347456\n",
            "Iteration Number: 975, Loss: 2.966019, 12 norm of gradients: 1.811275, 12 norm of weights: 2.346333\n",
            "Iteration Number: 976, Loss: 2.963377, 12 norm of gradients: 1.807402, 12 norm of weights: 2.345213\n",
            "Iteration Number: 977, Loss: 2.960744, 12 norm of gradients: 1.803536, 12 norm of weights: 2.344097\n",
            "Iteration Number: 978, Loss: 2.958120, 12 norm of gradients: 1.799678, 12 norm of weights: 2.342983\n",
            "Iteration Number: 979, Loss: 2.955505, 12 norm of gradients: 1.795829, 12 norm of weights: 2.341873\n",
            "Iteration Number: 980, Loss: 2.952898, 12 norm of gradients: 1.791987, 12 norm of weights: 2.340767\n",
            "Iteration Number: 981, Loss: 2.950301, 12 norm of gradients: 1.788153, 12 norm of weights: 2.339663\n",
            "Iteration Number: 982, Loss: 2.947712, 12 norm of gradients: 1.784328, 12 norm of weights: 2.338562\n",
            "Iteration Number: 983, Loss: 2.945132, 12 norm of gradients: 1.780510, 12 norm of weights: 2.337465\n",
            "Iteration Number: 984, Loss: 2.942561, 12 norm of gradients: 1.776700, 12 norm of weights: 2.336371\n",
            "Iteration Number: 985, Loss: 2.939998, 12 norm of gradients: 1.772898, 12 norm of weights: 2.335280\n",
            "Iteration Number: 986, Loss: 2.937444, 12 norm of gradients: 1.769104, 12 norm of weights: 2.334192\n",
            "Iteration Number: 987, Loss: 2.934898, 12 norm of gradients: 1.765318, 12 norm of weights: 2.333107\n",
            "Iteration Number: 988, Loss: 2.932361, 12 norm of gradients: 1.761540, 12 norm of weights: 2.332025\n",
            "Iteration Number: 989, Loss: 2.929833, 12 norm of gradients: 1.757769, 12 norm of weights: 2.330947\n",
            "Iteration Number: 990, Loss: 2.927313, 12 norm of gradients: 1.754007, 12 norm of weights: 2.329872\n",
            "Iteration Number: 991, Loss: 2.924802, 12 norm of gradients: 1.750252, 12 norm of weights: 2.328799\n",
            "Iteration Number: 992, Loss: 2.922299, 12 norm of gradients: 1.746505, 12 norm of weights: 2.327730\n",
            "Iteration Number: 993, Loss: 2.919805, 12 norm of gradients: 1.742766, 12 norm of weights: 2.326664\n",
            "Iteration Number: 994, Loss: 2.917318, 12 norm of gradients: 1.739035, 12 norm of weights: 2.325601\n",
            "Iteration Number: 995, Loss: 2.914841, 12 norm of gradients: 1.735312, 12 norm of weights: 2.324541\n",
            "Iteration Number: 996, Loss: 2.912371, 12 norm of gradients: 1.731596, 12 norm of weights: 2.323484\n",
            "Iteration Number: 997, Loss: 2.909910, 12 norm of gradients: 1.727888, 12 norm of weights: 2.322430\n",
            "Iteration Number: 998, Loss: 2.907458, 12 norm of gradients: 1.724188, 12 norm of weights: 2.321380\n",
            "Iteration Number: 999, Loss: 2.905013, 12 norm of gradients: 1.720496, 12 norm of weights: 2.320332\n",
            "Iteration Number: 1000, Loss: 2.902577, 12 norm of gradients: 1.716811, 12 norm of weights: 2.319287\n",
            "Iteration Number: 1001, Loss: 2.900149, 12 norm of gradients: 1.713134, 12 norm of weights: 2.318246\n",
            "Iteration Number: 1002, Loss: 2.897729, 12 norm of gradients: 1.709465, 12 norm of weights: 2.317207\n",
            "Iteration Number: 1003, Loss: 2.895318, 12 norm of gradients: 1.705804, 12 norm of weights: 2.316171\n",
            "Iteration Number: 1004, Loss: 2.892914, 12 norm of gradients: 1.702150, 12 norm of weights: 2.315139\n",
            "Iteration Number: 1005, Loss: 2.890519, 12 norm of gradients: 1.698504, 12 norm of weights: 2.314109\n",
            "Iteration Number: 1006, Loss: 2.888131, 12 norm of gradients: 1.694866, 12 norm of weights: 2.313083\n",
            "Iteration Number: 1007, Loss: 2.885752, 12 norm of gradients: 1.691235, 12 norm of weights: 2.312059\n",
            "Iteration Number: 1008, Loss: 2.883381, 12 norm of gradients: 1.687612, 12 norm of weights: 2.311038\n",
            "Iteration Number: 1009, Loss: 2.881017, 12 norm of gradients: 1.683997, 12 norm of weights: 2.310021\n",
            "Iteration Number: 1010, Loss: 2.878662, 12 norm of gradients: 1.680389, 12 norm of weights: 2.309006\n",
            "Iteration Number: 1011, Loss: 2.876315, 12 norm of gradients: 1.676789, 12 norm of weights: 2.307994\n",
            "Iteration Number: 1012, Loss: 2.873975, 12 norm of gradients: 1.673196, 12 norm of weights: 2.306986\n",
            "Iteration Number: 1013, Loss: 2.871643, 12 norm of gradients: 1.669611, 12 norm of weights: 2.305980\n",
            "Iteration Number: 1014, Loss: 2.869320, 12 norm of gradients: 1.666034, 12 norm of weights: 2.304977\n",
            "Iteration Number: 1015, Loss: 2.867004, 12 norm of gradients: 1.662464, 12 norm of weights: 2.303977\n",
            "Iteration Number: 1016, Loss: 2.864696, 12 norm of gradients: 1.658901, 12 norm of weights: 2.302980\n",
            "Iteration Number: 1017, Loss: 2.862395, 12 norm of gradients: 1.655347, 12 norm of weights: 2.301985\n",
            "Iteration Number: 1018, Loss: 2.860103, 12 norm of gradients: 1.651799, 12 norm of weights: 2.300994\n",
            "Iteration Number: 1019, Loss: 2.857818, 12 norm of gradients: 1.648260, 12 norm of weights: 2.300006\n",
            "Iteration Number: 1020, Loss: 2.855540, 12 norm of gradients: 1.644728, 12 norm of weights: 2.299020\n",
            "Iteration Number: 1021, Loss: 2.853271, 12 norm of gradients: 1.641203, 12 norm of weights: 2.298038\n",
            "Iteration Number: 1022, Loss: 2.851009, 12 norm of gradients: 1.637686, 12 norm of weights: 2.297058\n",
            "Iteration Number: 1023, Loss: 2.848755, 12 norm of gradients: 1.634176, 12 norm of weights: 2.296081\n",
            "Iteration Number: 1024, Loss: 2.846508, 12 norm of gradients: 1.630674, 12 norm of weights: 2.295107\n",
            "Iteration Number: 1025, Loss: 2.844269, 12 norm of gradients: 1.627179, 12 norm of weights: 2.294136\n",
            "Iteration Number: 1026, Loss: 2.842037, 12 norm of gradients: 1.623691, 12 norm of weights: 2.293167\n",
            "Iteration Number: 1027, Loss: 2.839813, 12 norm of gradients: 1.620211, 12 norm of weights: 2.292202\n",
            "Iteration Number: 1028, Loss: 2.837597, 12 norm of gradients: 1.616739, 12 norm of weights: 2.291239\n",
            "Iteration Number: 1029, Loss: 2.835388, 12 norm of gradients: 1.613274, 12 norm of weights: 2.290279\n",
            "Iteration Number: 1030, Loss: 2.833186, 12 norm of gradients: 1.609816, 12 norm of weights: 2.289322\n",
            "Iteration Number: 1031, Loss: 2.830992, 12 norm of gradients: 1.606365, 12 norm of weights: 2.288368\n",
            "Iteration Number: 1032, Loss: 2.828805, 12 norm of gradients: 1.602922, 12 norm of weights: 2.287417\n",
            "Iteration Number: 1033, Loss: 2.826625, 12 norm of gradients: 1.599487, 12 norm of weights: 2.286468\n",
            "Iteration Number: 1034, Loss: 2.824453, 12 norm of gradients: 1.596058, 12 norm of weights: 2.285522\n",
            "Iteration Number: 1035, Loss: 2.822288, 12 norm of gradients: 1.592637, 12 norm of weights: 2.284579\n",
            "Iteration Number: 1036, Loss: 2.820131, 12 norm of gradients: 1.589223, 12 norm of weights: 2.283639\n",
            "Iteration Number: 1037, Loss: 2.817980, 12 norm of gradients: 1.585817, 12 norm of weights: 2.282701\n",
            "Iteration Number: 1038, Loss: 2.815837, 12 norm of gradients: 1.582418, 12 norm of weights: 2.281766\n",
            "Iteration Number: 1039, Loss: 2.813701, 12 norm of gradients: 1.579026, 12 norm of weights: 2.280834\n",
            "Iteration Number: 1040, Loss: 2.811573, 12 norm of gradients: 1.575642, 12 norm of weights: 2.279905\n",
            "Iteration Number: 1041, Loss: 2.809451, 12 norm of gradients: 1.572264, 12 norm of weights: 2.278978\n",
            "Iteration Number: 1042, Loss: 2.807337, 12 norm of gradients: 1.568894, 12 norm of weights: 2.278054\n",
            "Iteration Number: 1043, Loss: 2.805230, 12 norm of gradients: 1.565531, 12 norm of weights: 2.277133\n",
            "Iteration Number: 1044, Loss: 2.803130, 12 norm of gradients: 1.562176, 12 norm of weights: 2.276215\n",
            "Iteration Number: 1045, Loss: 2.801036, 12 norm of gradients: 1.558828, 12 norm of weights: 2.275299\n",
            "Iteration Number: 1046, Loss: 2.798950, 12 norm of gradients: 1.555486, 12 norm of weights: 2.274386\n",
            "Iteration Number: 1047, Loss: 2.796871, 12 norm of gradients: 1.552152, 12 norm of weights: 2.273476\n",
            "Iteration Number: 1048, Loss: 2.794799, 12 norm of gradients: 1.548826, 12 norm of weights: 2.272568\n",
            "Iteration Number: 1049, Loss: 2.792734, 12 norm of gradients: 1.545506, 12 norm of weights: 2.271663\n",
            "Iteration Number: 1050, Loss: 2.790676, 12 norm of gradients: 1.542193, 12 norm of weights: 2.270761\n",
            "Iteration Number: 1051, Loss: 2.788625, 12 norm of gradients: 1.538888, 12 norm of weights: 2.269861\n",
            "Iteration Number: 1052, Loss: 2.786581, 12 norm of gradients: 1.535590, 12 norm of weights: 2.268964\n",
            "Iteration Number: 1053, Loss: 2.784543, 12 norm of gradients: 1.532299, 12 norm of weights: 2.268070\n",
            "Iteration Number: 1054, Loss: 2.782513, 12 norm of gradients: 1.529015, 12 norm of weights: 2.267178\n",
            "Iteration Number: 1055, Loss: 2.780489, 12 norm of gradients: 1.525738, 12 norm of weights: 2.266289\n",
            "Iteration Number: 1056, Loss: 2.778472, 12 norm of gradients: 1.522468, 12 norm of weights: 2.265402\n",
            "Iteration Number: 1057, Loss: 2.776462, 12 norm of gradients: 1.519205, 12 norm of weights: 2.264519\n",
            "Iteration Number: 1058, Loss: 2.774459, 12 norm of gradients: 1.515950, 12 norm of weights: 2.263637\n",
            "Iteration Number: 1059, Loss: 2.772462, 12 norm of gradients: 1.512701, 12 norm of weights: 2.262759\n",
            "Iteration Number: 1060, Loss: 2.770472, 12 norm of gradients: 1.509459, 12 norm of weights: 2.261883\n",
            "Iteration Number: 1061, Loss: 2.768489, 12 norm of gradients: 1.506225, 12 norm of weights: 2.261009\n",
            "Iteration Number: 1062, Loss: 2.766512, 12 norm of gradients: 1.502997, 12 norm of weights: 2.260138\n",
            "Iteration Number: 1063, Loss: 2.764542, 12 norm of gradients: 1.499777, 12 norm of weights: 2.259270\n",
            "Iteration Number: 1064, Loss: 2.762579, 12 norm of gradients: 1.496563, 12 norm of weights: 2.258404\n",
            "Iteration Number: 1065, Loss: 2.760622, 12 norm of gradients: 1.493357, 12 norm of weights: 2.257541\n",
            "Iteration Number: 1066, Loss: 2.758672, 12 norm of gradients: 1.490157, 12 norm of weights: 2.256681\n",
            "Iteration Number: 1067, Loss: 2.756729, 12 norm of gradients: 1.486964, 12 norm of weights: 2.255823\n",
            "Iteration Number: 1068, Loss: 2.754792, 12 norm of gradients: 1.483779, 12 norm of weights: 2.254967\n",
            "Iteration Number: 1069, Loss: 2.752861, 12 norm of gradients: 1.480600, 12 norm of weights: 2.254114\n",
            "Iteration Number: 1070, Loss: 2.750937, 12 norm of gradients: 1.477428, 12 norm of weights: 2.253264\n",
            "Iteration Number: 1071, Loss: 2.749019, 12 norm of gradients: 1.474263, 12 norm of weights: 2.252416\n",
            "Iteration Number: 1072, Loss: 2.747108, 12 norm of gradients: 1.471105, 12 norm of weights: 2.251570\n",
            "Iteration Number: 1073, Loss: 2.745203, 12 norm of gradients: 1.467954, 12 norm of weights: 2.250728\n",
            "Iteration Number: 1074, Loss: 2.743305, 12 norm of gradients: 1.464809, 12 norm of weights: 2.249887\n",
            "Iteration Number: 1075, Loss: 2.741413, 12 norm of gradients: 1.461672, 12 norm of weights: 2.249049\n",
            "Iteration Number: 1076, Loss: 2.739528, 12 norm of gradients: 1.458541, 12 norm of weights: 2.248214\n",
            "Iteration Number: 1077, Loss: 2.737648, 12 norm of gradients: 1.455417, 12 norm of weights: 2.247381\n",
            "Iteration Number: 1078, Loss: 2.735775, 12 norm of gradients: 1.452300, 12 norm of weights: 2.246550\n",
            "Iteration Number: 1079, Loss: 2.733909, 12 norm of gradients: 1.449190, 12 norm of weights: 2.245723\n",
            "Iteration Number: 1080, Loss: 2.732048, 12 norm of gradients: 1.446087, 12 norm of weights: 2.244897\n",
            "Iteration Number: 1081, Loss: 2.730194, 12 norm of gradients: 1.442990, 12 norm of weights: 2.244074\n",
            "Iteration Number: 1082, Loss: 2.728346, 12 norm of gradients: 1.439901, 12 norm of weights: 2.243253\n",
            "Iteration Number: 1083, Loss: 2.726504, 12 norm of gradients: 1.436818, 12 norm of weights: 2.242435\n",
            "Iteration Number: 1084, Loss: 2.724669, 12 norm of gradients: 1.433741, 12 norm of weights: 2.241619\n",
            "Iteration Number: 1085, Loss: 2.722839, 12 norm of gradients: 1.430672, 12 norm of weights: 2.240806\n",
            "Iteration Number: 1086, Loss: 2.721016, 12 norm of gradients: 1.427609, 12 norm of weights: 2.239995\n",
            "Iteration Number: 1087, Loss: 2.719199, 12 norm of gradients: 1.424553, 12 norm of weights: 2.239187\n",
            "Iteration Number: 1088, Loss: 2.717388, 12 norm of gradients: 1.421504, 12 norm of weights: 2.238381\n",
            "Iteration Number: 1089, Loss: 2.715583, 12 norm of gradients: 1.418461, 12 norm of weights: 2.237577\n",
            "Iteration Number: 1090, Loss: 2.713784, 12 norm of gradients: 1.415425, 12 norm of weights: 2.236776\n",
            "Iteration Number: 1091, Loss: 2.711991, 12 norm of gradients: 1.412396, 12 norm of weights: 2.235977\n",
            "Iteration Number: 1092, Loss: 2.710204, 12 norm of gradients: 1.409373, 12 norm of weights: 2.235180\n",
            "Iteration Number: 1093, Loss: 2.708424, 12 norm of gradients: 1.406357, 12 norm of weights: 2.234386\n",
            "Iteration Number: 1094, Loss: 2.706649, 12 norm of gradients: 1.403348, 12 norm of weights: 2.233594\n",
            "Iteration Number: 1095, Loss: 2.704880, 12 norm of gradients: 1.400345, 12 norm of weights: 2.232805\n",
            "Iteration Number: 1096, Loss: 2.703117, 12 norm of gradients: 1.397349, 12 norm of weights: 2.232018\n",
            "Iteration Number: 1097, Loss: 2.701360, 12 norm of gradients: 1.394359, 12 norm of weights: 2.231233\n",
            "Iteration Number: 1098, Loss: 2.699608, 12 norm of gradients: 1.391376, 12 norm of weights: 2.230451\n",
            "Iteration Number: 1099, Loss: 2.697863, 12 norm of gradients: 1.388400, 12 norm of weights: 2.229671\n",
            "Iteration Number: 1100, Loss: 2.696123, 12 norm of gradients: 1.385430, 12 norm of weights: 2.228893\n",
            "Iteration Number: 1101, Loss: 2.694390, 12 norm of gradients: 1.382467, 12 norm of weights: 2.228118\n",
            "Iteration Number: 1102, Loss: 2.692662, 12 norm of gradients: 1.379510, 12 norm of weights: 2.227345\n",
            "Iteration Number: 1103, Loss: 2.690940, 12 norm of gradients: 1.376560, 12 norm of weights: 2.226574\n",
            "Iteration Number: 1104, Loss: 2.689224, 12 norm of gradients: 1.373616, 12 norm of weights: 2.225805\n",
            "Iteration Number: 1105, Loss: 2.687513, 12 norm of gradients: 1.370679, 12 norm of weights: 2.225039\n",
            "Iteration Number: 1106, Loss: 2.685808, 12 norm of gradients: 1.367749, 12 norm of weights: 2.224275\n",
            "Iteration Number: 1107, Loss: 2.684109, 12 norm of gradients: 1.364825, 12 norm of weights: 2.223514\n",
            "Iteration Number: 1108, Loss: 2.682416, 12 norm of gradients: 1.361907, 12 norm of weights: 2.222754\n",
            "Iteration Number: 1109, Loss: 2.680728, 12 norm of gradients: 1.358996, 12 norm of weights: 2.221997\n",
            "Iteration Number: 1110, Loss: 2.679046, 12 norm of gradients: 1.356091, 12 norm of weights: 2.221243\n",
            "Iteration Number: 1111, Loss: 2.677369, 12 norm of gradients: 1.353193, 12 norm of weights: 2.220490\n",
            "Iteration Number: 1112, Loss: 2.675698, 12 norm of gradients: 1.350301, 12 norm of weights: 2.219740\n",
            "Iteration Number: 1113, Loss: 2.674033, 12 norm of gradients: 1.347415, 12 norm of weights: 2.218992\n",
            "Iteration Number: 1114, Loss: 2.672373, 12 norm of gradients: 1.344536, 12 norm of weights: 2.218246\n",
            "Iteration Number: 1115, Loss: 2.670719, 12 norm of gradients: 1.341663, 12 norm of weights: 2.217502\n",
            "Iteration Number: 1116, Loss: 2.669071, 12 norm of gradients: 1.338797, 12 norm of weights: 2.216761\n",
            "Iteration Number: 1117, Loss: 2.667427, 12 norm of gradients: 1.335937, 12 norm of weights: 2.216022\n",
            "Iteration Number: 1118, Loss: 2.665790, 12 norm of gradients: 1.333084, 12 norm of weights: 2.215285\n",
            "Iteration Number: 1119, Loss: 2.664158, 12 norm of gradients: 1.330236, 12 norm of weights: 2.214550\n",
            "Iteration Number: 1120, Loss: 2.662531, 12 norm of gradients: 1.327395, 12 norm of weights: 2.213818\n",
            "Iteration Number: 1121, Loss: 2.660910, 12 norm of gradients: 1.324561, 12 norm of weights: 2.213088\n",
            "Iteration Number: 1122, Loss: 2.659294, 12 norm of gradients: 1.321733, 12 norm of weights: 2.212360\n",
            "Iteration Number: 1123, Loss: 2.657684, 12 norm of gradients: 1.318911, 12 norm of weights: 2.211634\n",
            "Iteration Number: 1124, Loss: 2.656079, 12 norm of gradients: 1.316095, 12 norm of weights: 2.210910\n",
            "Iteration Number: 1125, Loss: 2.654479, 12 norm of gradients: 1.313286, 12 norm of weights: 2.210188\n",
            "Iteration Number: 1126, Loss: 2.652885, 12 norm of gradients: 1.310482, 12 norm of weights: 2.209469\n",
            "Iteration Number: 1127, Loss: 2.651296, 12 norm of gradients: 1.307686, 12 norm of weights: 2.208752\n",
            "Iteration Number: 1128, Loss: 2.649712, 12 norm of gradients: 1.304895, 12 norm of weights: 2.208037\n",
            "Iteration Number: 1129, Loss: 2.648133, 12 norm of gradients: 1.302110, 12 norm of weights: 2.207324\n",
            "Iteration Number: 1130, Loss: 2.646560, 12 norm of gradients: 1.299332, 12 norm of weights: 2.206613\n",
            "Iteration Number: 1131, Loss: 2.644992, 12 norm of gradients: 1.296560, 12 norm of weights: 2.205904\n",
            "Iteration Number: 1132, Loss: 2.643430, 12 norm of gradients: 1.293795, 12 norm of weights: 2.205198\n",
            "Iteration Number: 1133, Loss: 2.641872, 12 norm of gradients: 1.291035, 12 norm of weights: 2.204493\n",
            "Iteration Number: 1134, Loss: 2.640320, 12 norm of gradients: 1.288281, 12 norm of weights: 2.203791\n",
            "Iteration Number: 1135, Loss: 2.638773, 12 norm of gradients: 1.285534, 12 norm of weights: 2.203091\n",
            "Iteration Number: 1136, Loss: 2.637231, 12 norm of gradients: 1.282793, 12 norm of weights: 2.202393\n",
            "Iteration Number: 1137, Loss: 2.635694, 12 norm of gradients: 1.280058, 12 norm of weights: 2.201697\n",
            "Iteration Number: 1138, Loss: 2.634163, 12 norm of gradients: 1.277329, 12 norm of weights: 2.201003\n",
            "Iteration Number: 1139, Loss: 2.632636, 12 norm of gradients: 1.274607, 12 norm of weights: 2.200311\n",
            "Iteration Number: 1140, Loss: 2.631115, 12 norm of gradients: 1.271890, 12 norm of weights: 2.199621\n",
            "Iteration Number: 1141, Loss: 2.629599, 12 norm of gradients: 1.269179, 12 norm of weights: 2.198934\n",
            "Iteration Number: 1142, Loss: 2.628087, 12 norm of gradients: 1.266475, 12 norm of weights: 2.198248\n",
            "Iteration Number: 1143, Loss: 2.626581, 12 norm of gradients: 1.263776, 12 norm of weights: 2.197564\n",
            "Iteration Number: 1144, Loss: 2.625080, 12 norm of gradients: 1.261084, 12 norm of weights: 2.196883\n",
            "Iteration Number: 1145, Loss: 2.623584, 12 norm of gradients: 1.258398, 12 norm of weights: 2.196204\n",
            "Iteration Number: 1146, Loss: 2.622092, 12 norm of gradients: 1.255718, 12 norm of weights: 2.195526\n",
            "Iteration Number: 1147, Loss: 2.620606, 12 norm of gradients: 1.253043, 12 norm of weights: 2.194851\n",
            "Iteration Number: 1148, Loss: 2.619125, 12 norm of gradients: 1.250375, 12 norm of weights: 2.194178\n",
            "Iteration Number: 1149, Loss: 2.617648, 12 norm of gradients: 1.247713, 12 norm of weights: 2.193506\n",
            "Iteration Number: 1150, Loss: 2.616177, 12 norm of gradients: 1.245057, 12 norm of weights: 2.192837\n",
            "Iteration Number: 1151, Loss: 2.614710, 12 norm of gradients: 1.242406, 12 norm of weights: 2.192170\n",
            "Iteration Number: 1152, Loss: 2.613249, 12 norm of gradients: 1.239762, 12 norm of weights: 2.191504\n",
            "Iteration Number: 1153, Loss: 2.611792, 12 norm of gradients: 1.237124, 12 norm of weights: 2.190841\n",
            "Iteration Number: 1154, Loss: 2.610340, 12 norm of gradients: 1.234491, 12 norm of weights: 2.190180\n",
            "Iteration Number: 1155, Loss: 2.608893, 12 norm of gradients: 1.231865, 12 norm of weights: 2.189521\n",
            "Iteration Number: 1156, Loss: 2.607451, 12 norm of gradients: 1.229244, 12 norm of weights: 2.188864\n",
            "Iteration Number: 1157, Loss: 2.606013, 12 norm of gradients: 1.226630, 12 norm of weights: 2.188208\n",
            "Iteration Number: 1158, Loss: 2.604580, 12 norm of gradients: 1.224021, 12 norm of weights: 2.187555\n",
            "Iteration Number: 1159, Loss: 2.603152, 12 norm of gradients: 1.221418, 12 norm of weights: 2.186904\n",
            "Iteration Number: 1160, Loss: 2.601729, 12 norm of gradients: 1.218821, 12 norm of weights: 2.186254\n",
            "Iteration Number: 1161, Loss: 2.600311, 12 norm of gradients: 1.216230, 12 norm of weights: 2.185607\n",
            "Iteration Number: 1162, Loss: 2.598897, 12 norm of gradients: 1.213645, 12 norm of weights: 2.184961\n",
            "Iteration Number: 1163, Loss: 2.597488, 12 norm of gradients: 1.211065, 12 norm of weights: 2.184318\n",
            "Iteration Number: 1164, Loss: 2.596084, 12 norm of gradients: 1.208492, 12 norm of weights: 2.183676\n",
            "Iteration Number: 1165, Loss: 2.594684, 12 norm of gradients: 1.205924, 12 norm of weights: 2.183037\n",
            "Iteration Number: 1166, Loss: 2.593289, 12 norm of gradients: 1.203362, 12 norm of weights: 2.182399\n",
            "Iteration Number: 1167, Loss: 2.591898, 12 norm of gradients: 1.200806, 12 norm of weights: 2.181763\n",
            "Iteration Number: 1168, Loss: 2.590513, 12 norm of gradients: 1.198256, 12 norm of weights: 2.181129\n",
            "Iteration Number: 1169, Loss: 2.589131, 12 norm of gradients: 1.195711, 12 norm of weights: 2.180497\n",
            "Iteration Number: 1170, Loss: 2.587755, 12 norm of gradients: 1.193172, 12 norm of weights: 2.179867\n",
            "Iteration Number: 1171, Loss: 2.586383, 12 norm of gradients: 1.190639, 12 norm of weights: 2.179239\n",
            "Iteration Number: 1172, Loss: 2.585015, 12 norm of gradients: 1.188112, 12 norm of weights: 2.178613\n",
            "Iteration Number: 1173, Loss: 2.583652, 12 norm of gradients: 1.185590, 12 norm of weights: 2.177988\n",
            "Iteration Number: 1174, Loss: 2.582294, 12 norm of gradients: 1.183074, 12 norm of weights: 2.177366\n",
            "Iteration Number: 1175, Loss: 2.580940, 12 norm of gradients: 1.180564, 12 norm of weights: 2.176745\n",
            "Iteration Number: 1176, Loss: 2.579591, 12 norm of gradients: 1.178060, 12 norm of weights: 2.176127\n",
            "Iteration Number: 1177, Loss: 2.578246, 12 norm of gradients: 1.175561, 12 norm of weights: 2.175510\n",
            "Iteration Number: 1178, Loss: 2.576905, 12 norm of gradients: 1.173068, 12 norm of weights: 2.174895\n",
            "Iteration Number: 1179, Loss: 2.575569, 12 norm of gradients: 1.170580, 12 norm of weights: 2.174282\n",
            "Iteration Number: 1180, Loss: 2.574238, 12 norm of gradients: 1.168098, 12 norm of weights: 2.173670\n",
            "Iteration Number: 1181, Loss: 2.572910, 12 norm of gradients: 1.165622, 12 norm of weights: 2.173061\n",
            "Iteration Number: 1182, Loss: 2.571588, 12 norm of gradients: 1.163152, 12 norm of weights: 2.172453\n",
            "Iteration Number: 1183, Loss: 2.570269, 12 norm of gradients: 1.160687, 12 norm of weights: 2.171848\n",
            "Iteration Number: 1184, Loss: 2.568955, 12 norm of gradients: 1.158227, 12 norm of weights: 2.171244\n",
            "Iteration Number: 1185, Loss: 2.567645, 12 norm of gradients: 1.155774, 12 norm of weights: 2.170642\n",
            "Iteration Number: 1186, Loss: 2.566340, 12 norm of gradients: 1.153325, 12 norm of weights: 2.170041\n",
            "Iteration Number: 1187, Loss: 2.565039, 12 norm of gradients: 1.150883, 12 norm of weights: 2.169443\n",
            "Iteration Number: 1188, Loss: 2.563742, 12 norm of gradients: 1.148446, 12 norm of weights: 2.168846\n",
            "Iteration Number: 1189, Loss: 2.562450, 12 norm of gradients: 1.146014, 12 norm of weights: 2.168251\n",
            "Iteration Number: 1190, Loss: 2.561162, 12 norm of gradients: 1.143588, 12 norm of weights: 2.167658\n",
            "Iteration Number: 1191, Loss: 2.559878, 12 norm of gradients: 1.141168, 12 norm of weights: 2.167067\n",
            "Iteration Number: 1192, Loss: 2.558598, 12 norm of gradients: 1.138753, 12 norm of weights: 2.166477\n",
            "Iteration Number: 1193, Loss: 2.557323, 12 norm of gradients: 1.136344, 12 norm of weights: 2.165890\n",
            "Iteration Number: 1194, Loss: 2.556052, 12 norm of gradients: 1.133940, 12 norm of weights: 2.165304\n",
            "Iteration Number: 1195, Loss: 2.554785, 12 norm of gradients: 1.131541, 12 norm of weights: 2.164720\n",
            "Iteration Number: 1196, Loss: 2.553522, 12 norm of gradients: 1.129148, 12 norm of weights: 2.164137\n",
            "Iteration Number: 1197, Loss: 2.552263, 12 norm of gradients: 1.126761, 12 norm of weights: 2.163557\n",
            "Iteration Number: 1198, Loss: 2.551009, 12 norm of gradients: 1.124379, 12 norm of weights: 2.162978\n",
            "Iteration Number: 1199, Loss: 2.549759, 12 norm of gradients: 1.122002, 12 norm of weights: 2.162401\n",
            "Iteration Number: 1200, Loss: 2.548513, 12 norm of gradients: 1.119631, 12 norm of weights: 2.161825\n",
            "Iteration Number: 1201, Loss: 2.547271, 12 norm of gradients: 1.117265, 12 norm of weights: 2.161252\n",
            "Iteration Number: 1202, Loss: 2.546033, 12 norm of gradients: 1.114905, 12 norm of weights: 2.160680\n",
            "Iteration Number: 1203, Loss: 2.544799, 12 norm of gradients: 1.112550, 12 norm of weights: 2.160110\n",
            "Iteration Number: 1204, Loss: 2.543569, 12 norm of gradients: 1.110201, 12 norm of weights: 2.159541\n",
            "Iteration Number: 1205, Loss: 2.542343, 12 norm of gradients: 1.107856, 12 norm of weights: 2.158974\n",
            "Iteration Number: 1206, Loss: 2.541122, 12 norm of gradients: 1.105518, 12 norm of weights: 2.158409\n",
            "Iteration Number: 1207, Loss: 2.539904, 12 norm of gradients: 1.103184, 12 norm of weights: 2.157846\n",
            "Iteration Number: 1208, Loss: 2.538690, 12 norm of gradients: 1.100856, 12 norm of weights: 2.157285\n",
            "Iteration Number: 1209, Loss: 2.537481, 12 norm of gradients: 1.098533, 12 norm of weights: 2.156725\n",
            "Iteration Number: 1210, Loss: 2.536275, 12 norm of gradients: 1.096216, 12 norm of weights: 2.156167\n",
            "Iteration Number: 1211, Loss: 2.535073, 12 norm of gradients: 1.093904, 12 norm of weights: 2.155610\n",
            "Iteration Number: 1212, Loss: 2.533876, 12 norm of gradients: 1.091597, 12 norm of weights: 2.155055\n",
            "Iteration Number: 1213, Loss: 2.532682, 12 norm of gradients: 1.089295, 12 norm of weights: 2.154502\n",
            "Iteration Number: 1214, Loss: 2.531492, 12 norm of gradients: 1.086999, 12 norm of weights: 2.153951\n",
            "Iteration Number: 1215, Loss: 2.530306, 12 norm of gradients: 1.084708, 12 norm of weights: 2.153401\n",
            "Iteration Number: 1216, Loss: 2.529124, 12 norm of gradients: 1.082422, 12 norm of weights: 2.152853\n",
            "Iteration Number: 1217, Loss: 2.527946, 12 norm of gradients: 1.080141, 12 norm of weights: 2.152306\n",
            "Iteration Number: 1218, Loss: 2.526772, 12 norm of gradients: 1.077866, 12 norm of weights: 2.151761\n",
            "Iteration Number: 1219, Loss: 2.525602, 12 norm of gradients: 1.075596, 12 norm of weights: 2.151218\n",
            "Iteration Number: 1220, Loss: 2.524435, 12 norm of gradients: 1.073331, 12 norm of weights: 2.150677\n",
            "Iteration Number: 1221, Loss: 2.523273, 12 norm of gradients: 1.071072, 12 norm of weights: 2.150137\n",
            "Iteration Number: 1222, Loss: 2.522114, 12 norm of gradients: 1.068817, 12 norm of weights: 2.149599\n",
            "Iteration Number: 1223, Loss: 2.520959, 12 norm of gradients: 1.066568, 12 norm of weights: 2.149062\n",
            "Iteration Number: 1224, Loss: 2.519808, 12 norm of gradients: 1.064324, 12 norm of weights: 2.148527\n",
            "Iteration Number: 1225, Loss: 2.518660, 12 norm of gradients: 1.062085, 12 norm of weights: 2.147994\n",
            "Iteration Number: 1226, Loss: 2.517517, 12 norm of gradients: 1.059851, 12 norm of weights: 2.147462\n",
            "Iteration Number: 1227, Loss: 2.516377, 12 norm of gradients: 1.057622, 12 norm of weights: 2.146932\n",
            "Iteration Number: 1228, Loss: 2.515241, 12 norm of gradients: 1.055398, 12 norm of weights: 2.146403\n",
            "Iteration Number: 1229, Loss: 2.514108, 12 norm of gradients: 1.053180, 12 norm of weights: 2.145876\n",
            "Iteration Number: 1230, Loss: 2.512980, 12 norm of gradients: 1.050967, 12 norm of weights: 2.145351\n",
            "Iteration Number: 1231, Loss: 2.511855, 12 norm of gradients: 1.048758, 12 norm of weights: 2.144827\n",
            "Iteration Number: 1232, Loss: 2.510733, 12 norm of gradients: 1.046555, 12 norm of weights: 2.144305\n",
            "Iteration Number: 1233, Loss: 2.509616, 12 norm of gradients: 1.044357, 12 norm of weights: 2.143784\n",
            "Iteration Number: 1234, Loss: 2.508502, 12 norm of gradients: 1.042164, 12 norm of weights: 2.143265\n",
            "Iteration Number: 1235, Loss: 2.507392, 12 norm of gradients: 1.039976, 12 norm of weights: 2.142748\n",
            "Iteration Number: 1236, Loss: 2.506285, 12 norm of gradients: 1.037793, 12 norm of weights: 2.142232\n",
            "Iteration Number: 1237, Loss: 2.505182, 12 norm of gradients: 1.035615, 12 norm of weights: 2.141718\n",
            "Iteration Number: 1238, Loss: 2.504083, 12 norm of gradients: 1.033442, 12 norm of weights: 2.141205\n",
            "Iteration Number: 1239, Loss: 2.502987, 12 norm of gradients: 1.031274, 12 norm of weights: 2.140694\n",
            "Iteration Number: 1240, Loss: 2.501895, 12 norm of gradients: 1.029111, 12 norm of weights: 2.140184\n",
            "Iteration Number: 1241, Loss: 2.500806, 12 norm of gradients: 1.026953, 12 norm of weights: 2.139676\n",
            "Iteration Number: 1242, Loss: 2.499721, 12 norm of gradients: 1.024800, 12 norm of weights: 2.139170\n",
            "Iteration Number: 1243, Loss: 2.498640, 12 norm of gradients: 1.022652, 12 norm of weights: 2.138665\n",
            "Iteration Number: 1244, Loss: 2.497562, 12 norm of gradients: 1.020509, 12 norm of weights: 2.138161\n",
            "Iteration Number: 1245, Loss: 2.496488, 12 norm of gradients: 1.018371, 12 norm of weights: 2.137659\n",
            "Iteration Number: 1246, Loss: 2.495417, 12 norm of gradients: 1.016238, 12 norm of weights: 2.137159\n",
            "Iteration Number: 1247, Loss: 2.494349, 12 norm of gradients: 1.014110, 12 norm of weights: 2.136660\n",
            "Iteration Number: 1248, Loss: 2.493286, 12 norm of gradients: 1.011986, 12 norm of weights: 2.136162\n",
            "Iteration Number: 1249, Loss: 2.492225, 12 norm of gradients: 1.009868, 12 norm of weights: 2.135666\n",
            "Iteration Number: 1250, Loss: 2.491168, 12 norm of gradients: 1.007754, 12 norm of weights: 2.135172\n",
            "Iteration Number: 1251, Loss: 2.490115, 12 norm of gradients: 1.005646, 12 norm of weights: 2.134679\n",
            "Iteration Number: 1252, Loss: 2.489065, 12 norm of gradients: 1.003542, 12 norm of weights: 2.134188\n",
            "Iteration Number: 1253, Loss: 2.488018, 12 norm of gradients: 1.001443, 12 norm of weights: 2.133698\n",
            "Iteration Number: 1254, Loss: 2.486975, 12 norm of gradients: 0.999349, 12 norm of weights: 2.133209\n",
            "Iteration Number: 1255, Loss: 2.485936, 12 norm of gradients: 0.997260, 12 norm of weights: 2.132722\n",
            "Iteration Number: 1256, Loss: 2.484899, 12 norm of gradients: 0.995175, 12 norm of weights: 2.132237\n",
            "Iteration Number: 1257, Loss: 2.483866, 12 norm of gradients: 0.993096, 12 norm of weights: 2.131753\n",
            "Iteration Number: 1258, Loss: 2.482837, 12 norm of gradients: 0.991021, 12 norm of weights: 2.131270\n",
            "Iteration Number: 1259, Loss: 2.481811, 12 norm of gradients: 0.988951, 12 norm of weights: 2.130789\n",
            "Iteration Number: 1260, Loss: 2.480788, 12 norm of gradients: 0.986886, 12 norm of weights: 2.130310\n",
            "Iteration Number: 1261, Loss: 2.479768, 12 norm of gradients: 0.984825, 12 norm of weights: 2.129831\n",
            "Iteration Number: 1262, Loss: 2.478752, 12 norm of gradients: 0.982770, 12 norm of weights: 2.129355\n",
            "Iteration Number: 1263, Loss: 2.477739, 12 norm of gradients: 0.980719, 12 norm of weights: 2.128879\n",
            "Iteration Number: 1264, Loss: 2.476730, 12 norm of gradients: 0.978673, 12 norm of weights: 2.128406\n",
            "Iteration Number: 1265, Loss: 2.475723, 12 norm of gradients: 0.976631, 12 norm of weights: 2.127933\n",
            "Iteration Number: 1266, Loss: 2.474720, 12 norm of gradients: 0.974594, 12 norm of weights: 2.127462\n",
            "Iteration Number: 1267, Loss: 2.473721, 12 norm of gradients: 0.972563, 12 norm of weights: 2.126993\n",
            "Iteration Number: 1268, Loss: 2.472724, 12 norm of gradients: 0.970535, 12 norm of weights: 2.126524\n",
            "Iteration Number: 1269, Loss: 2.471731, 12 norm of gradients: 0.968513, 12 norm of weights: 2.126058\n",
            "Iteration Number: 1270, Loss: 2.470741, 12 norm of gradients: 0.966495, 12 norm of weights: 2.125592\n",
            "Iteration Number: 1271, Loss: 2.469754, 12 norm of gradients: 0.964482, 12 norm of weights: 2.125129\n",
            "Iteration Number: 1272, Loss: 2.468771, 12 norm of gradients: 0.962473, 12 norm of weights: 2.124666\n",
            "Iteration Number: 1273, Loss: 2.467791, 12 norm of gradients: 0.960469, 12 norm of weights: 2.124205\n",
            "Iteration Number: 1274, Loss: 2.466813, 12 norm of gradients: 0.958470, 12 norm of weights: 2.123745\n",
            "Iteration Number: 1275, Loss: 2.465840, 12 norm of gradients: 0.956475, 12 norm of weights: 2.123287\n",
            "Iteration Number: 1276, Loss: 2.464869, 12 norm of gradients: 0.954485, 12 norm of weights: 2.122830\n",
            "Iteration Number: 1277, Loss: 2.463901, 12 norm of gradients: 0.952500, 12 norm of weights: 2.122375\n",
            "Iteration Number: 1278, Loss: 2.462937, 12 norm of gradients: 0.950519, 12 norm of weights: 2.121921\n",
            "Iteration Number: 1279, Loss: 2.461975, 12 norm of gradients: 0.948543, 12 norm of weights: 2.121468\n",
            "Iteration Number: 1280, Loss: 2.461017, 12 norm of gradients: 0.946571, 12 norm of weights: 2.121016\n",
            "Iteration Number: 1281, Loss: 2.460062, 12 norm of gradients: 0.944604, 12 norm of weights: 2.120566\n",
            "Iteration Number: 1282, Loss: 2.459110, 12 norm of gradients: 0.942642, 12 norm of weights: 2.120118\n",
            "Iteration Number: 1283, Loss: 2.458161, 12 norm of gradients: 0.940684, 12 norm of weights: 2.119670\n",
            "Iteration Number: 1284, Loss: 2.457216, 12 norm of gradients: 0.938731, 12 norm of weights: 2.119225\n",
            "Iteration Number: 1285, Loss: 2.456273, 12 norm of gradients: 0.936782, 12 norm of weights: 2.118780\n",
            "Iteration Number: 1286, Loss: 2.455333, 12 norm of gradients: 0.934837, 12 norm of weights: 2.118337\n",
            "Iteration Number: 1287, Loss: 2.454397, 12 norm of gradients: 0.932898, 12 norm of weights: 2.117895\n",
            "Iteration Number: 1288, Loss: 2.453463, 12 norm of gradients: 0.930962, 12 norm of weights: 2.117454\n",
            "Iteration Number: 1289, Loss: 2.452532, 12 norm of gradients: 0.929032, 12 norm of weights: 2.117015\n",
            "Iteration Number: 1290, Loss: 2.451605, 12 norm of gradients: 0.927105, 12 norm of weights: 2.116577\n",
            "Iteration Number: 1291, Loss: 2.450680, 12 norm of gradients: 0.925183, 12 norm of weights: 2.116140\n",
            "Iteration Number: 1292, Loss: 2.449759, 12 norm of gradients: 0.923266, 12 norm of weights: 2.115705\n",
            "Iteration Number: 1293, Loss: 2.448840, 12 norm of gradients: 0.921353, 12 norm of weights: 2.115271\n",
            "Iteration Number: 1294, Loss: 2.447925, 12 norm of gradients: 0.919445, 12 norm of weights: 2.114839\n",
            "Iteration Number: 1295, Loss: 2.447012, 12 norm of gradients: 0.917541, 12 norm of weights: 2.114407\n",
            "Iteration Number: 1296, Loss: 2.446103, 12 norm of gradients: 0.915641, 12 norm of weights: 2.113977\n",
            "Iteration Number: 1297, Loss: 2.445196, 12 norm of gradients: 0.913746, 12 norm of weights: 2.113549\n",
            "Iteration Number: 1298, Loss: 2.444292, 12 norm of gradients: 0.911855, 12 norm of weights: 2.113121\n",
            "Iteration Number: 1299, Loss: 2.443391, 12 norm of gradients: 0.909968, 12 norm of weights: 2.112695\n",
            "Iteration Number: 1300, Loss: 2.442494, 12 norm of gradients: 0.908086, 12 norm of weights: 2.112270\n",
            "Iteration Number: 1301, Loss: 2.441599, 12 norm of gradients: 0.906209, 12 norm of weights: 2.111847\n",
            "Iteration Number: 1302, Loss: 2.440706, 12 norm of gradients: 0.904335, 12 norm of weights: 2.111425\n",
            "Iteration Number: 1303, Loss: 2.439817, 12 norm of gradients: 0.902467, 12 norm of weights: 2.111004\n",
            "Iteration Number: 1304, Loss: 2.438931, 12 norm of gradients: 0.900602, 12 norm of weights: 2.110584\n",
            "Iteration Number: 1305, Loss: 2.438047, 12 norm of gradients: 0.898742, 12 norm of weights: 2.110165\n",
            "Iteration Number: 1306, Loss: 2.437167, 12 norm of gradients: 0.896886, 12 norm of weights: 2.109748\n",
            "Iteration Number: 1307, Loss: 2.436289, 12 norm of gradients: 0.895034, 12 norm of weights: 2.109332\n",
            "Iteration Number: 1308, Loss: 2.435414, 12 norm of gradients: 0.893187, 12 norm of weights: 2.108918\n",
            "Iteration Number: 1309, Loss: 2.434542, 12 norm of gradients: 0.891344, 12 norm of weights: 2.108504\n",
            "Iteration Number: 1310, Loss: 2.433673, 12 norm of gradients: 0.889505, 12 norm of weights: 2.108092\n",
            "Iteration Number: 1311, Loss: 2.432806, 12 norm of gradients: 0.887671, 12 norm of weights: 2.107681\n",
            "Iteration Number: 1312, Loss: 2.431943, 12 norm of gradients: 0.885840, 12 norm of weights: 2.107272\n",
            "Iteration Number: 1313, Loss: 2.431082, 12 norm of gradients: 0.884015, 12 norm of weights: 2.106863\n",
            "Iteration Number: 1314, Loss: 2.430224, 12 norm of gradients: 0.882193, 12 norm of weights: 2.106456\n",
            "Iteration Number: 1315, Loss: 2.429368, 12 norm of gradients: 0.880375, 12 norm of weights: 2.106050\n",
            "Iteration Number: 1316, Loss: 2.428516, 12 norm of gradients: 0.878562, 12 norm of weights: 2.105645\n",
            "Iteration Number: 1317, Loss: 2.427666, 12 norm of gradients: 0.876753, 12 norm of weights: 2.105242\n",
            "Iteration Number: 1318, Loss: 2.426819, 12 norm of gradients: 0.874948, 12 norm of weights: 2.104840\n",
            "Iteration Number: 1319, Loss: 2.425974, 12 norm of gradients: 0.873148, 12 norm of weights: 2.104438\n",
            "Iteration Number: 1320, Loss: 2.425133, 12 norm of gradients: 0.871351, 12 norm of weights: 2.104039\n",
            "Iteration Number: 1321, Loss: 2.424294, 12 norm of gradients: 0.869559, 12 norm of weights: 2.103640\n",
            "Iteration Number: 1322, Loss: 2.423458, 12 norm of gradients: 0.867771, 12 norm of weights: 2.103243\n",
            "Iteration Number: 1323, Loss: 2.422624, 12 norm of gradients: 0.865987, 12 norm of weights: 2.102846\n",
            "Iteration Number: 1324, Loss: 2.421793, 12 norm of gradients: 0.864208, 12 norm of weights: 2.102451\n",
            "Iteration Number: 1325, Loss: 2.420965, 12 norm of gradients: 0.862432, 12 norm of weights: 2.102057\n",
            "Iteration Number: 1326, Loss: 2.420139, 12 norm of gradients: 0.860661, 12 norm of weights: 2.101665\n",
            "Iteration Number: 1327, Loss: 2.419317, 12 norm of gradients: 0.858893, 12 norm of weights: 2.101273\n",
            "Iteration Number: 1328, Loss: 2.418496, 12 norm of gradients: 0.857130, 12 norm of weights: 2.100883\n",
            "Iteration Number: 1329, Loss: 2.417679, 12 norm of gradients: 0.855371, 12 norm of weights: 2.100494\n",
            "Iteration Number: 1330, Loss: 2.416864, 12 norm of gradients: 0.853616, 12 norm of weights: 2.100106\n",
            "Iteration Number: 1331, Loss: 2.416051, 12 norm of gradients: 0.851865, 12 norm of weights: 2.099719\n",
            "Iteration Number: 1332, Loss: 2.415242, 12 norm of gradients: 0.850118, 12 norm of weights: 2.099334\n",
            "Iteration Number: 1333, Loss: 2.414435, 12 norm of gradients: 0.848375, 12 norm of weights: 2.098949\n",
            "Iteration Number: 1334, Loss: 2.413630, 12 norm of gradients: 0.846637, 12 norm of weights: 2.098566\n",
            "Iteration Number: 1335, Loss: 2.412828, 12 norm of gradients: 0.844902, 12 norm of weights: 2.098184\n",
            "Iteration Number: 1336, Loss: 2.412029, 12 norm of gradients: 0.843171, 12 norm of weights: 2.097803\n",
            "Iteration Number: 1337, Loss: 2.411232, 12 norm of gradients: 0.841445, 12 norm of weights: 2.097423\n",
            "Iteration Number: 1338, Loss: 2.410438, 12 norm of gradients: 0.839722, 12 norm of weights: 2.097044\n",
            "Iteration Number: 1339, Loss: 2.409646, 12 norm of gradients: 0.838004, 12 norm of weights: 2.096667\n",
            "Iteration Number: 1340, Loss: 2.408857, 12 norm of gradients: 0.836289, 12 norm of weights: 2.096291\n",
            "Iteration Number: 1341, Loss: 2.408070, 12 norm of gradients: 0.834579, 12 norm of weights: 2.095915\n",
            "Iteration Number: 1342, Loss: 2.407286, 12 norm of gradients: 0.832872, 12 norm of weights: 2.095541\n",
            "Iteration Number: 1343, Loss: 2.406504, 12 norm of gradients: 0.831169, 12 norm of weights: 2.095168\n",
            "Iteration Number: 1344, Loss: 2.405725, 12 norm of gradients: 0.829471, 12 norm of weights: 2.094796\n",
            "Iteration Number: 1345, Loss: 2.404949, 12 norm of gradients: 0.827776, 12 norm of weights: 2.094426\n",
            "Iteration Number: 1346, Loss: 2.404174, 12 norm of gradients: 0.826085, 12 norm of weights: 2.094056\n",
            "Iteration Number: 1347, Loss: 2.403403, 12 norm of gradients: 0.824399, 12 norm of weights: 2.093687\n",
            "Iteration Number: 1348, Loss: 2.402634, 12 norm of gradients: 0.822716, 12 norm of weights: 2.093320\n",
            "Iteration Number: 1349, Loss: 2.401867, 12 norm of gradients: 0.821037, 12 norm of weights: 2.092954\n",
            "Iteration Number: 1350, Loss: 2.401103, 12 norm of gradients: 0.819362, 12 norm of weights: 2.092589\n",
            "Iteration Number: 1351, Loss: 2.400341, 12 norm of gradients: 0.817691, 12 norm of weights: 2.092225\n",
            "Iteration Number: 1352, Loss: 2.399581, 12 norm of gradients: 0.816023, 12 norm of weights: 2.091862\n",
            "Iteration Number: 1353, Loss: 2.398824, 12 norm of gradients: 0.814360, 12 norm of weights: 2.091500\n",
            "Iteration Number: 1354, Loss: 2.398070, 12 norm of gradients: 0.812701, 12 norm of weights: 2.091139\n",
            "Iteration Number: 1355, Loss: 2.397318, 12 norm of gradients: 0.811045, 12 norm of weights: 2.090779\n",
            "Iteration Number: 1356, Loss: 2.396568, 12 norm of gradients: 0.809393, 12 norm of weights: 2.090421\n",
            "Iteration Number: 1357, Loss: 2.395820, 12 norm of gradients: 0.807745, 12 norm of weights: 2.090063\n",
            "Iteration Number: 1358, Loss: 2.395075, 12 norm of gradients: 0.806101, 12 norm of weights: 2.089707\n",
            "Iteration Number: 1359, Loss: 2.394333, 12 norm of gradients: 0.804461, 12 norm of weights: 2.089351\n",
            "Iteration Number: 1360, Loss: 2.393593, 12 norm of gradients: 0.802825, 12 norm of weights: 2.088997\n",
            "Iteration Number: 1361, Loss: 2.392855, 12 norm of gradients: 0.801192, 12 norm of weights: 2.088644\n",
            "Iteration Number: 1362, Loss: 2.392119, 12 norm of gradients: 0.799563, 12 norm of weights: 2.088291\n",
            "Iteration Number: 1363, Loss: 2.391386, 12 norm of gradients: 0.797938, 12 norm of weights: 2.087940\n",
            "Iteration Number: 1364, Loss: 2.390655, 12 norm of gradients: 0.796317, 12 norm of weights: 2.087590\n",
            "Iteration Number: 1365, Loss: 2.389927, 12 norm of gradients: 0.794700, 12 norm of weights: 2.087241\n",
            "Iteration Number: 1366, Loss: 2.389200, 12 norm of gradients: 0.793086, 12 norm of weights: 2.086893\n",
            "Iteration Number: 1367, Loss: 2.388477, 12 norm of gradients: 0.791476, 12 norm of weights: 2.086546\n",
            "Iteration Number: 1368, Loss: 2.387755, 12 norm of gradients: 0.789870, 12 norm of weights: 2.086200\n",
            "Iteration Number: 1369, Loss: 2.387036, 12 norm of gradients: 0.788267, 12 norm of weights: 2.085856\n",
            "Iteration Number: 1370, Loss: 2.386319, 12 norm of gradients: 0.786669, 12 norm of weights: 2.085512\n",
            "Iteration Number: 1371, Loss: 2.385604, 12 norm of gradients: 0.785074, 12 norm of weights: 2.085169\n",
            "Iteration Number: 1372, Loss: 2.384892, 12 norm of gradients: 0.783483, 12 norm of weights: 2.084827\n",
            "Iteration Number: 1373, Loss: 2.384181, 12 norm of gradients: 0.781895, 12 norm of weights: 2.084487\n",
            "Iteration Number: 1374, Loss: 2.383473, 12 norm of gradients: 0.780311, 12 norm of weights: 2.084147\n",
            "Iteration Number: 1375, Loss: 2.382768, 12 norm of gradients: 0.778731, 12 norm of weights: 2.083808\n",
            "Iteration Number: 1376, Loss: 2.382064, 12 norm of gradients: 0.777155, 12 norm of weights: 2.083471\n",
            "Iteration Number: 1377, Loss: 2.381363, 12 norm of gradients: 0.775582, 12 norm of weights: 2.083134\n",
            "Iteration Number: 1378, Loss: 2.380664, 12 norm of gradients: 0.774013, 12 norm of weights: 2.082798\n",
            "Iteration Number: 1379, Loss: 2.379967, 12 norm of gradients: 0.772447, 12 norm of weights: 2.082464\n",
            "Iteration Number: 1380, Loss: 2.379273, 12 norm of gradients: 0.770885, 12 norm of weights: 2.082130\n",
            "Iteration Number: 1381, Loss: 2.378580, 12 norm of gradients: 0.769327, 12 norm of weights: 2.081797\n",
            "Iteration Number: 1382, Loss: 2.377890, 12 norm of gradients: 0.767772, 12 norm of weights: 2.081466\n",
            "Iteration Number: 1383, Loss: 2.377202, 12 norm of gradients: 0.766222, 12 norm of weights: 2.081135\n",
            "Iteration Number: 1384, Loss: 2.376516, 12 norm of gradients: 0.764674, 12 norm of weights: 2.080806\n",
            "Iteration Number: 1385, Loss: 2.375833, 12 norm of gradients: 0.763130, 12 norm of weights: 2.080477\n",
            "Iteration Number: 1386, Loss: 2.375151, 12 norm of gradients: 0.761590, 12 norm of weights: 2.080149\n",
            "Iteration Number: 1387, Loss: 2.374472, 12 norm of gradients: 0.760054, 12 norm of weights: 2.079823\n",
            "Iteration Number: 1388, Loss: 2.373795, 12 norm of gradients: 0.758521, 12 norm of weights: 2.079497\n",
            "Iteration Number: 1389, Loss: 2.373120, 12 norm of gradients: 0.756991, 12 norm of weights: 2.079172\n",
            "Iteration Number: 1390, Loss: 2.372447, 12 norm of gradients: 0.755465, 12 norm of weights: 2.078849\n",
            "Iteration Number: 1391, Loss: 2.371776, 12 norm of gradients: 0.753943, 12 norm of weights: 2.078526\n",
            "Iteration Number: 1392, Loss: 2.371107, 12 norm of gradients: 0.752424, 12 norm of weights: 2.078204\n",
            "Iteration Number: 1393, Loss: 2.370441, 12 norm of gradients: 0.750909, 12 norm of weights: 2.077883\n",
            "Iteration Number: 1394, Loss: 2.369776, 12 norm of gradients: 0.749397, 12 norm of weights: 2.077563\n",
            "Iteration Number: 1395, Loss: 2.369114, 12 norm of gradients: 0.747889, 12 norm of weights: 2.077244\n",
            "Iteration Number: 1396, Loss: 2.368454, 12 norm of gradients: 0.746385, 12 norm of weights: 2.076927\n",
            "Iteration Number: 1397, Loss: 2.367796, 12 norm of gradients: 0.744883, 12 norm of weights: 2.076610\n",
            "Iteration Number: 1398, Loss: 2.367140, 12 norm of gradients: 0.743386, 12 norm of weights: 2.076294\n",
            "Iteration Number: 1399, Loss: 2.366486, 12 norm of gradients: 0.741891, 12 norm of weights: 2.075978\n",
            "Iteration Number: 1400, Loss: 2.365834, 12 norm of gradients: 0.740401, 12 norm of weights: 2.075664\n",
            "Iteration Number: 1401, Loss: 2.365184, 12 norm of gradients: 0.738913, 12 norm of weights: 2.075351\n",
            "Iteration Number: 1402, Loss: 2.364536, 12 norm of gradients: 0.737430, 12 norm of weights: 2.075039\n",
            "Iteration Number: 1403, Loss: 2.363890, 12 norm of gradients: 0.735949, 12 norm of weights: 2.074727\n",
            "Iteration Number: 1404, Loss: 2.363246, 12 norm of gradients: 0.734472, 12 norm of weights: 2.074417\n",
            "Iteration Number: 1405, Loss: 2.362605, 12 norm of gradients: 0.732999, 12 norm of weights: 2.074108\n",
            "Iteration Number: 1406, Loss: 2.361965, 12 norm of gradients: 0.731529, 12 norm of weights: 2.073799\n",
            "Iteration Number: 1407, Loss: 2.361327, 12 norm of gradients: 0.730062, 12 norm of weights: 2.073491\n",
            "Iteration Number: 1408, Loss: 2.360691, 12 norm of gradients: 0.728599, 12 norm of weights: 2.073185\n",
            "Iteration Number: 1409, Loss: 2.360058, 12 norm of gradients: 0.727139, 12 norm of weights: 2.072879\n",
            "Iteration Number: 1410, Loss: 2.359426, 12 norm of gradients: 0.725683, 12 norm of weights: 2.072574\n",
            "Iteration Number: 1411, Loss: 2.358796, 12 norm of gradients: 0.724230, 12 norm of weights: 2.072270\n",
            "Iteration Number: 1412, Loss: 2.358168, 12 norm of gradients: 0.722780, 12 norm of weights: 2.071967\n",
            "Iteration Number: 1413, Loss: 2.357543, 12 norm of gradients: 0.721334, 12 norm of weights: 2.071665\n",
            "Iteration Number: 1414, Loss: 2.356919, 12 norm of gradients: 0.719891, 12 norm of weights: 2.071364\n",
            "Iteration Number: 1415, Loss: 2.356297, 12 norm of gradients: 0.718451, 12 norm of weights: 2.071063\n",
            "Iteration Number: 1416, Loss: 2.355677, 12 norm of gradients: 0.717015, 12 norm of weights: 2.070764\n",
            "Iteration Number: 1417, Loss: 2.355059, 12 norm of gradients: 0.715582, 12 norm of weights: 2.070465\n",
            "Iteration Number: 1418, Loss: 2.354443, 12 norm of gradients: 0.714153, 12 norm of weights: 2.070168\n",
            "Iteration Number: 1419, Loss: 2.353829, 12 norm of gradients: 0.712726, 12 norm of weights: 2.069871\n",
            "Iteration Number: 1420, Loss: 2.353217, 12 norm of gradients: 0.711303, 12 norm of weights: 2.069575\n",
            "Iteration Number: 1421, Loss: 2.352606, 12 norm of gradients: 0.709884, 12 norm of weights: 2.069280\n",
            "Iteration Number: 1422, Loss: 2.351998, 12 norm of gradients: 0.708467, 12 norm of weights: 2.068986\n",
            "Iteration Number: 1423, Loss: 2.351392, 12 norm of gradients: 0.707054, 12 norm of weights: 2.068693\n",
            "Iteration Number: 1424, Loss: 2.350787, 12 norm of gradients: 0.705645, 12 norm of weights: 2.068400\n",
            "Iteration Number: 1425, Loss: 2.350184, 12 norm of gradients: 0.704238, 12 norm of weights: 2.068109\n",
            "Iteration Number: 1426, Loss: 2.349584, 12 norm of gradients: 0.702835, 12 norm of weights: 2.067818\n",
            "Iteration Number: 1427, Loss: 2.348985, 12 norm of gradients: 0.701435, 12 norm of weights: 2.067528\n",
            "Iteration Number: 1428, Loss: 2.348388, 12 norm of gradients: 0.700038, 12 norm of weights: 2.067239\n",
            "Iteration Number: 1429, Loss: 2.347793, 12 norm of gradients: 0.698645, 12 norm of weights: 2.066951\n",
            "Iteration Number: 1430, Loss: 2.347199, 12 norm of gradients: 0.697254, 12 norm of weights: 2.066664\n",
            "Iteration Number: 1431, Loss: 2.346608, 12 norm of gradients: 0.695867, 12 norm of weights: 2.066378\n",
            "Iteration Number: 1432, Loss: 2.346018, 12 norm of gradients: 0.694484, 12 norm of weights: 2.066092\n",
            "Iteration Number: 1433, Loss: 2.345430, 12 norm of gradients: 0.693103, 12 norm of weights: 2.065808\n",
            "Iteration Number: 1434, Loss: 2.344845, 12 norm of gradients: 0.691726, 12 norm of weights: 2.065524\n",
            "Iteration Number: 1435, Loss: 2.344260, 12 norm of gradients: 0.690351, 12 norm of weights: 2.065241\n",
            "Iteration Number: 1436, Loss: 2.343678, 12 norm of gradients: 0.688980, 12 norm of weights: 2.064959\n",
            "Iteration Number: 1437, Loss: 2.343098, 12 norm of gradients: 0.687612, 12 norm of weights: 2.064678\n",
            "Iteration Number: 1438, Loss: 2.342519, 12 norm of gradients: 0.686248, 12 norm of weights: 2.064397\n",
            "Iteration Number: 1439, Loss: 2.341942, 12 norm of gradients: 0.684886, 12 norm of weights: 2.064118\n",
            "Iteration Number: 1440, Loss: 2.341367, 12 norm of gradients: 0.683528, 12 norm of weights: 2.063839\n",
            "Iteration Number: 1441, Loss: 2.340794, 12 norm of gradients: 0.682173, 12 norm of weights: 2.063561\n",
            "Iteration Number: 1442, Loss: 2.340222, 12 norm of gradients: 0.680821, 12 norm of weights: 2.063284\n",
            "Iteration Number: 1443, Loss: 2.339653, 12 norm of gradients: 0.679472, 12 norm of weights: 2.063008\n",
            "Iteration Number: 1444, Loss: 2.339085, 12 norm of gradients: 0.678126, 12 norm of weights: 2.062732\n",
            "Iteration Number: 1445, Loss: 2.338519, 12 norm of gradients: 0.676783, 12 norm of weights: 2.062458\n",
            "Iteration Number: 1446, Loss: 2.337954, 12 norm of gradients: 0.675444, 12 norm of weights: 2.062184\n",
            "Iteration Number: 1447, Loss: 2.337392, 12 norm of gradients: 0.674107, 12 norm of weights: 2.061911\n",
            "Iteration Number: 1448, Loss: 2.336831, 12 norm of gradients: 0.672774, 12 norm of weights: 2.061639\n",
            "Iteration Number: 1449, Loss: 2.336271, 12 norm of gradients: 0.671443, 12 norm of weights: 2.061367\n",
            "Iteration Number: 1450, Loss: 2.335714, 12 norm of gradients: 0.670116, 12 norm of weights: 2.061097\n",
            "Iteration Number: 1451, Loss: 2.335158, 12 norm of gradients: 0.668792, 12 norm of weights: 2.060827\n",
            "Iteration Number: 1452, Loss: 2.334604, 12 norm of gradients: 0.667471, 12 norm of weights: 2.060558\n",
            "Iteration Number: 1453, Loss: 2.334052, 12 norm of gradients: 0.666153, 12 norm of weights: 2.060290\n",
            "Iteration Number: 1454, Loss: 2.333502, 12 norm of gradients: 0.664838, 12 norm of weights: 2.060023\n",
            "Iteration Number: 1455, Loss: 2.332953, 12 norm of gradients: 0.663526, 12 norm of weights: 2.059756\n",
            "Iteration Number: 1456, Loss: 2.332406, 12 norm of gradients: 0.662217, 12 norm of weights: 2.059490\n",
            "Iteration Number: 1457, Loss: 2.331860, 12 norm of gradients: 0.660911, 12 norm of weights: 2.059225\n",
            "Iteration Number: 1458, Loss: 2.331316, 12 norm of gradients: 0.659608, 12 norm of weights: 2.058961\n",
            "Iteration Number: 1459, Loss: 2.330774, 12 norm of gradients: 0.658309, 12 norm of weights: 2.058698\n",
            "Iteration Number: 1460, Loss: 2.330234, 12 norm of gradients: 0.657012, 12 norm of weights: 2.058435\n",
            "Iteration Number: 1461, Loss: 2.329695, 12 norm of gradients: 0.655718, 12 norm of weights: 2.058173\n",
            "Iteration Number: 1462, Loss: 2.329158, 12 norm of gradients: 0.654427, 12 norm of weights: 2.057912\n",
            "Iteration Number: 1463, Loss: 2.328623, 12 norm of gradients: 0.653139, 12 norm of weights: 2.057652\n",
            "Iteration Number: 1464, Loss: 2.328089, 12 norm of gradients: 0.651854, 12 norm of weights: 2.057392\n",
            "Iteration Number: 1465, Loss: 2.327557, 12 norm of gradients: 0.650572, 12 norm of weights: 2.057133\n",
            "Iteration Number: 1466, Loss: 2.327026, 12 norm of gradients: 0.649293, 12 norm of weights: 2.056875\n",
            "Iteration Number: 1467, Loss: 2.326497, 12 norm of gradients: 0.648017, 12 norm of weights: 2.056618\n",
            "Iteration Number: 1468, Loss: 2.325970, 12 norm of gradients: 0.646744, 12 norm of weights: 2.056362\n",
            "Iteration Number: 1469, Loss: 2.325445, 12 norm of gradients: 0.645474, 12 norm of weights: 2.056106\n",
            "Iteration Number: 1470, Loss: 2.324921, 12 norm of gradients: 0.644207, 12 norm of weights: 2.055851\n",
            "Iteration Number: 1471, Loss: 2.324398, 12 norm of gradients: 0.642943, 12 norm of weights: 2.055597\n",
            "Iteration Number: 1472, Loss: 2.323878, 12 norm of gradients: 0.641681, 12 norm of weights: 2.055343\n",
            "Iteration Number: 1473, Loss: 2.323359, 12 norm of gradients: 0.640423, 12 norm of weights: 2.055091\n",
            "Iteration Number: 1474, Loss: 2.322841, 12 norm of gradients: 0.639167, 12 norm of weights: 2.054839\n",
            "Iteration Number: 1475, Loss: 2.322325, 12 norm of gradients: 0.637915, 12 norm of weights: 2.054587\n",
            "Iteration Number: 1476, Loss: 2.321811, 12 norm of gradients: 0.636665, 12 norm of weights: 2.054337\n",
            "Iteration Number: 1477, Loss: 2.321298, 12 norm of gradients: 0.635418, 12 norm of weights: 2.054087\n",
            "Iteration Number: 1478, Loss: 2.320787, 12 norm of gradients: 0.634174, 12 norm of weights: 2.053838\n",
            "Iteration Number: 1479, Loss: 2.320277, 12 norm of gradients: 0.632933, 12 norm of weights: 2.053590\n",
            "Iteration Number: 1480, Loss: 2.319769, 12 norm of gradients: 0.631695, 12 norm of weights: 2.053342\n",
            "Iteration Number: 1481, Loss: 2.319263, 12 norm of gradients: 0.630459, 12 norm of weights: 2.053096\n",
            "Iteration Number: 1482, Loss: 2.318758, 12 norm of gradients: 0.629227, 12 norm of weights: 2.052849\n",
            "Iteration Number: 1483, Loss: 2.318254, 12 norm of gradients: 0.627997, 12 norm of weights: 2.052604\n",
            "Iteration Number: 1484, Loss: 2.317753, 12 norm of gradients: 0.626770, 12 norm of weights: 2.052359\n",
            "Iteration Number: 1485, Loss: 2.317252, 12 norm of gradients: 0.625546, 12 norm of weights: 2.052116\n",
            "Iteration Number: 1486, Loss: 2.316754, 12 norm of gradients: 0.624324, 12 norm of weights: 2.051872\n",
            "Iteration Number: 1487, Loss: 2.316256, 12 norm of gradients: 0.623106, 12 norm of weights: 2.051630\n",
            "Iteration Number: 1488, Loss: 2.315761, 12 norm of gradients: 0.621890, 12 norm of weights: 2.051388\n",
            "Iteration Number: 1489, Loss: 2.315266, 12 norm of gradients: 0.620677, 12 norm of weights: 2.051147\n",
            "Iteration Number: 1490, Loss: 2.314774, 12 norm of gradients: 0.619467, 12 norm of weights: 2.050907\n",
            "Iteration Number: 1491, Loss: 2.314283, 12 norm of gradients: 0.618260, 12 norm of weights: 2.050667\n",
            "Iteration Number: 1492, Loss: 2.313793, 12 norm of gradients: 0.617056, 12 norm of weights: 2.050428\n",
            "Iteration Number: 1493, Loss: 2.313305, 12 norm of gradients: 0.615854, 12 norm of weights: 2.050190\n",
            "Iteration Number: 1494, Loss: 2.312818, 12 norm of gradients: 0.614655, 12 norm of weights: 2.049952\n",
            "Iteration Number: 1495, Loss: 2.312333, 12 norm of gradients: 0.613459, 12 norm of weights: 2.049716\n",
            "Iteration Number: 1496, Loss: 2.311849, 12 norm of gradients: 0.612265, 12 norm of weights: 2.049479\n",
            "Iteration Number: 1497, Loss: 2.311367, 12 norm of gradients: 0.611074, 12 norm of weights: 2.049244\n",
            "Iteration Number: 1498, Loss: 2.310886, 12 norm of gradients: 0.609886, 12 norm of weights: 2.049009\n",
            "Iteration Number: 1499, Loss: 2.310407, 12 norm of gradients: 0.608701, 12 norm of weights: 2.048775\n",
            "Iteration Number: 1500, Loss: 2.309929, 12 norm of gradients: 0.607519, 12 norm of weights: 2.048542\n",
            "Iteration Number: 1501, Loss: 2.309453, 12 norm of gradients: 0.606339, 12 norm of weights: 2.048309\n",
            "Iteration Number: 1502, Loss: 2.308978, 12 norm of gradients: 0.605162, 12 norm of weights: 2.048077\n",
            "Iteration Number: 1503, Loss: 2.308504, 12 norm of gradients: 0.603987, 12 norm of weights: 2.047846\n",
            "Iteration Number: 1504, Loss: 2.308032, 12 norm of gradients: 0.602815, 12 norm of weights: 2.047615\n",
            "Iteration Number: 1505, Loss: 2.307562, 12 norm of gradients: 0.601646, 12 norm of weights: 2.047385\n",
            "Iteration Number: 1506, Loss: 2.307093, 12 norm of gradients: 0.600480, 12 norm of weights: 2.047156\n",
            "Iteration Number: 1507, Loss: 2.306625, 12 norm of gradients: 0.599316, 12 norm of weights: 2.046927\n",
            "Iteration Number: 1508, Loss: 2.306159, 12 norm of gradients: 0.598155, 12 norm of weights: 2.046699\n",
            "Iteration Number: 1509, Loss: 2.305694, 12 norm of gradients: 0.596997, 12 norm of weights: 2.046472\n",
            "Iteration Number: 1510, Loss: 2.305230, 12 norm of gradients: 0.595841, 12 norm of weights: 2.046246\n",
            "Iteration Number: 1511, Loss: 2.304768, 12 norm of gradients: 0.594688, 12 norm of weights: 2.046020\n",
            "Iteration Number: 1512, Loss: 2.304308, 12 norm of gradients: 0.593538, 12 norm of weights: 2.045794\n",
            "Iteration Number: 1513, Loss: 2.303848, 12 norm of gradients: 0.592390, 12 norm of weights: 2.045570\n",
            "Iteration Number: 1514, Loss: 2.303390, 12 norm of gradients: 0.591245, 12 norm of weights: 2.045346\n",
            "Iteration Number: 1515, Loss: 2.302934, 12 norm of gradients: 0.590102, 12 norm of weights: 2.045122\n",
            "Iteration Number: 1516, Loss: 2.302479, 12 norm of gradients: 0.588962, 12 norm of weights: 2.044900\n",
            "Iteration Number: 1517, Loss: 2.302025, 12 norm of gradients: 0.587825, 12 norm of weights: 2.044678\n",
            "Iteration Number: 1518, Loss: 2.301573, 12 norm of gradients: 0.586690, 12 norm of weights: 2.044456\n",
            "Iteration Number: 1519, Loss: 2.301122, 12 norm of gradients: 0.585558, 12 norm of weights: 2.044236\n",
            "Iteration Number: 1520, Loss: 2.300672, 12 norm of gradients: 0.584429, 12 norm of weights: 2.044016\n",
            "Iteration Number: 1521, Loss: 2.300224, 12 norm of gradients: 0.583302, 12 norm of weights: 2.043796\n",
            "Iteration Number: 1522, Loss: 2.299777, 12 norm of gradients: 0.582177, 12 norm of weights: 2.043577\n",
            "Iteration Number: 1523, Loss: 2.299332, 12 norm of gradients: 0.581056, 12 norm of weights: 2.043359\n",
            "Iteration Number: 1524, Loss: 2.298887, 12 norm of gradients: 0.579936, 12 norm of weights: 2.043142\n",
            "Iteration Number: 1525, Loss: 2.298445, 12 norm of gradients: 0.578820, 12 norm of weights: 2.042925\n",
            "Iteration Number: 1526, Loss: 2.298003, 12 norm of gradients: 0.577706, 12 norm of weights: 2.042709\n",
            "Iteration Number: 1527, Loss: 2.297563, 12 norm of gradients: 0.576594, 12 norm of weights: 2.042493\n",
            "Iteration Number: 1528, Loss: 2.297124, 12 norm of gradients: 0.575485, 12 norm of weights: 2.042278\n",
            "Iteration Number: 1529, Loss: 2.296687, 12 norm of gradients: 0.574378, 12 norm of weights: 2.042064\n",
            "Iteration Number: 1530, Loss: 2.296250, 12 norm of gradients: 0.573274, 12 norm of weights: 2.041850\n",
            "Iteration Number: 1531, Loss: 2.295815, 12 norm of gradients: 0.572173, 12 norm of weights: 2.041637\n",
            "Iteration Number: 1532, Loss: 2.295382, 12 norm of gradients: 0.571074, 12 norm of weights: 2.041424\n",
            "Iteration Number: 1533, Loss: 2.294950, 12 norm of gradients: 0.569978, 12 norm of weights: 2.041213\n",
            "Iteration Number: 1534, Loss: 2.294519, 12 norm of gradients: 0.568884, 12 norm of weights: 2.041001\n",
            "Iteration Number: 1535, Loss: 2.294089, 12 norm of gradients: 0.567792, 12 norm of weights: 2.040791\n",
            "Iteration Number: 1536, Loss: 2.293661, 12 norm of gradients: 0.566703, 12 norm of weights: 2.040581\n",
            "Iteration Number: 1537, Loss: 2.293234, 12 norm of gradients: 0.565617, 12 norm of weights: 2.040371\n",
            "Iteration Number: 1538, Loss: 2.292808, 12 norm of gradients: 0.564533, 12 norm of weights: 2.040162\n",
            "Iteration Number: 1539, Loss: 2.292383, 12 norm of gradients: 0.563451, 12 norm of weights: 2.039954\n",
            "Iteration Number: 1540, Loss: 2.291960, 12 norm of gradients: 0.562372, 12 norm of weights: 2.039747\n",
            "Iteration Number: 1541, Loss: 2.291538, 12 norm of gradients: 0.561295, 12 norm of weights: 2.039540\n",
            "Iteration Number: 1542, Loss: 2.291117, 12 norm of gradients: 0.560221, 12 norm of weights: 2.039333\n",
            "Iteration Number: 1543, Loss: 2.290698, 12 norm of gradients: 0.559150, 12 norm of weights: 2.039127\n",
            "Iteration Number: 1544, Loss: 2.290280, 12 norm of gradients: 0.558080, 12 norm of weights: 2.038922\n",
            "Iteration Number: 1545, Loss: 2.289863, 12 norm of gradients: 0.557013, 12 norm of weights: 2.038718\n",
            "Iteration Number: 1546, Loss: 2.289447, 12 norm of gradients: 0.555949, 12 norm of weights: 2.038514\n",
            "Iteration Number: 1547, Loss: 2.289033, 12 norm of gradients: 0.554887, 12 norm of weights: 2.038310\n",
            "Iteration Number: 1548, Loss: 2.288620, 12 norm of gradients: 0.553827, 12 norm of weights: 2.038107\n",
            "Iteration Number: 1549, Loss: 2.288208, 12 norm of gradients: 0.552770, 12 norm of weights: 2.037905\n",
            "Iteration Number: 1550, Loss: 2.287797, 12 norm of gradients: 0.551715, 12 norm of weights: 2.037704\n",
            "Iteration Number: 1551, Loss: 2.287388, 12 norm of gradients: 0.550663, 12 norm of weights: 2.037503\n",
            "Iteration Number: 1552, Loss: 2.286979, 12 norm of gradients: 0.549613, 12 norm of weights: 2.037302\n",
            "Iteration Number: 1553, Loss: 2.286572, 12 norm of gradients: 0.548565, 12 norm of weights: 2.037102\n",
            "Iteration Number: 1554, Loss: 2.286167, 12 norm of gradients: 0.547520, 12 norm of weights: 2.036903\n",
            "Iteration Number: 1555, Loss: 2.285762, 12 norm of gradients: 0.546477, 12 norm of weights: 2.036704\n",
            "Iteration Number: 1556, Loss: 2.285359, 12 norm of gradients: 0.545437, 12 norm of weights: 2.036506\n",
            "Iteration Number: 1557, Loss: 2.284956, 12 norm of gradients: 0.544399, 12 norm of weights: 2.036308\n",
            "Iteration Number: 1558, Loss: 2.284555, 12 norm of gradients: 0.543363, 12 norm of weights: 2.036111\n",
            "Iteration Number: 1559, Loss: 2.284156, 12 norm of gradients: 0.542329, 12 norm of weights: 2.035915\n",
            "Iteration Number: 1560, Loss: 2.283757, 12 norm of gradients: 0.541298, 12 norm of weights: 2.035719\n",
            "Iteration Number: 1561, Loss: 2.283360, 12 norm of gradients: 0.540269, 12 norm of weights: 2.035524\n",
            "Iteration Number: 1562, Loss: 2.282963, 12 norm of gradients: 0.539243, 12 norm of weights: 2.035329\n",
            "Iteration Number: 1563, Loss: 2.282568, 12 norm of gradients: 0.538219, 12 norm of weights: 2.035135\n",
            "Iteration Number: 1564, Loss: 2.282175, 12 norm of gradients: 0.537197, 12 norm of weights: 2.034941\n",
            "Iteration Number: 1565, Loss: 2.281782, 12 norm of gradients: 0.536178, 12 norm of weights: 2.034748\n",
            "Iteration Number: 1566, Loss: 2.281390, 12 norm of gradients: 0.535160, 12 norm of weights: 2.034556\n",
            "Iteration Number: 1567, Loss: 2.281000, 12 norm of gradients: 0.534146, 12 norm of weights: 2.034364\n",
            "Iteration Number: 1568, Loss: 2.280611, 12 norm of gradients: 0.533133, 12 norm of weights: 2.034172\n",
            "Iteration Number: 1569, Loss: 2.280223, 12 norm of gradients: 0.532123, 12 norm of weights: 2.033981\n",
            "Iteration Number: 1570, Loss: 2.279836, 12 norm of gradients: 0.531115, 12 norm of weights: 2.033791\n",
            "Iteration Number: 1571, Loss: 2.279450, 12 norm of gradients: 0.530109, 12 norm of weights: 2.033601\n",
            "Iteration Number: 1572, Loss: 2.279066, 12 norm of gradients: 0.529105, 12 norm of weights: 2.033412\n",
            "Iteration Number: 1573, Loss: 2.278682, 12 norm of gradients: 0.528104, 12 norm of weights: 2.033223\n",
            "Iteration Number: 1574, Loss: 2.278300, 12 norm of gradients: 0.527105, 12 norm of weights: 2.033035\n",
            "Iteration Number: 1575, Loss: 2.277919, 12 norm of gradients: 0.526109, 12 norm of weights: 2.032848\n",
            "Iteration Number: 1576, Loss: 2.277539, 12 norm of gradients: 0.525114, 12 norm of weights: 2.032661\n",
            "Iteration Number: 1577, Loss: 2.277160, 12 norm of gradients: 0.524122, 12 norm of weights: 2.032474\n",
            "Iteration Number: 1578, Loss: 2.276782, 12 norm of gradients: 0.523132, 12 norm of weights: 2.032288\n",
            "Iteration Number: 1579, Loss: 2.276405, 12 norm of gradients: 0.522144, 12 norm of weights: 2.032103\n",
            "Iteration Number: 1580, Loss: 2.276030, 12 norm of gradients: 0.521159, 12 norm of weights: 2.031918\n",
            "Iteration Number: 1581, Loss: 2.275655, 12 norm of gradients: 0.520176, 12 norm of weights: 2.031733\n",
            "Iteration Number: 1582, Loss: 2.275282, 12 norm of gradients: 0.519195, 12 norm of weights: 2.031550\n",
            "Iteration Number: 1583, Loss: 2.274910, 12 norm of gradients: 0.518216, 12 norm of weights: 2.031366\n",
            "Iteration Number: 1584, Loss: 2.274538, 12 norm of gradients: 0.517239, 12 norm of weights: 2.031184\n",
            "Iteration Number: 1585, Loss: 2.274168, 12 norm of gradients: 0.516265, 12 norm of weights: 2.031001\n",
            "Iteration Number: 1586, Loss: 2.273799, 12 norm of gradients: 0.515293, 12 norm of weights: 2.030820\n",
            "Iteration Number: 1587, Loss: 2.273432, 12 norm of gradients: 0.514323, 12 norm of weights: 2.030638\n",
            "Iteration Number: 1588, Loss: 2.273065, 12 norm of gradients: 0.513355, 12 norm of weights: 2.030458\n",
            "Iteration Number: 1589, Loss: 2.272699, 12 norm of gradients: 0.512389, 12 norm of weights: 2.030277\n",
            "Iteration Number: 1590, Loss: 2.272335, 12 norm of gradients: 0.511425, 12 norm of weights: 2.030098\n",
            "Iteration Number: 1591, Loss: 2.271971, 12 norm of gradients: 0.510464, 12 norm of weights: 2.029919\n",
            "Iteration Number: 1592, Loss: 2.271608, 12 norm of gradients: 0.509505, 12 norm of weights: 2.029740\n",
            "Iteration Number: 1593, Loss: 2.271247, 12 norm of gradients: 0.508548, 12 norm of weights: 2.029562\n",
            "Iteration Number: 1594, Loss: 2.270887, 12 norm of gradients: 0.507593, 12 norm of weights: 2.029384\n",
            "Iteration Number: 1595, Loss: 2.270527, 12 norm of gradients: 0.506640, 12 norm of weights: 2.029207\n",
            "Iteration Number: 1596, Loss: 2.270169, 12 norm of gradients: 0.505690, 12 norm of weights: 2.029031\n",
            "Iteration Number: 1597, Loss: 2.269812, 12 norm of gradients: 0.504741, 12 norm of weights: 2.028854\n",
            "Iteration Number: 1598, Loss: 2.269456, 12 norm of gradients: 0.503795, 12 norm of weights: 2.028679\n",
            "Iteration Number: 1599, Loss: 2.269101, 12 norm of gradients: 0.502851, 12 norm of weights: 2.028504\n",
            "Iteration Number: 1600, Loss: 2.268747, 12 norm of gradients: 0.501909, 12 norm of weights: 2.028329\n",
            "Iteration Number: 1601, Loss: 2.268394, 12 norm of gradients: 0.500969, 12 norm of weights: 2.028155\n",
            "Iteration Number: 1602, Loss: 2.268042, 12 norm of gradients: 0.500031, 12 norm of weights: 2.027981\n",
            "Iteration Number: 1603, Loss: 2.267691, 12 norm of gradients: 0.499095, 12 norm of weights: 2.027808\n",
            "Iteration Number: 1604, Loss: 2.267341, 12 norm of gradients: 0.498161, 12 norm of weights: 2.027636\n",
            "Iteration Number: 1605, Loss: 2.266992, 12 norm of gradients: 0.497230, 12 norm of weights: 2.027464\n",
            "Iteration Number: 1606, Loss: 2.266644, 12 norm of gradients: 0.496300, 12 norm of weights: 2.027292\n",
            "Iteration Number: 1607, Loss: 2.266297, 12 norm of gradients: 0.495373, 12 norm of weights: 2.027121\n",
            "Iteration Number: 1608, Loss: 2.265952, 12 norm of gradients: 0.494447, 12 norm of weights: 2.026950\n",
            "Iteration Number: 1609, Loss: 2.265607, 12 norm of gradients: 0.493524, 12 norm of weights: 2.026780\n",
            "Iteration Number: 1610, Loss: 2.265263, 12 norm of gradients: 0.492603, 12 norm of weights: 2.026610\n",
            "Iteration Number: 1611, Loss: 2.264920, 12 norm of gradients: 0.491684, 12 norm of weights: 2.026441\n",
            "Iteration Number: 1612, Loss: 2.264578, 12 norm of gradients: 0.490767, 12 norm of weights: 2.026272\n",
            "Iteration Number: 1613, Loss: 2.264238, 12 norm of gradients: 0.489852, 12 norm of weights: 2.026104\n",
            "Iteration Number: 1614, Loss: 2.263898, 12 norm of gradients: 0.488939, 12 norm of weights: 2.025936\n",
            "Iteration Number: 1615, Loss: 2.263559, 12 norm of gradients: 0.488028, 12 norm of weights: 2.025769\n",
            "Iteration Number: 1616, Loss: 2.263221, 12 norm of gradients: 0.487119, 12 norm of weights: 2.025602\n",
            "Iteration Number: 1617, Loss: 2.262884, 12 norm of gradients: 0.486212, 12 norm of weights: 2.025436\n",
            "Iteration Number: 1618, Loss: 2.262548, 12 norm of gradients: 0.485307, 12 norm of weights: 2.025270\n",
            "Iteration Number: 1619, Loss: 2.262214, 12 norm of gradients: 0.484404, 12 norm of weights: 2.025105\n",
            "Iteration Number: 1620, Loss: 2.261880, 12 norm of gradients: 0.483503, 12 norm of weights: 2.024940\n",
            "Iteration Number: 1621, Loss: 2.261547, 12 norm of gradients: 0.482605, 12 norm of weights: 2.024775\n",
            "Iteration Number: 1622, Loss: 2.261215, 12 norm of gradients: 0.481708, 12 norm of weights: 2.024611\n",
            "Iteration Number: 1623, Loss: 2.260884, 12 norm of gradients: 0.480813, 12 norm of weights: 2.024448\n",
            "Iteration Number: 1624, Loss: 2.260554, 12 norm of gradients: 0.479920, 12 norm of weights: 2.024285\n",
            "Iteration Number: 1625, Loss: 2.260225, 12 norm of gradients: 0.479029, 12 norm of weights: 2.024122\n",
            "Iteration Number: 1626, Loss: 2.259896, 12 norm of gradients: 0.478140, 12 norm of weights: 2.023960\n",
            "Iteration Number: 1627, Loss: 2.259569, 12 norm of gradients: 0.477254, 12 norm of weights: 2.023798\n",
            "Iteration Number: 1628, Loss: 2.259243, 12 norm of gradients: 0.476369, 12 norm of weights: 2.023637\n",
            "Iteration Number: 1629, Loss: 2.258918, 12 norm of gradients: 0.475486, 12 norm of weights: 2.023476\n",
            "Iteration Number: 1630, Loss: 2.258593, 12 norm of gradients: 0.474605, 12 norm of weights: 2.023316\n",
            "Iteration Number: 1631, Loss: 2.258270, 12 norm of gradients: 0.473726, 12 norm of weights: 2.023156\n",
            "Iteration Number: 1632, Loss: 2.257948, 12 norm of gradients: 0.472849, 12 norm of weights: 2.022997\n",
            "Iteration Number: 1633, Loss: 2.257626, 12 norm of gradients: 0.471974, 12 norm of weights: 2.022838\n",
            "Iteration Number: 1634, Loss: 2.257305, 12 norm of gradients: 0.471100, 12 norm of weights: 2.022679\n",
            "Iteration Number: 1635, Loss: 2.256986, 12 norm of gradients: 0.470229, 12 norm of weights: 2.022521\n",
            "Iteration Number: 1636, Loss: 2.256667, 12 norm of gradients: 0.469360, 12 norm of weights: 2.022363\n",
            "Iteration Number: 1637, Loss: 2.256349, 12 norm of gradients: 0.468493, 12 norm of weights: 2.022206\n",
            "Iteration Number: 1638, Loss: 2.256032, 12 norm of gradients: 0.467627, 12 norm of weights: 2.022050\n",
            "Iteration Number: 1639, Loss: 2.255716, 12 norm of gradients: 0.466764, 12 norm of weights: 2.021893\n",
            "Iteration Number: 1640, Loss: 2.255401, 12 norm of gradients: 0.465902, 12 norm of weights: 2.021737\n",
            "Iteration Number: 1641, Loss: 2.255087, 12 norm of gradients: 0.465042, 12 norm of weights: 2.021582\n",
            "Iteration Number: 1642, Loss: 2.254774, 12 norm of gradients: 0.464185, 12 norm of weights: 2.021427\n",
            "Iteration Number: 1643, Loss: 2.254461, 12 norm of gradients: 0.463329, 12 norm of weights: 2.021272\n",
            "Iteration Number: 1644, Loss: 2.254150, 12 norm of gradients: 0.462475, 12 norm of weights: 2.021118\n",
            "Iteration Number: 1645, Loss: 2.253839, 12 norm of gradients: 0.461623, 12 norm of weights: 2.020965\n",
            "Iteration Number: 1646, Loss: 2.253530, 12 norm of gradients: 0.460772, 12 norm of weights: 2.020811\n",
            "Iteration Number: 1647, Loss: 2.253221, 12 norm of gradients: 0.459924, 12 norm of weights: 2.020659\n",
            "Iteration Number: 1648, Loss: 2.252913, 12 norm of gradients: 0.459078, 12 norm of weights: 2.020506\n",
            "Iteration Number: 1649, Loss: 2.252606, 12 norm of gradients: 0.458233, 12 norm of weights: 2.020354\n",
            "Iteration Number: 1650, Loss: 2.252300, 12 norm of gradients: 0.457390, 12 norm of weights: 2.020203\n",
            "Iteration Number: 1651, Loss: 2.251994, 12 norm of gradients: 0.456549, 12 norm of weights: 2.020052\n",
            "Iteration Number: 1652, Loss: 2.251690, 12 norm of gradients: 0.455710, 12 norm of weights: 2.019901\n",
            "Iteration Number: 1653, Loss: 2.251387, 12 norm of gradients: 0.454873, 12 norm of weights: 2.019751\n",
            "Iteration Number: 1654, Loss: 2.251084, 12 norm of gradients: 0.454038, 12 norm of weights: 2.019601\n",
            "Iteration Number: 1655, Loss: 2.250782, 12 norm of gradients: 0.453205, 12 norm of weights: 2.019451\n",
            "Iteration Number: 1656, Loss: 2.250481, 12 norm of gradients: 0.452373, 12 norm of weights: 2.019302\n",
            "Iteration Number: 1657, Loss: 2.250181, 12 norm of gradients: 0.451543, 12 norm of weights: 2.019154\n",
            "Iteration Number: 1658, Loss: 2.249882, 12 norm of gradients: 0.450715, 12 norm of weights: 2.019006\n",
            "Iteration Number: 1659, Loss: 2.249584, 12 norm of gradients: 0.449889, 12 norm of weights: 2.018858\n",
            "Iteration Number: 1660, Loss: 2.249286, 12 norm of gradients: 0.449065, 12 norm of weights: 2.018710\n",
            "Iteration Number: 1661, Loss: 2.248990, 12 norm of gradients: 0.448242, 12 norm of weights: 2.018563\n",
            "Iteration Number: 1662, Loss: 2.248694, 12 norm of gradients: 0.447422, 12 norm of weights: 2.018417\n",
            "Iteration Number: 1663, Loss: 2.248399, 12 norm of gradients: 0.446603, 12 norm of weights: 2.018271\n",
            "Iteration Number: 1664, Loss: 2.248105, 12 norm of gradients: 0.445786, 12 norm of weights: 2.018125\n",
            "Iteration Number: 1665, Loss: 2.247811, 12 norm of gradients: 0.444970, 12 norm of weights: 2.017980\n",
            "Iteration Number: 1666, Loss: 2.247519, 12 norm of gradients: 0.444157, 12 norm of weights: 2.017835\n",
            "Iteration Number: 1667, Loss: 2.247227, 12 norm of gradients: 0.443345, 12 norm of weights: 2.017690\n",
            "Iteration Number: 1668, Loss: 2.246937, 12 norm of gradients: 0.442535, 12 norm of weights: 2.017546\n",
            "Iteration Number: 1669, Loss: 2.246647, 12 norm of gradients: 0.441727, 12 norm of weights: 2.017403\n",
            "Iteration Number: 1670, Loss: 2.246358, 12 norm of gradients: 0.440921, 12 norm of weights: 2.017259\n",
            "Iteration Number: 1671, Loss: 2.246069, 12 norm of gradients: 0.440116, 12 norm of weights: 2.017116\n",
            "Iteration Number: 1672, Loss: 2.245782, 12 norm of gradients: 0.439314, 12 norm of weights: 2.016974\n",
            "Iteration Number: 1673, Loss: 2.245495, 12 norm of gradients: 0.438513, 12 norm of weights: 2.016832\n",
            "Iteration Number: 1674, Loss: 2.245209, 12 norm of gradients: 0.437713, 12 norm of weights: 2.016690\n",
            "Iteration Number: 1675, Loss: 2.244924, 12 norm of gradients: 0.436916, 12 norm of weights: 2.016549\n",
            "Iteration Number: 1676, Loss: 2.244640, 12 norm of gradients: 0.436120, 12 norm of weights: 2.016408\n",
            "Iteration Number: 1677, Loss: 2.244357, 12 norm of gradients: 0.435326, 12 norm of weights: 2.016267\n",
            "Iteration Number: 1678, Loss: 2.244074, 12 norm of gradients: 0.434534, 12 norm of weights: 2.016127\n",
            "Iteration Number: 1679, Loss: 2.243792, 12 norm of gradients: 0.433743, 12 norm of weights: 2.015987\n",
            "Iteration Number: 1680, Loss: 2.243511, 12 norm of gradients: 0.432954, 12 norm of weights: 2.015848\n",
            "Iteration Number: 1681, Loss: 2.243231, 12 norm of gradients: 0.432167, 12 norm of weights: 2.015709\n",
            "Iteration Number: 1682, Loss: 2.242952, 12 norm of gradients: 0.431382, 12 norm of weights: 2.015571\n",
            "Iteration Number: 1683, Loss: 2.242673, 12 norm of gradients: 0.430598, 12 norm of weights: 2.015432\n",
            "Iteration Number: 1684, Loss: 2.242395, 12 norm of gradients: 0.429816, 12 norm of weights: 2.015294\n",
            "Iteration Number: 1685, Loss: 2.242118, 12 norm of gradients: 0.429036, 12 norm of weights: 2.015157\n",
            "Iteration Number: 1686, Loss: 2.241842, 12 norm of gradients: 0.428258, 12 norm of weights: 2.015020\n",
            "Iteration Number: 1687, Loss: 2.241566, 12 norm of gradients: 0.427481, 12 norm of weights: 2.014883\n",
            "Iteration Number: 1688, Loss: 2.241292, 12 norm of gradients: 0.426706, 12 norm of weights: 2.014747\n",
            "Iteration Number: 1689, Loss: 2.241018, 12 norm of gradients: 0.425932, 12 norm of weights: 2.014611\n",
            "Iteration Number: 1690, Loss: 2.240744, 12 norm of gradients: 0.425161, 12 norm of weights: 2.014475\n",
            "Iteration Number: 1691, Loss: 2.240472, 12 norm of gradients: 0.424390, 12 norm of weights: 2.014340\n",
            "Iteration Number: 1692, Loss: 2.240200, 12 norm of gradients: 0.423622, 12 norm of weights: 2.014205\n",
            "Iteration Number: 1693, Loss: 2.239930, 12 norm of gradients: 0.422855, 12 norm of weights: 2.014071\n",
            "Iteration Number: 1694, Loss: 2.239659, 12 norm of gradients: 0.422090, 12 norm of weights: 2.013937\n",
            "Iteration Number: 1695, Loss: 2.239390, 12 norm of gradients: 0.421327, 12 norm of weights: 2.013803\n",
            "Iteration Number: 1696, Loss: 2.239121, 12 norm of gradients: 0.420565, 12 norm of weights: 2.013670\n",
            "Iteration Number: 1697, Loss: 2.238854, 12 norm of gradients: 0.419805, 12 norm of weights: 2.013537\n",
            "Iteration Number: 1698, Loss: 2.238586, 12 norm of gradients: 0.419047, 12 norm of weights: 2.013404\n",
            "Iteration Number: 1699, Loss: 2.238320, 12 norm of gradients: 0.418290, 12 norm of weights: 2.013272\n",
            "Iteration Number: 1700, Loss: 2.238055, 12 norm of gradients: 0.417535, 12 norm of weights: 2.013140\n",
            "Iteration Number: 1701, Loss: 2.237790, 12 norm of gradients: 0.416782, 12 norm of weights: 2.013009\n",
            "Iteration Number: 1702, Loss: 2.237526, 12 norm of gradients: 0.416030, 12 norm of weights: 2.012878\n",
            "Iteration Number: 1703, Loss: 2.237262, 12 norm of gradients: 0.415280, 12 norm of weights: 2.012747\n",
            "Iteration Number: 1704, Loss: 2.237000, 12 norm of gradients: 0.414531, 12 norm of weights: 2.012616\n",
            "Iteration Number: 1705, Loss: 2.236738, 12 norm of gradients: 0.413784, 12 norm of weights: 2.012486\n",
            "Iteration Number: 1706, Loss: 2.236476, 12 norm of gradients: 0.413039, 12 norm of weights: 2.012357\n",
            "Iteration Number: 1707, Loss: 2.236216, 12 norm of gradients: 0.412295, 12 norm of weights: 2.012227\n",
            "Iteration Number: 1708, Loss: 2.235956, 12 norm of gradients: 0.411553, 12 norm of weights: 2.012098\n",
            "Iteration Number: 1709, Loss: 2.235697, 12 norm of gradients: 0.410813, 12 norm of weights: 2.011970\n",
            "Iteration Number: 1710, Loss: 2.235439, 12 norm of gradients: 0.410074, 12 norm of weights: 2.011841\n",
            "Iteration Number: 1711, Loss: 2.235182, 12 norm of gradients: 0.409337, 12 norm of weights: 2.011713\n",
            "Iteration Number: 1712, Loss: 2.234925, 12 norm of gradients: 0.408601, 12 norm of weights: 2.011586\n",
            "Iteration Number: 1713, Loss: 2.234669, 12 norm of gradients: 0.407867, 12 norm of weights: 2.011458\n",
            "Iteration Number: 1714, Loss: 2.234413, 12 norm of gradients: 0.407135, 12 norm of weights: 2.011332\n",
            "Iteration Number: 1715, Loss: 2.234158, 12 norm of gradients: 0.406404, 12 norm of weights: 2.011205\n",
            "Iteration Number: 1716, Loss: 2.233904, 12 norm of gradients: 0.405675, 12 norm of weights: 2.011079\n",
            "Iteration Number: 1717, Loss: 2.233651, 12 norm of gradients: 0.404947, 12 norm of weights: 2.010953\n",
            "Iteration Number: 1718, Loss: 2.233399, 12 norm of gradients: 0.404221, 12 norm of weights: 2.010827\n",
            "Iteration Number: 1719, Loss: 2.233147, 12 norm of gradients: 0.403496, 12 norm of weights: 2.010702\n",
            "Iteration Number: 1720, Loss: 2.232896, 12 norm of gradients: 0.402773, 12 norm of weights: 2.010577\n",
            "Iteration Number: 1721, Loss: 2.232645, 12 norm of gradients: 0.402052, 12 norm of weights: 2.010453\n",
            "Iteration Number: 1722, Loss: 2.232395, 12 norm of gradients: 0.401332, 12 norm of weights: 2.010329\n",
            "Iteration Number: 1723, Loss: 2.232146, 12 norm of gradients: 0.400614, 12 norm of weights: 2.010205\n",
            "Iteration Number: 1724, Loss: 2.231898, 12 norm of gradients: 0.399897, 12 norm of weights: 2.010081\n",
            "Iteration Number: 1725, Loss: 2.231650, 12 norm of gradients: 0.399182, 12 norm of weights: 2.009958\n",
            "Iteration Number: 1726, Loss: 2.231403, 12 norm of gradients: 0.398468, 12 norm of weights: 2.009835\n",
            "Iteration Number: 1727, Loss: 2.231157, 12 norm of gradients: 0.397756, 12 norm of weights: 2.009713\n",
            "Iteration Number: 1728, Loss: 2.230911, 12 norm of gradients: 0.397046, 12 norm of weights: 2.009591\n",
            "Iteration Number: 1729, Loss: 2.230666, 12 norm of gradients: 0.396336, 12 norm of weights: 2.009469\n",
            "Iteration Number: 1730, Loss: 2.230422, 12 norm of gradients: 0.395629, 12 norm of weights: 2.009347\n",
            "Iteration Number: 1731, Loss: 2.230178, 12 norm of gradients: 0.394923, 12 norm of weights: 2.009226\n",
            "Iteration Number: 1732, Loss: 2.229935, 12 norm of gradients: 0.394219, 12 norm of weights: 2.009105\n",
            "Iteration Number: 1733, Loss: 2.229693, 12 norm of gradients: 0.393516, 12 norm of weights: 2.008985\n",
            "Iteration Number: 1734, Loss: 2.229451, 12 norm of gradients: 0.392814, 12 norm of weights: 2.008865\n",
            "Iteration Number: 1735, Loss: 2.229210, 12 norm of gradients: 0.392114, 12 norm of weights: 2.008745\n",
            "Iteration Number: 1736, Loss: 2.228970, 12 norm of gradients: 0.391416, 12 norm of weights: 2.008625\n",
            "Iteration Number: 1737, Loss: 2.228730, 12 norm of gradients: 0.390719, 12 norm of weights: 2.008506\n",
            "Iteration Number: 1738, Loss: 2.228491, 12 norm of gradients: 0.390023, 12 norm of weights: 2.008387\n",
            "Iteration Number: 1739, Loss: 2.228253, 12 norm of gradients: 0.389329, 12 norm of weights: 2.008269\n",
            "Iteration Number: 1740, Loss: 2.228015, 12 norm of gradients: 0.388637, 12 norm of weights: 2.008150\n",
            "Iteration Number: 1741, Loss: 2.227778, 12 norm of gradients: 0.387946, 12 norm of weights: 2.008033\n",
            "Iteration Number: 1742, Loss: 2.227542, 12 norm of gradients: 0.387257, 12 norm of weights: 2.007915\n",
            "Iteration Number: 1743, Loss: 2.227306, 12 norm of gradients: 0.386569, 12 norm of weights: 2.007798\n",
            "Iteration Number: 1744, Loss: 2.227071, 12 norm of gradients: 0.385882, 12 norm of weights: 2.007681\n",
            "Iteration Number: 1745, Loss: 2.226837, 12 norm of gradients: 0.385197, 12 norm of weights: 2.007564\n",
            "Iteration Number: 1746, Loss: 2.226603, 12 norm of gradients: 0.384514, 12 norm of weights: 2.007448\n",
            "Iteration Number: 1747, Loss: 2.226370, 12 norm of gradients: 0.383831, 12 norm of weights: 2.007332\n",
            "Iteration Number: 1748, Loss: 2.226137, 12 norm of gradients: 0.383151, 12 norm of weights: 2.007216\n",
            "Iteration Number: 1749, Loss: 2.225905, 12 norm of gradients: 0.382472, 12 norm of weights: 2.007101\n",
            "Iteration Number: 1750, Loss: 2.225674, 12 norm of gradients: 0.381794, 12 norm of weights: 2.006985\n",
            "Iteration Number: 1751, Loss: 2.225444, 12 norm of gradients: 0.381118, 12 norm of weights: 2.006871\n",
            "Iteration Number: 1752, Loss: 2.225214, 12 norm of gradients: 0.380443, 12 norm of weights: 2.006756\n",
            "Iteration Number: 1753, Loss: 2.224984, 12 norm of gradients: 0.379769, 12 norm of weights: 2.006642\n",
            "Iteration Number: 1754, Loss: 2.224756, 12 norm of gradients: 0.379097, 12 norm of weights: 2.006528\n",
            "Iteration Number: 1755, Loss: 2.224527, 12 norm of gradients: 0.378427, 12 norm of weights: 2.006415\n",
            "Iteration Number: 1756, Loss: 2.224300, 12 norm of gradients: 0.377758, 12 norm of weights: 2.006301\n",
            "Iteration Number: 1757, Loss: 2.224073, 12 norm of gradients: 0.377090, 12 norm of weights: 2.006188\n",
            "Iteration Number: 1758, Loss: 2.223847, 12 norm of gradients: 0.376424, 12 norm of weights: 2.006076\n",
            "Iteration Number: 1759, Loss: 2.223621, 12 norm of gradients: 0.375759, 12 norm of weights: 2.005963\n",
            "Iteration Number: 1760, Loss: 2.223396, 12 norm of gradients: 0.375096, 12 norm of weights: 2.005851\n",
            "Iteration Number: 1761, Loss: 2.223172, 12 norm of gradients: 0.374434, 12 norm of weights: 2.005740\n",
            "Iteration Number: 1762, Loss: 2.222948, 12 norm of gradients: 0.373773, 12 norm of weights: 2.005628\n",
            "Iteration Number: 1763, Loss: 2.222725, 12 norm of gradients: 0.373114, 12 norm of weights: 2.005517\n",
            "Iteration Number: 1764, Loss: 2.222502, 12 norm of gradients: 0.372456, 12 norm of weights: 2.005406\n",
            "Iteration Number: 1765, Loss: 2.222280, 12 norm of gradients: 0.371800, 12 norm of weights: 2.005296\n",
            "Iteration Number: 1766, Loss: 2.222059, 12 norm of gradients: 0.371145, 12 norm of weights: 2.005185\n",
            "Iteration Number: 1767, Loss: 2.221838, 12 norm of gradients: 0.370492, 12 norm of weights: 2.005075\n",
            "Iteration Number: 1768, Loss: 2.221618, 12 norm of gradients: 0.369839, 12 norm of weights: 2.004966\n",
            "Iteration Number: 1769, Loss: 2.221398, 12 norm of gradients: 0.369189, 12 norm of weights: 2.004856\n",
            "Iteration Number: 1770, Loss: 2.221179, 12 norm of gradients: 0.368539, 12 norm of weights: 2.004747\n",
            "Iteration Number: 1771, Loss: 2.220961, 12 norm of gradients: 0.367891, 12 norm of weights: 2.004638\n",
            "Iteration Number: 1772, Loss: 2.220743, 12 norm of gradients: 0.367245, 12 norm of weights: 2.004530\n",
            "Iteration Number: 1773, Loss: 2.220526, 12 norm of gradients: 0.366599, 12 norm of weights: 2.004422\n",
            "Iteration Number: 1774, Loss: 2.220309, 12 norm of gradients: 0.365956, 12 norm of weights: 2.004314\n",
            "Iteration Number: 1775, Loss: 2.220093, 12 norm of gradients: 0.365313, 12 norm of weights: 2.004206\n",
            "Iteration Number: 1776, Loss: 2.219878, 12 norm of gradients: 0.364672, 12 norm of weights: 2.004099\n",
            "Iteration Number: 1777, Loss: 2.219663, 12 norm of gradients: 0.364032, 12 norm of weights: 2.003992\n",
            "Iteration Number: 1778, Loss: 2.219448, 12 norm of gradients: 0.363394, 12 norm of weights: 2.003885\n",
            "Iteration Number: 1779, Loss: 2.219235, 12 norm of gradients: 0.362757, 12 norm of weights: 2.003778\n",
            "Iteration Number: 1780, Loss: 2.219022, 12 norm of gradients: 0.362121, 12 norm of weights: 2.003672\n",
            "Iteration Number: 1781, Loss: 2.218809, 12 norm of gradients: 0.361486, 12 norm of weights: 2.003566\n",
            "Iteration Number: 1782, Loss: 2.218597, 12 norm of gradients: 0.360853, 12 norm of weights: 2.003460\n",
            "Iteration Number: 1783, Loss: 2.218385, 12 norm of gradients: 0.360222, 12 norm of weights: 2.003355\n",
            "Iteration Number: 1784, Loss: 2.218175, 12 norm of gradients: 0.359591, 12 norm of weights: 2.003250\n",
            "Iteration Number: 1785, Loss: 2.217964, 12 norm of gradients: 0.358962, 12 norm of weights: 2.003145\n",
            "Iteration Number: 1786, Loss: 2.217754, 12 norm of gradients: 0.358335, 12 norm of weights: 2.003040\n",
            "Iteration Number: 1787, Loss: 2.217545, 12 norm of gradients: 0.357708, 12 norm of weights: 2.002936\n",
            "Iteration Number: 1788, Loss: 2.217337, 12 norm of gradients: 0.357083, 12 norm of weights: 2.002832\n",
            "Iteration Number: 1789, Loss: 2.217129, 12 norm of gradients: 0.356460, 12 norm of weights: 2.002728\n",
            "Iteration Number: 1790, Loss: 2.216921, 12 norm of gradients: 0.355837, 12 norm of weights: 2.002625\n",
            "Iteration Number: 1791, Loss: 2.216714, 12 norm of gradients: 0.355216, 12 norm of weights: 2.002522\n",
            "Iteration Number: 1792, Loss: 2.216508, 12 norm of gradients: 0.354596, 12 norm of weights: 2.002419\n",
            "Iteration Number: 1793, Loss: 2.216302, 12 norm of gradients: 0.353978, 12 norm of weights: 2.002316\n",
            "Iteration Number: 1794, Loss: 2.216096, 12 norm of gradients: 0.353361, 12 norm of weights: 2.002214\n",
            "Iteration Number: 1795, Loss: 2.215892, 12 norm of gradients: 0.352745, 12 norm of weights: 2.002112\n",
            "Iteration Number: 1796, Loss: 2.215687, 12 norm of gradients: 0.352130, 12 norm of weights: 2.002010\n",
            "Iteration Number: 1797, Loss: 2.215484, 12 norm of gradients: 0.351517, 12 norm of weights: 2.001908\n",
            "Iteration Number: 1798, Loss: 2.215281, 12 norm of gradients: 0.350905, 12 norm of weights: 2.001807\n",
            "Iteration Number: 1799, Loss: 2.215078, 12 norm of gradients: 0.350294, 12 norm of weights: 2.001706\n",
            "Iteration Number: 1800, Loss: 2.214876, 12 norm of gradients: 0.349685, 12 norm of weights: 2.001605\n",
            "Iteration Number: 1801, Loss: 2.214674, 12 norm of gradients: 0.349077, 12 norm of weights: 2.001505\n",
            "Iteration Number: 1802, Loss: 2.214473, 12 norm of gradients: 0.348470, 12 norm of weights: 2.001404\n",
            "Iteration Number: 1803, Loss: 2.214273, 12 norm of gradients: 0.347864, 12 norm of weights: 2.001304\n",
            "Iteration Number: 1804, Loss: 2.214073, 12 norm of gradients: 0.347260, 12 norm of weights: 2.001205\n",
            "Iteration Number: 1805, Loss: 2.213873, 12 norm of gradients: 0.346657, 12 norm of weights: 2.001105\n",
            "Iteration Number: 1806, Loss: 2.213675, 12 norm of gradients: 0.346055, 12 norm of weights: 2.001006\n",
            "Iteration Number: 1807, Loss: 2.213476, 12 norm of gradients: 0.345455, 12 norm of weights: 2.000907\n",
            "Iteration Number: 1808, Loss: 2.213278, 12 norm of gradients: 0.344856, 12 norm of weights: 2.000808\n",
            "Iteration Number: 1809, Loss: 2.213081, 12 norm of gradients: 0.344258, 12 norm of weights: 2.000710\n",
            "Iteration Number: 1810, Loss: 2.212884, 12 norm of gradients: 0.343661, 12 norm of weights: 2.000612\n",
            "Iteration Number: 1811, Loss: 2.212688, 12 norm of gradients: 0.343065, 12 norm of weights: 2.000514\n",
            "Iteration Number: 1812, Loss: 2.212492, 12 norm of gradients: 0.342471, 12 norm of weights: 2.000416\n",
            "Iteration Number: 1813, Loss: 2.212297, 12 norm of gradients: 0.341878, 12 norm of weights: 2.000319\n",
            "Iteration Number: 1814, Loss: 2.212102, 12 norm of gradients: 0.341287, 12 norm of weights: 2.000222\n",
            "Iteration Number: 1815, Loss: 2.211908, 12 norm of gradients: 0.340696, 12 norm of weights: 2.000125\n",
            "Iteration Number: 1816, Loss: 2.211714, 12 norm of gradients: 0.340107, 12 norm of weights: 2.000028\n",
            "Iteration Number: 1817, Loss: 2.211521, 12 norm of gradients: 0.339519, 12 norm of weights: 1.999932\n",
            "Iteration Number: 1818, Loss: 2.211329, 12 norm of gradients: 0.338932, 12 norm of weights: 1.999835\n",
            "Iteration Number: 1819, Loss: 2.211136, 12 norm of gradients: 0.338346, 12 norm of weights: 1.999740\n",
            "Iteration Number: 1820, Loss: 2.210945, 12 norm of gradients: 0.337762, 12 norm of weights: 1.999644\n",
            "Iteration Number: 1821, Loss: 2.210754, 12 norm of gradients: 0.337179, 12 norm of weights: 1.999549\n",
            "Iteration Number: 1822, Loss: 2.210563, 12 norm of gradients: 0.336597, 12 norm of weights: 1.999453\n",
            "Iteration Number: 1823, Loss: 2.210373, 12 norm of gradients: 0.336016, 12 norm of weights: 1.999358\n",
            "Iteration Number: 1824, Loss: 2.210183, 12 norm of gradients: 0.335437, 12 norm of weights: 1.999264\n",
            "Iteration Number: 1825, Loss: 2.209994, 12 norm of gradients: 0.334859, 12 norm of weights: 1.999169\n",
            "Iteration Number: 1826, Loss: 2.209805, 12 norm of gradients: 0.334282, 12 norm of weights: 1.999075\n",
            "Iteration Number: 1827, Loss: 2.209617, 12 norm of gradients: 0.333706, 12 norm of weights: 1.998981\n",
            "Iteration Number: 1828, Loss: 2.209429, 12 norm of gradients: 0.333131, 12 norm of weights: 1.998888\n",
            "Iteration Number: 1829, Loss: 2.209242, 12 norm of gradients: 0.332558, 12 norm of weights: 1.998794\n",
            "Iteration Number: 1830, Loss: 2.209055, 12 norm of gradients: 0.331985, 12 norm of weights: 1.998701\n",
            "Iteration Number: 1831, Loss: 2.208869, 12 norm of gradients: 0.331414, 12 norm of weights: 1.998608\n",
            "Iteration Number: 1832, Loss: 2.208683, 12 norm of gradients: 0.330845, 12 norm of weights: 1.998515\n",
            "Iteration Number: 1833, Loss: 2.208498, 12 norm of gradients: 0.330276, 12 norm of weights: 1.998423\n",
            "Iteration Number: 1834, Loss: 2.208313, 12 norm of gradients: 0.329708, 12 norm of weights: 1.998330\n",
            "Iteration Number: 1835, Loss: 2.208129, 12 norm of gradients: 0.329142, 12 norm of weights: 1.998238\n",
            "Iteration Number: 1836, Loss: 2.207945, 12 norm of gradients: 0.328577, 12 norm of weights: 1.998147\n",
            "Iteration Number: 1837, Loss: 2.207762, 12 norm of gradients: 0.328013, 12 norm of weights: 1.998055\n",
            "Iteration Number: 1838, Loss: 2.207579, 12 norm of gradients: 0.327450, 12 norm of weights: 1.997964\n",
            "Iteration Number: 1839, Loss: 2.207397, 12 norm of gradients: 0.326889, 12 norm of weights: 1.997873\n",
            "Iteration Number: 1840, Loss: 2.207215, 12 norm of gradients: 0.326328, 12 norm of weights: 1.997782\n",
            "Iteration Number: 1841, Loss: 2.207033, 12 norm of gradients: 0.325769, 12 norm of weights: 1.997691\n",
            "Iteration Number: 1842, Loss: 2.206852, 12 norm of gradients: 0.325211, 12 norm of weights: 1.997601\n",
            "Iteration Number: 1843, Loss: 2.206672, 12 norm of gradients: 0.324654, 12 norm of weights: 1.997511\n",
            "Iteration Number: 1844, Loss: 2.206492, 12 norm of gradients: 0.324098, 12 norm of weights: 1.997421\n",
            "Iteration Number: 1845, Loss: 2.206312, 12 norm of gradients: 0.323543, 12 norm of weights: 1.997331\n",
            "Iteration Number: 1846, Loss: 2.206133, 12 norm of gradients: 0.322990, 12 norm of weights: 1.997242\n",
            "Iteration Number: 1847, Loss: 2.205954, 12 norm of gradients: 0.322437, 12 norm of weights: 1.997153\n",
            "Iteration Number: 1848, Loss: 2.205776, 12 norm of gradients: 0.321886, 12 norm of weights: 1.997064\n",
            "Iteration Number: 1849, Loss: 2.205598, 12 norm of gradients: 0.321336, 12 norm of weights: 1.996975\n",
            "Iteration Number: 1850, Loss: 2.205421, 12 norm of gradients: 0.320787, 12 norm of weights: 1.996886\n",
            "Iteration Number: 1851, Loss: 2.205244, 12 norm of gradients: 0.320239, 12 norm of weights: 1.996798\n",
            "Iteration Number: 1852, Loss: 2.205068, 12 norm of gradients: 0.319692, 12 norm of weights: 1.996710\n",
            "Iteration Number: 1853, Loss: 2.204892, 12 norm of gradients: 0.319147, 12 norm of weights: 1.996622\n",
            "Iteration Number: 1854, Loss: 2.204716, 12 norm of gradients: 0.318603, 12 norm of weights: 1.996534\n",
            "Iteration Number: 1855, Loss: 2.204541, 12 norm of gradients: 0.318059, 12 norm of weights: 1.996447\n",
            "Iteration Number: 1856, Loss: 2.204367, 12 norm of gradients: 0.317517, 12 norm of weights: 1.996360\n",
            "Iteration Number: 1857, Loss: 2.204193, 12 norm of gradients: 0.316976, 12 norm of weights: 1.996273\n",
            "Iteration Number: 1858, Loss: 2.204019, 12 norm of gradients: 0.316436, 12 norm of weights: 1.996186\n",
            "Iteration Number: 1859, Loss: 2.203846, 12 norm of gradients: 0.315897, 12 norm of weights: 1.996099\n",
            "Iteration Number: 1860, Loss: 2.203673, 12 norm of gradients: 0.315359, 12 norm of weights: 1.996013\n",
            "Iteration Number: 1861, Loss: 2.203501, 12 norm of gradients: 0.314823, 12 norm of weights: 1.995927\n",
            "Iteration Number: 1862, Loss: 2.203329, 12 norm of gradients: 0.314287, 12 norm of weights: 1.995841\n",
            "Iteration Number: 1863, Loss: 2.203157, 12 norm of gradients: 0.313753, 12 norm of weights: 1.995755\n",
            "Iteration Number: 1864, Loss: 2.202986, 12 norm of gradients: 0.313220, 12 norm of weights: 1.995670\n",
            "Iteration Number: 1865, Loss: 2.202816, 12 norm of gradients: 0.312687, 12 norm of weights: 1.995585\n",
            "Iteration Number: 1866, Loss: 2.202646, 12 norm of gradients: 0.312156, 12 norm of weights: 1.995500\n",
            "Iteration Number: 1867, Loss: 2.202476, 12 norm of gradients: 0.311626, 12 norm of weights: 1.995415\n",
            "Iteration Number: 1868, Loss: 2.202307, 12 norm of gradients: 0.311097, 12 norm of weights: 1.995330\n",
            "Iteration Number: 1869, Loss: 2.202138, 12 norm of gradients: 0.310570, 12 norm of weights: 1.995246\n",
            "Iteration Number: 1870, Loss: 2.201969, 12 norm of gradients: 0.310043, 12 norm of weights: 1.995162\n",
            "Iteration Number: 1871, Loss: 2.201801, 12 norm of gradients: 0.309517, 12 norm of weights: 1.995078\n",
            "Iteration Number: 1872, Loss: 2.201634, 12 norm of gradients: 0.308993, 12 norm of weights: 1.994994\n",
            "Iteration Number: 1873, Loss: 2.201467, 12 norm of gradients: 0.308469, 12 norm of weights: 1.994911\n",
            "Iteration Number: 1874, Loss: 2.201300, 12 norm of gradients: 0.307947, 12 norm of weights: 1.994827\n",
            "Iteration Number: 1875, Loss: 2.201134, 12 norm of gradients: 0.307425, 12 norm of weights: 1.994744\n",
            "Iteration Number: 1876, Loss: 2.200968, 12 norm of gradients: 0.306905, 12 norm of weights: 1.994661\n",
            "Iteration Number: 1877, Loss: 2.200802, 12 norm of gradients: 0.306386, 12 norm of weights: 1.994578\n",
            "Iteration Number: 1878, Loss: 2.200637, 12 norm of gradients: 0.305868, 12 norm of weights: 1.994496\n",
            "Iteration Number: 1879, Loss: 2.200473, 12 norm of gradients: 0.305350, 12 norm of weights: 1.994414\n",
            "Iteration Number: 1880, Loss: 2.200308, 12 norm of gradients: 0.304834, 12 norm of weights: 1.994332\n",
            "Iteration Number: 1881, Loss: 2.200144, 12 norm of gradients: 0.304319, 12 norm of weights: 1.994250\n",
            "Iteration Number: 1882, Loss: 2.199981, 12 norm of gradients: 0.303806, 12 norm of weights: 1.994168\n",
            "Iteration Number: 1883, Loss: 2.199818, 12 norm of gradients: 0.303293, 12 norm of weights: 1.994087\n",
            "Iteration Number: 1884, Loss: 2.199656, 12 norm of gradients: 0.302781, 12 norm of weights: 1.994005\n",
            "Iteration Number: 1885, Loss: 2.199493, 12 norm of gradients: 0.302270, 12 norm of weights: 1.993924\n",
            "Iteration Number: 1886, Loss: 2.199332, 12 norm of gradients: 0.301760, 12 norm of weights: 1.993843\n",
            "Iteration Number: 1887, Loss: 2.199170, 12 norm of gradients: 0.301252, 12 norm of weights: 1.993763\n",
            "Iteration Number: 1888, Loss: 2.199009, 12 norm of gradients: 0.300744, 12 norm of weights: 1.993682\n",
            "Iteration Number: 1889, Loss: 2.198849, 12 norm of gradients: 0.300237, 12 norm of weights: 1.993602\n",
            "Iteration Number: 1890, Loss: 2.198689, 12 norm of gradients: 0.299732, 12 norm of weights: 1.993522\n",
            "Iteration Number: 1891, Loss: 2.198529, 12 norm of gradients: 0.299227, 12 norm of weights: 1.993442\n",
            "Iteration Number: 1892, Loss: 2.198370, 12 norm of gradients: 0.298724, 12 norm of weights: 1.993363\n",
            "Iteration Number: 1893, Loss: 2.198211, 12 norm of gradients: 0.298221, 12 norm of weights: 1.993283\n",
            "Iteration Number: 1894, Loss: 2.198052, 12 norm of gradients: 0.297720, 12 norm of weights: 1.993204\n",
            "Iteration Number: 1895, Loss: 2.197894, 12 norm of gradients: 0.297219, 12 norm of weights: 1.993125\n",
            "Iteration Number: 1896, Loss: 2.197736, 12 norm of gradients: 0.296720, 12 norm of weights: 1.993046\n",
            "Iteration Number: 1897, Loss: 2.197579, 12 norm of gradients: 0.296221, 12 norm of weights: 1.992967\n",
            "Iteration Number: 1898, Loss: 2.197422, 12 norm of gradients: 0.295724, 12 norm of weights: 1.992889\n",
            "Iteration Number: 1899, Loss: 2.197265, 12 norm of gradients: 0.295227, 12 norm of weights: 1.992811\n",
            "Iteration Number: 1900, Loss: 2.197109, 12 norm of gradients: 0.294732, 12 norm of weights: 1.992732\n",
            "Iteration Number: 1901, Loss: 2.196954, 12 norm of gradients: 0.294238, 12 norm of weights: 1.992655\n",
            "Iteration Number: 1902, Loss: 2.196798, 12 norm of gradients: 0.293744, 12 norm of weights: 1.992577\n",
            "Iteration Number: 1903, Loss: 2.196643, 12 norm of gradients: 0.293252, 12 norm of weights: 1.992499\n",
            "Iteration Number: 1904, Loss: 2.196488, 12 norm of gradients: 0.292761, 12 norm of weights: 1.992422\n",
            "Iteration Number: 1905, Loss: 2.196334, 12 norm of gradients: 0.292270, 12 norm of weights: 1.992345\n",
            "Iteration Number: 1906, Loss: 2.196180, 12 norm of gradients: 0.291781, 12 norm of weights: 1.992268\n",
            "Iteration Number: 1907, Loss: 2.196027, 12 norm of gradients: 0.291292, 12 norm of weights: 1.992191\n",
            "Iteration Number: 1908, Loss: 2.195874, 12 norm of gradients: 0.290805, 12 norm of weights: 1.992115\n",
            "Iteration Number: 1909, Loss: 2.195721, 12 norm of gradients: 0.290319, 12 norm of weights: 1.992038\n",
            "Iteration Number: 1910, Loss: 2.195569, 12 norm of gradients: 0.289833, 12 norm of weights: 1.991962\n",
            "Iteration Number: 1911, Loss: 2.195417, 12 norm of gradients: 0.289349, 12 norm of weights: 1.991886\n",
            "Iteration Number: 1912, Loss: 2.195265, 12 norm of gradients: 0.288865, 12 norm of weights: 1.991810\n",
            "Iteration Number: 1913, Loss: 2.195114, 12 norm of gradients: 0.288383, 12 norm of weights: 1.991735\n",
            "Iteration Number: 1914, Loss: 2.194963, 12 norm of gradients: 0.287901, 12 norm of weights: 1.991659\n",
            "Iteration Number: 1915, Loss: 2.194813, 12 norm of gradients: 0.287421, 12 norm of weights: 1.991584\n",
            "Iteration Number: 1916, Loss: 2.194663, 12 norm of gradients: 0.286941, 12 norm of weights: 1.991509\n",
            "Iteration Number: 1917, Loss: 2.194513, 12 norm of gradients: 0.286463, 12 norm of weights: 1.991434\n",
            "Iteration Number: 1918, Loss: 2.194363, 12 norm of gradients: 0.285985, 12 norm of weights: 1.991359\n",
            "Iteration Number: 1919, Loss: 2.194214, 12 norm of gradients: 0.285508, 12 norm of weights: 1.991285\n",
            "Iteration Number: 1920, Loss: 2.194066, 12 norm of gradients: 0.285033, 12 norm of weights: 1.991210\n",
            "Iteration Number: 1921, Loss: 2.193918, 12 norm of gradients: 0.284558, 12 norm of weights: 1.991136\n",
            "Iteration Number: 1922, Loss: 2.193770, 12 norm of gradients: 0.284084, 12 norm of weights: 1.991062\n",
            "Iteration Number: 1923, Loss: 2.193622, 12 norm of gradients: 0.283611, 12 norm of weights: 1.990989\n",
            "Iteration Number: 1924, Loss: 2.193475, 12 norm of gradients: 0.283140, 12 norm of weights: 1.990915\n",
            "Iteration Number: 1925, Loss: 2.193328, 12 norm of gradients: 0.282669, 12 norm of weights: 1.990841\n",
            "Iteration Number: 1926, Loss: 2.193182, 12 norm of gradients: 0.282199, 12 norm of weights: 1.990768\n",
            "Iteration Number: 1927, Loss: 2.193036, 12 norm of gradients: 0.281730, 12 norm of weights: 1.990695\n",
            "Iteration Number: 1928, Loss: 2.192890, 12 norm of gradients: 0.281262, 12 norm of weights: 1.990622\n",
            "Iteration Number: 1929, Loss: 2.192744, 12 norm of gradients: 0.280794, 12 norm of weights: 1.990549\n",
            "Iteration Number: 1930, Loss: 2.192599, 12 norm of gradients: 0.280328, 12 norm of weights: 1.990477\n",
            "Iteration Number: 1931, Loss: 2.192455, 12 norm of gradients: 0.279863, 12 norm of weights: 1.990405\n",
            "Iteration Number: 1932, Loss: 2.192310, 12 norm of gradients: 0.279399, 12 norm of weights: 1.990332\n",
            "Iteration Number: 1933, Loss: 2.192166, 12 norm of gradients: 0.278935, 12 norm of weights: 1.990260\n",
            "Iteration Number: 1934, Loss: 2.192023, 12 norm of gradients: 0.278473, 12 norm of weights: 1.990188\n",
            "Iteration Number: 1935, Loss: 2.191879, 12 norm of gradients: 0.278011, 12 norm of weights: 1.990117\n",
            "Iteration Number: 1936, Loss: 2.191737, 12 norm of gradients: 0.277551, 12 norm of weights: 1.990045\n",
            "Iteration Number: 1937, Loss: 2.191594, 12 norm of gradients: 0.277091, 12 norm of weights: 1.989974\n",
            "Iteration Number: 1938, Loss: 2.191452, 12 norm of gradients: 0.276632, 12 norm of weights: 1.989903\n",
            "Iteration Number: 1939, Loss: 2.191310, 12 norm of gradients: 0.276175, 12 norm of weights: 1.989832\n",
            "Iteration Number: 1940, Loss: 2.191168, 12 norm of gradients: 0.275718, 12 norm of weights: 1.989761\n",
            "Iteration Number: 1941, Loss: 2.191027, 12 norm of gradients: 0.275262, 12 norm of weights: 1.989690\n",
            "Iteration Number: 1942, Loss: 2.190886, 12 norm of gradients: 0.274807, 12 norm of weights: 1.989620\n",
            "Iteration Number: 1943, Loss: 2.190746, 12 norm of gradients: 0.274352, 12 norm of weights: 1.989550\n",
            "Iteration Number: 1944, Loss: 2.190605, 12 norm of gradients: 0.273899, 12 norm of weights: 1.989479\n",
            "Iteration Number: 1945, Loss: 2.190466, 12 norm of gradients: 0.273447, 12 norm of weights: 1.989409\n",
            "Iteration Number: 1946, Loss: 2.190326, 12 norm of gradients: 0.272995, 12 norm of weights: 1.989340\n",
            "Iteration Number: 1947, Loss: 2.190187, 12 norm of gradients: 0.272545, 12 norm of weights: 1.989270\n",
            "Iteration Number: 1948, Loss: 2.190048, 12 norm of gradients: 0.272095, 12 norm of weights: 1.989201\n",
            "Iteration Number: 1949, Loss: 2.189910, 12 norm of gradients: 0.271647, 12 norm of weights: 1.989131\n",
            "Iteration Number: 1950, Loss: 2.189771, 12 norm of gradients: 0.271199, 12 norm of weights: 1.989062\n",
            "Iteration Number: 1951, Loss: 2.189634, 12 norm of gradients: 0.270752, 12 norm of weights: 1.988993\n",
            "Iteration Number: 1952, Loss: 2.189496, 12 norm of gradients: 0.270306, 12 norm of weights: 1.988924\n",
            "Iteration Number: 1953, Loss: 2.189359, 12 norm of gradients: 0.269861, 12 norm of weights: 1.988856\n",
            "Iteration Number: 1954, Loss: 2.189222, 12 norm of gradients: 0.269416, 12 norm of weights: 1.988787\n",
            "Iteration Number: 1955, Loss: 2.189085, 12 norm of gradients: 0.268973, 12 norm of weights: 1.988719\n",
            "Iteration Number: 1956, Loss: 2.188949, 12 norm of gradients: 0.268530, 12 norm of weights: 1.988651\n",
            "Iteration Number: 1957, Loss: 2.188813, 12 norm of gradients: 0.268089, 12 norm of weights: 1.988583\n",
            "Iteration Number: 1958, Loss: 2.188678, 12 norm of gradients: 0.267648, 12 norm of weights: 1.988515\n",
            "Iteration Number: 1959, Loss: 2.188543, 12 norm of gradients: 0.267208, 12 norm of weights: 1.988447\n",
            "Iteration Number: 1960, Loss: 2.188408, 12 norm of gradients: 0.266769, 12 norm of weights: 1.988380\n",
            "Iteration Number: 1961, Loss: 2.188273, 12 norm of gradients: 0.266331, 12 norm of weights: 1.988312\n",
            "Iteration Number: 1962, Loss: 2.188139, 12 norm of gradients: 0.265894, 12 norm of weights: 1.988245\n",
            "Iteration Number: 1963, Loss: 2.188005, 12 norm of gradients: 0.265457, 12 norm of weights: 1.988178\n",
            "Iteration Number: 1964, Loss: 2.187871, 12 norm of gradients: 0.265022, 12 norm of weights: 1.988111\n",
            "Iteration Number: 1965, Loss: 2.187738, 12 norm of gradients: 0.264587, 12 norm of weights: 1.988045\n",
            "Iteration Number: 1966, Loss: 2.187605, 12 norm of gradients: 0.264154, 12 norm of weights: 1.987978\n",
            "Iteration Number: 1967, Loss: 2.187472, 12 norm of gradients: 0.263721, 12 norm of weights: 1.987912\n",
            "Iteration Number: 1968, Loss: 2.187340, 12 norm of gradients: 0.263289, 12 norm of weights: 1.987845\n",
            "Iteration Number: 1969, Loss: 2.187208, 12 norm of gradients: 0.262857, 12 norm of weights: 1.987779\n",
            "Iteration Number: 1970, Loss: 2.187076, 12 norm of gradients: 0.262427, 12 norm of weights: 1.987713\n",
            "Iteration Number: 1971, Loss: 2.186945, 12 norm of gradients: 0.261998, 12 norm of weights: 1.987648\n",
            "Iteration Number: 1972, Loss: 2.186814, 12 norm of gradients: 0.261569, 12 norm of weights: 1.987582\n",
            "Iteration Number: 1973, Loss: 2.186683, 12 norm of gradients: 0.261141, 12 norm of weights: 1.987517\n",
            "Iteration Number: 1974, Loss: 2.186552, 12 norm of gradients: 0.260714, 12 norm of weights: 1.987451\n",
            "Iteration Number: 1975, Loss: 2.186422, 12 norm of gradients: 0.260288, 12 norm of weights: 1.987386\n",
            "Iteration Number: 1976, Loss: 2.186292, 12 norm of gradients: 0.259863, 12 norm of weights: 1.987321\n",
            "Iteration Number: 1977, Loss: 2.186163, 12 norm of gradients: 0.259439, 12 norm of weights: 1.987256\n",
            "Iteration Number: 1978, Loss: 2.186033, 12 norm of gradients: 0.259015, 12 norm of weights: 1.987192\n",
            "Iteration Number: 1979, Loss: 2.185904, 12 norm of gradients: 0.258592, 12 norm of weights: 1.987127\n",
            "Iteration Number: 1980, Loss: 2.185776, 12 norm of gradients: 0.258170, 12 norm of weights: 1.987063\n",
            "Iteration Number: 1981, Loss: 2.185647, 12 norm of gradients: 0.257749, 12 norm of weights: 1.986998\n",
            "Iteration Number: 1982, Loss: 2.185519, 12 norm of gradients: 0.257329, 12 norm of weights: 1.986934\n",
            "Iteration Number: 1983, Loss: 2.185391, 12 norm of gradients: 0.256910, 12 norm of weights: 1.986870\n",
            "Iteration Number: 1984, Loss: 2.185264, 12 norm of gradients: 0.256491, 12 norm of weights: 1.986807\n",
            "Iteration Number: 1985, Loss: 2.185137, 12 norm of gradients: 0.256074, 12 norm of weights: 1.986743\n",
            "Iteration Number: 1986, Loss: 2.185010, 12 norm of gradients: 0.255657, 12 norm of weights: 1.986679\n",
            "Iteration Number: 1987, Loss: 2.184883, 12 norm of gradients: 0.255241, 12 norm of weights: 1.986616\n",
            "Iteration Number: 1988, Loss: 2.184757, 12 norm of gradients: 0.254825, 12 norm of weights: 1.986553\n",
            "Iteration Number: 1989, Loss: 2.184631, 12 norm of gradients: 0.254411, 12 norm of weights: 1.986490\n",
            "Iteration Number: 1990, Loss: 2.184505, 12 norm of gradients: 0.253997, 12 norm of weights: 1.986427\n",
            "Iteration Number: 1991, Loss: 2.184380, 12 norm of gradients: 0.253584, 12 norm of weights: 1.986364\n",
            "Iteration Number: 1992, Loss: 2.184255, 12 norm of gradients: 0.253172, 12 norm of weights: 1.986302\n",
            "Iteration Number: 1993, Loss: 2.184130, 12 norm of gradients: 0.252761, 12 norm of weights: 1.986239\n",
            "Iteration Number: 1994, Loss: 2.184006, 12 norm of gradients: 0.252351, 12 norm of weights: 1.986177\n",
            "Iteration Number: 1995, Loss: 2.183881, 12 norm of gradients: 0.251941, 12 norm of weights: 1.986115\n",
            "Iteration Number: 1996, Loss: 2.183757, 12 norm of gradients: 0.251533, 12 norm of weights: 1.986053\n",
            "Iteration Number: 1997, Loss: 2.183634, 12 norm of gradients: 0.251125, 12 norm of weights: 1.985991\n",
            "Iteration Number: 1998, Loss: 2.183510, 12 norm of gradients: 0.250718, 12 norm of weights: 1.985929\n",
            "Iteration Number: 1999, Loss: 2.183387, 12 norm of gradients: 0.250311, 12 norm of weights: 1.985867\n",
            "Iteration Number: 2000, Loss: 2.183264, 12 norm of gradients: 0.249906, 12 norm of weights: 1.985806\n",
            "Iteration Number: 2001, Loss: 2.183142, 12 norm of gradients: 0.249501, 12 norm of weights: 1.985744\n",
            "Iteration Number: 2002, Loss: 2.183020, 12 norm of gradients: 0.249097, 12 norm of weights: 1.985683\n",
            "Iteration Number: 2003, Loss: 2.182898, 12 norm of gradients: 0.248694, 12 norm of weights: 1.985622\n",
            "Iteration Number: 2004, Loss: 2.182776, 12 norm of gradients: 0.248291, 12 norm of weights: 1.985561\n",
            "Iteration Number: 2005, Loss: 2.182655, 12 norm of gradients: 0.247890, 12 norm of weights: 1.985501\n",
            "Iteration Number: 2006, Loss: 2.182533, 12 norm of gradients: 0.247489, 12 norm of weights: 1.985440\n",
            "Iteration Number: 2007, Loss: 2.182413, 12 norm of gradients: 0.247089, 12 norm of weights: 1.985379\n",
            "Iteration Number: 2008, Loss: 2.182292, 12 norm of gradients: 0.246690, 12 norm of weights: 1.985319\n",
            "Iteration Number: 2009, Loss: 2.182172, 12 norm of gradients: 0.246291, 12 norm of weights: 1.985259\n",
            "Iteration Number: 2010, Loss: 2.182052, 12 norm of gradients: 0.245894, 12 norm of weights: 1.985199\n",
            "Iteration Number: 2011, Loss: 2.181932, 12 norm of gradients: 0.245497, 12 norm of weights: 1.985139\n",
            "Iteration Number: 2012, Loss: 2.181812, 12 norm of gradients: 0.245101, 12 norm of weights: 1.985079\n",
            "Iteration Number: 2013, Loss: 2.181693, 12 norm of gradients: 0.244705, 12 norm of weights: 1.985019\n",
            "Iteration Number: 2014, Loss: 2.181574, 12 norm of gradients: 0.244311, 12 norm of weights: 1.984960\n",
            "Iteration Number: 2015, Loss: 2.181456, 12 norm of gradients: 0.243917, 12 norm of weights: 1.984901\n",
            "Iteration Number: 2016, Loss: 2.181337, 12 norm of gradients: 0.243524, 12 norm of weights: 1.984841\n",
            "Iteration Number: 2017, Loss: 2.181219, 12 norm of gradients: 0.243132, 12 norm of weights: 1.984782\n",
            "Iteration Number: 2018, Loss: 2.181101, 12 norm of gradients: 0.242740, 12 norm of weights: 1.984723\n",
            "Iteration Number: 2019, Loss: 2.180984, 12 norm of gradients: 0.242350, 12 norm of weights: 1.984664\n",
            "Iteration Number: 2020, Loss: 2.180866, 12 norm of gradients: 0.241960, 12 norm of weights: 1.984606\n",
            "Iteration Number: 2021, Loss: 2.180749, 12 norm of gradients: 0.241570, 12 norm of weights: 1.984547\n",
            "Iteration Number: 2022, Loss: 2.180633, 12 norm of gradients: 0.241182, 12 norm of weights: 1.984489\n",
            "Iteration Number: 2023, Loss: 2.180516, 12 norm of gradients: 0.240794, 12 norm of weights: 1.984430\n",
            "Iteration Number: 2024, Loss: 2.180400, 12 norm of gradients: 0.240407, 12 norm of weights: 1.984372\n",
            "Iteration Number: 2025, Loss: 2.180284, 12 norm of gradients: 0.240021, 12 norm of weights: 1.984314\n",
            "Iteration Number: 2026, Loss: 2.180168, 12 norm of gradients: 0.239636, 12 norm of weights: 1.984256\n",
            "Iteration Number: 2027, Loss: 2.180053, 12 norm of gradients: 0.239251, 12 norm of weights: 1.984198\n",
            "Iteration Number: 2028, Loss: 2.179938, 12 norm of gradients: 0.238867, 12 norm of weights: 1.984141\n",
            "Iteration Number: 2029, Loss: 2.179823, 12 norm of gradients: 0.238484, 12 norm of weights: 1.984083\n",
            "Iteration Number: 2030, Loss: 2.179708, 12 norm of gradients: 0.238102, 12 norm of weights: 1.984026\n",
            "Iteration Number: 2031, Loss: 2.179593, 12 norm of gradients: 0.237720, 12 norm of weights: 1.983968\n",
            "Iteration Number: 2032, Loss: 2.179479, 12 norm of gradients: 0.237339, 12 norm of weights: 1.983911\n",
            "Iteration Number: 2033, Loss: 2.179365, 12 norm of gradients: 0.236959, 12 norm of weights: 1.983854\n",
            "Iteration Number: 2034, Loss: 2.179252, 12 norm of gradients: 0.236579, 12 norm of weights: 1.983797\n",
            "Iteration Number: 2035, Loss: 2.179138, 12 norm of gradients: 0.236201, 12 norm of weights: 1.983741\n",
            "Iteration Number: 2036, Loss: 2.179025, 12 norm of gradients: 0.235823, 12 norm of weights: 1.983684\n",
            "Iteration Number: 2037, Loss: 2.178912, 12 norm of gradients: 0.235445, 12 norm of weights: 1.983627\n",
            "Iteration Number: 2038, Loss: 2.178800, 12 norm of gradients: 0.235069, 12 norm of weights: 1.983571\n",
            "Iteration Number: 2039, Loss: 2.178687, 12 norm of gradients: 0.234693, 12 norm of weights: 1.983515\n",
            "Iteration Number: 2040, Loss: 2.178575, 12 norm of gradients: 0.234318, 12 norm of weights: 1.983459\n",
            "Iteration Number: 2041, Loss: 2.178463, 12 norm of gradients: 0.233944, 12 norm of weights: 1.983403\n",
            "Iteration Number: 2042, Loss: 2.178352, 12 norm of gradients: 0.233570, 12 norm of weights: 1.983347\n",
            "Iteration Number: 2043, Loss: 2.178240, 12 norm of gradients: 0.233197, 12 norm of weights: 1.983291\n",
            "Iteration Number: 2044, Loss: 2.178129, 12 norm of gradients: 0.232825, 12 norm of weights: 1.983235\n",
            "Iteration Number: 2045, Loss: 2.178018, 12 norm of gradients: 0.232454, 12 norm of weights: 1.983180\n",
            "Iteration Number: 2046, Loss: 2.177908, 12 norm of gradients: 0.232083, 12 norm of weights: 1.983125\n",
            "Iteration Number: 2047, Loss: 2.177797, 12 norm of gradients: 0.231713, 12 norm of weights: 1.983069\n",
            "Iteration Number: 2048, Loss: 2.177687, 12 norm of gradients: 0.231344, 12 norm of weights: 1.983014\n",
            "Iteration Number: 2049, Loss: 2.177577, 12 norm of gradients: 0.230975, 12 norm of weights: 1.982959\n",
            "Iteration Number: 2050, Loss: 2.177467, 12 norm of gradients: 0.230607, 12 norm of weights: 1.982904\n",
            "Iteration Number: 2051, Loss: 2.177358, 12 norm of gradients: 0.230240, 12 norm of weights: 1.982849\n",
            "Iteration Number: 2052, Loss: 2.177249, 12 norm of gradients: 0.229873, 12 norm of weights: 1.982795\n",
            "Iteration Number: 2053, Loss: 2.177140, 12 norm of gradients: 0.229508, 12 norm of weights: 1.982740\n",
            "Iteration Number: 2054, Loss: 2.177031, 12 norm of gradients: 0.229143, 12 norm of weights: 1.982686\n",
            "Iteration Number: 2055, Loss: 2.176923, 12 norm of gradients: 0.228778, 12 norm of weights: 1.982632\n",
            "Iteration Number: 2056, Loss: 2.176814, 12 norm of gradients: 0.228415, 12 norm of weights: 1.982577\n",
            "Iteration Number: 2057, Loss: 2.176706, 12 norm of gradients: 0.228052, 12 norm of weights: 1.982523\n",
            "Iteration Number: 2058, Loss: 2.176599, 12 norm of gradients: 0.227690, 12 norm of weights: 1.982469\n",
            "Iteration Number: 2059, Loss: 2.176491, 12 norm of gradients: 0.227328, 12 norm of weights: 1.982416\n",
            "Iteration Number: 2060, Loss: 2.176384, 12 norm of gradients: 0.226967, 12 norm of weights: 1.982362\n",
            "Iteration Number: 2061, Loss: 2.176277, 12 norm of gradients: 0.226607, 12 norm of weights: 1.982308\n",
            "Iteration Number: 2062, Loss: 2.176170, 12 norm of gradients: 0.226248, 12 norm of weights: 1.982255\n",
            "Iteration Number: 2063, Loss: 2.176064, 12 norm of gradients: 0.225889, 12 norm of weights: 1.982202\n",
            "Iteration Number: 2064, Loss: 2.175957, 12 norm of gradients: 0.225531, 12 norm of weights: 1.982148\n",
            "Iteration Number: 2065, Loss: 2.175851, 12 norm of gradients: 0.225174, 12 norm of weights: 1.982095\n",
            "Iteration Number: 2066, Loss: 2.175745, 12 norm of gradients: 0.224817, 12 norm of weights: 1.982042\n",
            "Iteration Number: 2067, Loss: 2.175640, 12 norm of gradients: 0.224461, 12 norm of weights: 1.981989\n",
            "Iteration Number: 2068, Loss: 2.175534, 12 norm of gradients: 0.224106, 12 norm of weights: 1.981937\n",
            "Iteration Number: 2069, Loss: 2.175429, 12 norm of gradients: 0.223751, 12 norm of weights: 1.981884\n",
            "Iteration Number: 2070, Loss: 2.175324, 12 norm of gradients: 0.223397, 12 norm of weights: 1.981831\n",
            "Iteration Number: 2071, Loss: 2.175219, 12 norm of gradients: 0.223044, 12 norm of weights: 1.981779\n",
            "Iteration Number: 2072, Loss: 2.175115, 12 norm of gradients: 0.222691, 12 norm of weights: 1.981727\n",
            "Iteration Number: 2073, Loss: 2.175010, 12 norm of gradients: 0.222339, 12 norm of weights: 1.981675\n",
            "Iteration Number: 2074, Loss: 2.174906, 12 norm of gradients: 0.221988, 12 norm of weights: 1.981622\n",
            "Iteration Number: 2075, Loss: 2.174803, 12 norm of gradients: 0.221637, 12 norm of weights: 1.981570\n",
            "Iteration Number: 2076, Loss: 2.174699, 12 norm of gradients: 0.221287, 12 norm of weights: 1.981519\n",
            "Iteration Number: 2077, Loss: 2.174596, 12 norm of gradients: 0.220938, 12 norm of weights: 1.981467\n",
            "Iteration Number: 2078, Loss: 2.174492, 12 norm of gradients: 0.220590, 12 norm of weights: 1.981415\n",
            "Iteration Number: 2079, Loss: 2.174390, 12 norm of gradients: 0.220242, 12 norm of weights: 1.981364\n",
            "Iteration Number: 2080, Loss: 2.174287, 12 norm of gradients: 0.219895, 12 norm of weights: 1.981312\n",
            "Iteration Number: 2081, Loss: 2.174184, 12 norm of gradients: 0.219548, 12 norm of weights: 1.981261\n",
            "Iteration Number: 2082, Loss: 2.174082, 12 norm of gradients: 0.219202, 12 norm of weights: 1.981210\n",
            "Iteration Number: 2083, Loss: 2.173980, 12 norm of gradients: 0.218857, 12 norm of weights: 1.981159\n",
            "Iteration Number: 2084, Loss: 2.173878, 12 norm of gradients: 0.218512, 12 norm of weights: 1.981108\n",
            "Iteration Number: 2085, Loss: 2.173777, 12 norm of gradients: 0.218168, 12 norm of weights: 1.981057\n",
            "Iteration Number: 2086, Loss: 2.173675, 12 norm of gradients: 0.217825, 12 norm of weights: 1.981006\n",
            "Iteration Number: 2087, Loss: 2.173574, 12 norm of gradients: 0.217483, 12 norm of weights: 1.980956\n",
            "Iteration Number: 2088, Loss: 2.173473, 12 norm of gradients: 0.217141, 12 norm of weights: 1.980905\n",
            "Iteration Number: 2089, Loss: 2.173372, 12 norm of gradients: 0.216799, 12 norm of weights: 1.980855\n",
            "Iteration Number: 2090, Loss: 2.173272, 12 norm of gradients: 0.216459, 12 norm of weights: 1.980804\n",
            "Iteration Number: 2091, Loss: 2.173171, 12 norm of gradients: 0.216119, 12 norm of weights: 1.980754\n",
            "Iteration Number: 2092, Loss: 2.173071, 12 norm of gradients: 0.215779, 12 norm of weights: 1.980704\n",
            "Iteration Number: 2093, Loss: 2.172972, 12 norm of gradients: 0.215441, 12 norm of weights: 1.980654\n",
            "Iteration Number: 2094, Loss: 2.172872, 12 norm of gradients: 0.215103, 12 norm of weights: 1.980604\n",
            "Iteration Number: 2095, Loss: 2.172772, 12 norm of gradients: 0.214765, 12 norm of weights: 1.980554\n",
            "Iteration Number: 2096, Loss: 2.172673, 12 norm of gradients: 0.214428, 12 norm of weights: 1.980505\n",
            "Iteration Number: 2097, Loss: 2.172574, 12 norm of gradients: 0.214092, 12 norm of weights: 1.980455\n",
            "Iteration Number: 2098, Loss: 2.172475, 12 norm of gradients: 0.213757, 12 norm of weights: 1.980406\n",
            "Iteration Number: 2099, Loss: 2.172377, 12 norm of gradients: 0.213422, 12 norm of weights: 1.980356\n",
            "Iteration Number: 2100, Loss: 2.172278, 12 norm of gradients: 0.213088, 12 norm of weights: 1.980307\n",
            "Iteration Number: 2101, Loss: 2.172180, 12 norm of gradients: 0.212754, 12 norm of weights: 1.980258\n",
            "Iteration Number: 2102, Loss: 2.172082, 12 norm of gradients: 0.212421, 12 norm of weights: 1.980209\n",
            "Iteration Number: 2103, Loss: 2.171984, 12 norm of gradients: 0.212089, 12 norm of weights: 1.980160\n",
            "Iteration Number: 2104, Loss: 2.171887, 12 norm of gradients: 0.211757, 12 norm of weights: 1.980111\n",
            "Iteration Number: 2105, Loss: 2.171789, 12 norm of gradients: 0.211426, 12 norm of weights: 1.980062\n",
            "Iteration Number: 2106, Loss: 2.171692, 12 norm of gradients: 0.211096, 12 norm of weights: 1.980014\n",
            "Iteration Number: 2107, Loss: 2.171595, 12 norm of gradients: 0.210766, 12 norm of weights: 1.979965\n",
            "Iteration Number: 2108, Loss: 2.171498, 12 norm of gradients: 0.210437, 12 norm of weights: 1.979917\n",
            "Iteration Number: 2109, Loss: 2.171402, 12 norm of gradients: 0.210108, 12 norm of weights: 1.979869\n",
            "Iteration Number: 2110, Loss: 2.171305, 12 norm of gradients: 0.209780, 12 norm of weights: 1.979820\n",
            "Iteration Number: 2111, Loss: 2.171209, 12 norm of gradients: 0.209453, 12 norm of weights: 1.979772\n",
            "Iteration Number: 2112, Loss: 2.171113, 12 norm of gradients: 0.209126, 12 norm of weights: 1.979724\n",
            "Iteration Number: 2113, Loss: 2.171018, 12 norm of gradients: 0.208800, 12 norm of weights: 1.979676\n",
            "Iteration Number: 2114, Loss: 2.170922, 12 norm of gradients: 0.208475, 12 norm of weights: 1.979629\n",
            "Iteration Number: 2115, Loss: 2.170827, 12 norm of gradients: 0.208150, 12 norm of weights: 1.979581\n",
            "Iteration Number: 2116, Loss: 2.170731, 12 norm of gradients: 0.207826, 12 norm of weights: 1.979533\n",
            "Iteration Number: 2117, Loss: 2.170637, 12 norm of gradients: 0.207502, 12 norm of weights: 1.979486\n",
            "Iteration Number: 2118, Loss: 2.170542, 12 norm of gradients: 0.207179, 12 norm of weights: 1.979438\n",
            "Iteration Number: 2119, Loss: 2.170447, 12 norm of gradients: 0.206857, 12 norm of weights: 1.979391\n",
            "Iteration Number: 2120, Loss: 2.170353, 12 norm of gradients: 0.206535, 12 norm of weights: 1.979344\n",
            "Iteration Number: 2121, Loss: 2.170259, 12 norm of gradients: 0.206214, 12 norm of weights: 1.979297\n",
            "Iteration Number: 2122, Loss: 2.170165, 12 norm of gradients: 0.205893, 12 norm of weights: 1.979250\n",
            "Iteration Number: 2123, Loss: 2.170071, 12 norm of gradients: 0.205573, 12 norm of weights: 1.979203\n",
            "Iteration Number: 2124, Loss: 2.169977, 12 norm of gradients: 0.205254, 12 norm of weights: 1.979156\n",
            "Iteration Number: 2125, Loss: 2.169884, 12 norm of gradients: 0.204935, 12 norm of weights: 1.979109\n",
            "Iteration Number: 2126, Loss: 2.169791, 12 norm of gradients: 0.204617, 12 norm of weights: 1.979063\n",
            "Iteration Number: 2127, Loss: 2.169698, 12 norm of gradients: 0.204300, 12 norm of weights: 1.979016\n",
            "Iteration Number: 2128, Loss: 2.169605, 12 norm of gradients: 0.203983, 12 norm of weights: 1.978970\n",
            "Iteration Number: 2129, Loss: 2.169512, 12 norm of gradients: 0.203666, 12 norm of weights: 1.978923\n",
            "Iteration Number: 2130, Loss: 2.169420, 12 norm of gradients: 0.203350, 12 norm of weights: 1.978877\n",
            "Iteration Number: 2131, Loss: 2.169327, 12 norm of gradients: 0.203035, 12 norm of weights: 1.978831\n",
            "Iteration Number: 2132, Loss: 2.169235, 12 norm of gradients: 0.202721, 12 norm of weights: 1.978785\n",
            "Iteration Number: 2133, Loss: 2.169144, 12 norm of gradients: 0.202407, 12 norm of weights: 1.978739\n",
            "Iteration Number: 2134, Loss: 2.169052, 12 norm of gradients: 0.202093, 12 norm of weights: 1.978693\n",
            "Iteration Number: 2135, Loss: 2.168960, 12 norm of gradients: 0.201781, 12 norm of weights: 1.978647\n",
            "Iteration Number: 2136, Loss: 2.168869, 12 norm of gradients: 0.201468, 12 norm of weights: 1.978601\n",
            "Iteration Number: 2137, Loss: 2.168778, 12 norm of gradients: 0.201157, 12 norm of weights: 1.978556\n",
            "Iteration Number: 2138, Loss: 2.168687, 12 norm of gradients: 0.200846, 12 norm of weights: 1.978510\n",
            "Iteration Number: 2139, Loss: 2.168596, 12 norm of gradients: 0.200535, 12 norm of weights: 1.978465\n",
            "Iteration Number: 2140, Loss: 2.168506, 12 norm of gradients: 0.200225, 12 norm of weights: 1.978420\n",
            "Iteration Number: 2141, Loss: 2.168415, 12 norm of gradients: 0.199916, 12 norm of weights: 1.978374\n",
            "Iteration Number: 2142, Loss: 2.168325, 12 norm of gradients: 0.199607, 12 norm of weights: 1.978329\n",
            "Iteration Number: 2143, Loss: 2.168235, 12 norm of gradients: 0.199299, 12 norm of weights: 1.978284\n",
            "Iteration Number: 2144, Loss: 2.168145, 12 norm of gradients: 0.198992, 12 norm of weights: 1.978239\n",
            "Iteration Number: 2145, Loss: 2.168056, 12 norm of gradients: 0.198685, 12 norm of weights: 1.978195\n",
            "Iteration Number: 2146, Loss: 2.167966, 12 norm of gradients: 0.198378, 12 norm of weights: 1.978150\n",
            "Iteration Number: 2147, Loss: 2.167877, 12 norm of gradients: 0.198073, 12 norm of weights: 1.978105\n",
            "Iteration Number: 2148, Loss: 2.167788, 12 norm of gradients: 0.197767, 12 norm of weights: 1.978061\n",
            "Iteration Number: 2149, Loss: 2.167699, 12 norm of gradients: 0.197463, 12 norm of weights: 1.978016\n",
            "Iteration Number: 2150, Loss: 2.167610, 12 norm of gradients: 0.197159, 12 norm of weights: 1.977972\n",
            "Iteration Number: 2151, Loss: 2.167521, 12 norm of gradients: 0.196855, 12 norm of weights: 1.977927\n",
            "Iteration Number: 2152, Loss: 2.167433, 12 norm of gradients: 0.196552, 12 norm of weights: 1.977883\n",
            "Iteration Number: 2153, Loss: 2.167345, 12 norm of gradients: 0.196250, 12 norm of weights: 1.977839\n",
            "Iteration Number: 2154, Loss: 2.167257, 12 norm of gradients: 0.195948, 12 norm of weights: 1.977795\n",
            "Iteration Number: 2155, Loss: 2.167169, 12 norm of gradients: 0.195647, 12 norm of weights: 1.977751\n",
            "Iteration Number: 2156, Loss: 2.167081, 12 norm of gradients: 0.195346, 12 norm of weights: 1.977707\n",
            "Iteration Number: 2157, Loss: 2.166994, 12 norm of gradients: 0.195046, 12 norm of weights: 1.977663\n",
            "Iteration Number: 2158, Loss: 2.166906, 12 norm of gradients: 0.194746, 12 norm of weights: 1.977620\n",
            "Iteration Number: 2159, Loss: 2.166819, 12 norm of gradients: 0.194447, 12 norm of weights: 1.977576\n",
            "Iteration Number: 2160, Loss: 2.166732, 12 norm of gradients: 0.194149, 12 norm of weights: 1.977533\n",
            "Iteration Number: 2161, Loss: 2.166645, 12 norm of gradients: 0.193851, 12 norm of weights: 1.977489\n",
            "Iteration Number: 2162, Loss: 2.166559, 12 norm of gradients: 0.193553, 12 norm of weights: 1.977446\n",
            "Iteration Number: 2163, Loss: 2.166472, 12 norm of gradients: 0.193257, 12 norm of weights: 1.977403\n",
            "Iteration Number: 2164, Loss: 2.166386, 12 norm of gradients: 0.192960, 12 norm of weights: 1.977360\n",
            "Iteration Number: 2165, Loss: 2.166300, 12 norm of gradients: 0.192665, 12 norm of weights: 1.977316\n",
            "Iteration Number: 2166, Loss: 2.166214, 12 norm of gradients: 0.192369, 12 norm of weights: 1.977273\n",
            "Iteration Number: 2167, Loss: 2.166128, 12 norm of gradients: 0.192075, 12 norm of weights: 1.977231\n",
            "Iteration Number: 2168, Loss: 2.166042, 12 norm of gradients: 0.191781, 12 norm of weights: 1.977188\n",
            "Iteration Number: 2169, Loss: 2.165957, 12 norm of gradients: 0.191487, 12 norm of weights: 1.977145\n",
            "Iteration Number: 2170, Loss: 2.165872, 12 norm of gradients: 0.191194, 12 norm of weights: 1.977102\n",
            "Iteration Number: 2171, Loss: 2.165787, 12 norm of gradients: 0.190902, 12 norm of weights: 1.977060\n",
            "Iteration Number: 2172, Loss: 2.165702, 12 norm of gradients: 0.190610, 12 norm of weights: 1.977017\n",
            "Iteration Number: 2173, Loss: 2.165617, 12 norm of gradients: 0.190319, 12 norm of weights: 1.976975\n",
            "Iteration Number: 2174, Loss: 2.165532, 12 norm of gradients: 0.190028, 12 norm of weights: 1.976933\n",
            "Iteration Number: 2175, Loss: 2.165448, 12 norm of gradients: 0.189738, 12 norm of weights: 1.976890\n",
            "Iteration Number: 2176, Loss: 2.165363, 12 norm of gradients: 0.189448, 12 norm of weights: 1.976848\n",
            "Iteration Number: 2177, Loss: 2.165279, 12 norm of gradients: 0.189159, 12 norm of weights: 1.976806\n",
            "Iteration Number: 2178, Loss: 2.165195, 12 norm of gradients: 0.188870, 12 norm of weights: 1.976764\n",
            "Iteration Number: 2179, Loss: 2.165112, 12 norm of gradients: 0.188582, 12 norm of weights: 1.976722\n",
            "Iteration Number: 2180, Loss: 2.165028, 12 norm of gradients: 0.188295, 12 norm of weights: 1.976681\n",
            "Iteration Number: 2181, Loss: 2.164944, 12 norm of gradients: 0.188008, 12 norm of weights: 1.976639\n",
            "Iteration Number: 2182, Loss: 2.164861, 12 norm of gradients: 0.187721, 12 norm of weights: 1.976597\n",
            "Iteration Number: 2183, Loss: 2.164778, 12 norm of gradients: 0.187435, 12 norm of weights: 1.976556\n",
            "Iteration Number: 2184, Loss: 2.164695, 12 norm of gradients: 0.187150, 12 norm of weights: 1.976514\n",
            "Iteration Number: 2185, Loss: 2.164612, 12 norm of gradients: 0.186865, 12 norm of weights: 1.976473\n",
            "Iteration Number: 2186, Loss: 2.164530, 12 norm of gradients: 0.186580, 12 norm of weights: 1.976431\n",
            "Iteration Number: 2187, Loss: 2.164447, 12 norm of gradients: 0.186297, 12 norm of weights: 1.976390\n",
            "Iteration Number: 2188, Loss: 2.164365, 12 norm of gradients: 0.186013, 12 norm of weights: 1.976349\n",
            "Iteration Number: 2189, Loss: 2.164283, 12 norm of gradients: 0.185730, 12 norm of weights: 1.976308\n",
            "Iteration Number: 2190, Loss: 2.164201, 12 norm of gradients: 0.185448, 12 norm of weights: 1.976267\n",
            "Iteration Number: 2191, Loss: 2.164119, 12 norm of gradients: 0.185166, 12 norm of weights: 1.976226\n",
            "Iteration Number: 2192, Loss: 2.164037, 12 norm of gradients: 0.184885, 12 norm of weights: 1.976185\n",
            "Iteration Number: 2193, Loss: 2.163955, 12 norm of gradients: 0.184604, 12 norm of weights: 1.976144\n",
            "Iteration Number: 2194, Loss: 2.163874, 12 norm of gradients: 0.184324, 12 norm of weights: 1.976104\n",
            "Iteration Number: 2195, Loss: 2.163793, 12 norm of gradients: 0.184045, 12 norm of weights: 1.976063\n",
            "Iteration Number: 2196, Loss: 2.163712, 12 norm of gradients: 0.183765, 12 norm of weights: 1.976023\n",
            "Iteration Number: 2197, Loss: 2.163631, 12 norm of gradients: 0.183487, 12 norm of weights: 1.975982\n",
            "Iteration Number: 2198, Loss: 2.163550, 12 norm of gradients: 0.183209, 12 norm of weights: 1.975942\n",
            "Iteration Number: 2199, Loss: 2.163469, 12 norm of gradients: 0.182931, 12 norm of weights: 1.975901\n",
            "Iteration Number: 2200, Loss: 2.163389, 12 norm of gradients: 0.182654, 12 norm of weights: 1.975861\n",
            "Iteration Number: 2201, Loss: 2.163309, 12 norm of gradients: 0.182377, 12 norm of weights: 1.975821\n",
            "Iteration Number: 2202, Loss: 2.163229, 12 norm of gradients: 0.182101, 12 norm of weights: 1.975781\n",
            "Iteration Number: 2203, Loss: 2.163149, 12 norm of gradients: 0.181826, 12 norm of weights: 1.975741\n",
            "Iteration Number: 2204, Loss: 2.163069, 12 norm of gradients: 0.181550, 12 norm of weights: 1.975701\n",
            "Iteration Number: 2205, Loss: 2.162989, 12 norm of gradients: 0.181276, 12 norm of weights: 1.975661\n",
            "Iteration Number: 2206, Loss: 2.162909, 12 norm of gradients: 0.181002, 12 norm of weights: 1.975622\n",
            "Iteration Number: 2207, Loss: 2.162830, 12 norm of gradients: 0.180728, 12 norm of weights: 1.975582\n",
            "Iteration Number: 2208, Loss: 2.162751, 12 norm of gradients: 0.180455, 12 norm of weights: 1.975542\n",
            "Iteration Number: 2209, Loss: 2.162672, 12 norm of gradients: 0.180183, 12 norm of weights: 1.975503\n",
            "Iteration Number: 2210, Loss: 2.162593, 12 norm of gradients: 0.179911, 12 norm of weights: 1.975463\n",
            "Iteration Number: 2211, Loss: 2.162514, 12 norm of gradients: 0.179639, 12 norm of weights: 1.975424\n",
            "Iteration Number: 2212, Loss: 2.162435, 12 norm of gradients: 0.179368, 12 norm of weights: 1.975385\n",
            "Iteration Number: 2213, Loss: 2.162357, 12 norm of gradients: 0.179097, 12 norm of weights: 1.975345\n",
            "Iteration Number: 2214, Loss: 2.162278, 12 norm of gradients: 0.178827, 12 norm of weights: 1.975306\n",
            "Iteration Number: 2215, Loss: 2.162200, 12 norm of gradients: 0.178558, 12 norm of weights: 1.975267\n",
            "Iteration Number: 2216, Loss: 2.162122, 12 norm of gradients: 0.178289, 12 norm of weights: 1.975228\n",
            "Iteration Number: 2217, Loss: 2.162044, 12 norm of gradients: 0.178020, 12 norm of weights: 1.975189\n",
            "Iteration Number: 2218, Loss: 2.161967, 12 norm of gradients: 0.177752, 12 norm of weights: 1.975150\n",
            "Iteration Number: 2219, Loss: 2.161889, 12 norm of gradients: 0.177485, 12 norm of weights: 1.975112\n",
            "Iteration Number: 2220, Loss: 2.161811, 12 norm of gradients: 0.177217, 12 norm of weights: 1.975073\n",
            "Iteration Number: 2221, Loss: 2.161734, 12 norm of gradients: 0.176951, 12 norm of weights: 1.975034\n",
            "Iteration Number: 2222, Loss: 2.161657, 12 norm of gradients: 0.176685, 12 norm of weights: 1.974996\n",
            "Iteration Number: 2223, Loss: 2.161580, 12 norm of gradients: 0.176419, 12 norm of weights: 1.974957\n",
            "Iteration Number: 2224, Loss: 2.161503, 12 norm of gradients: 0.176154, 12 norm of weights: 1.974919\n",
            "Iteration Number: 2225, Loss: 2.161426, 12 norm of gradients: 0.175889, 12 norm of weights: 1.974880\n",
            "Iteration Number: 2226, Loss: 2.161350, 12 norm of gradients: 0.175625, 12 norm of weights: 1.974842\n",
            "Iteration Number: 2227, Loss: 2.161273, 12 norm of gradients: 0.175361, 12 norm of weights: 1.974804\n",
            "Iteration Number: 2228, Loss: 2.161197, 12 norm of gradients: 0.175098, 12 norm of weights: 1.974766\n",
            "Iteration Number: 2229, Loss: 2.161121, 12 norm of gradients: 0.174835, 12 norm of weights: 1.974728\n",
            "Iteration Number: 2230, Loss: 2.161045, 12 norm of gradients: 0.174573, 12 norm of weights: 1.974690\n",
            "Iteration Number: 2231, Loss: 2.160969, 12 norm of gradients: 0.174311, 12 norm of weights: 1.974652\n",
            "Iteration Number: 2232, Loss: 2.160893, 12 norm of gradients: 0.174050, 12 norm of weights: 1.974614\n",
            "Iteration Number: 2233, Loss: 2.160817, 12 norm of gradients: 0.173789, 12 norm of weights: 1.974576\n",
            "Iteration Number: 2234, Loss: 2.160742, 12 norm of gradients: 0.173529, 12 norm of weights: 1.974539\n",
            "Iteration Number: 2235, Loss: 2.160666, 12 norm of gradients: 0.173269, 12 norm of weights: 1.974501\n",
            "Iteration Number: 2236, Loss: 2.160591, 12 norm of gradients: 0.173010, 12 norm of weights: 1.974463\n",
            "Iteration Number: 2237, Loss: 2.160516, 12 norm of gradients: 0.172751, 12 norm of weights: 1.974426\n",
            "Iteration Number: 2238, Loss: 2.160441, 12 norm of gradients: 0.172492, 12 norm of weights: 1.974388\n",
            "Iteration Number: 2239, Loss: 2.160366, 12 norm of gradients: 0.172234, 12 norm of weights: 1.974351\n",
            "Iteration Number: 2240, Loss: 2.160292, 12 norm of gradients: 0.171977, 12 norm of weights: 1.974314\n",
            "Iteration Number: 2241, Loss: 2.160217, 12 norm of gradients: 0.171720, 12 norm of weights: 1.974277\n",
            "Iteration Number: 2242, Loss: 2.160143, 12 norm of gradients: 0.171463, 12 norm of weights: 1.974239\n",
            "Iteration Number: 2243, Loss: 2.160069, 12 norm of gradients: 0.171207, 12 norm of weights: 1.974202\n",
            "Iteration Number: 2244, Loss: 2.159995, 12 norm of gradients: 0.170951, 12 norm of weights: 1.974165\n",
            "Iteration Number: 2245, Loss: 2.159921, 12 norm of gradients: 0.170696, 12 norm of weights: 1.974128\n",
            "Iteration Number: 2246, Loss: 2.159847, 12 norm of gradients: 0.170442, 12 norm of weights: 1.974091\n",
            "Iteration Number: 2247, Loss: 2.159773, 12 norm of gradients: 0.170187, 12 norm of weights: 1.974055\n",
            "Iteration Number: 2248, Loss: 2.159699, 12 norm of gradients: 0.169934, 12 norm of weights: 1.974018\n",
            "Iteration Number: 2249, Loss: 2.159626, 12 norm of gradients: 0.169680, 12 norm of weights: 1.973981\n",
            "Iteration Number: 2250, Loss: 2.159553, 12 norm of gradients: 0.169427, 12 norm of weights: 1.973945\n",
            "Iteration Number: 2251, Loss: 2.159479, 12 norm of gradients: 0.169175, 12 norm of weights: 1.973908\n",
            "Iteration Number: 2252, Loss: 2.159406, 12 norm of gradients: 0.168923, 12 norm of weights: 1.973872\n",
            "Iteration Number: 2253, Loss: 2.159333, 12 norm of gradients: 0.168672, 12 norm of weights: 1.973835\n",
            "Iteration Number: 2254, Loss: 2.159261, 12 norm of gradients: 0.168420, 12 norm of weights: 1.973799\n",
            "Iteration Number: 2255, Loss: 2.159188, 12 norm of gradients: 0.168170, 12 norm of weights: 1.973763\n",
            "Iteration Number: 2256, Loss: 2.159115, 12 norm of gradients: 0.167920, 12 norm of weights: 1.973726\n",
            "Iteration Number: 2257, Loss: 2.159043, 12 norm of gradients: 0.167670, 12 norm of weights: 1.973690\n",
            "Iteration Number: 2258, Loss: 2.158971, 12 norm of gradients: 0.167421, 12 norm of weights: 1.973654\n",
            "Iteration Number: 2259, Loss: 2.158899, 12 norm of gradients: 0.167172, 12 norm of weights: 1.973618\n",
            "Iteration Number: 2260, Loss: 2.158827, 12 norm of gradients: 0.166924, 12 norm of weights: 1.973582\n",
            "Iteration Number: 2261, Loss: 2.158755, 12 norm of gradients: 0.166676, 12 norm of weights: 1.973546\n",
            "Iteration Number: 2262, Loss: 2.158683, 12 norm of gradients: 0.166429, 12 norm of weights: 1.973510\n",
            "Iteration Number: 2263, Loss: 2.158611, 12 norm of gradients: 0.166182, 12 norm of weights: 1.973475\n",
            "Iteration Number: 2264, Loss: 2.158540, 12 norm of gradients: 0.165935, 12 norm of weights: 1.973439\n",
            "Iteration Number: 2265, Loss: 2.158469, 12 norm of gradients: 0.165689, 12 norm of weights: 1.973403\n",
            "Iteration Number: 2266, Loss: 2.158397, 12 norm of gradients: 0.165443, 12 norm of weights: 1.973368\n",
            "Iteration Number: 2267, Loss: 2.158326, 12 norm of gradients: 0.165198, 12 norm of weights: 1.973332\n",
            "Iteration Number: 2268, Loss: 2.158255, 12 norm of gradients: 0.164953, 12 norm of weights: 1.973297\n",
            "Iteration Number: 2269, Loss: 2.158184, 12 norm of gradients: 0.164709, 12 norm of weights: 1.973261\n",
            "Iteration Number: 2270, Loss: 2.158114, 12 norm of gradients: 0.164465, 12 norm of weights: 1.973226\n",
            "Iteration Number: 2271, Loss: 2.158043, 12 norm of gradients: 0.164222, 12 norm of weights: 1.973191\n",
            "Iteration Number: 2272, Loss: 2.157972, 12 norm of gradients: 0.163979, 12 norm of weights: 1.973156\n",
            "Iteration Number: 2273, Loss: 2.157902, 12 norm of gradients: 0.163736, 12 norm of weights: 1.973121\n",
            "Iteration Number: 2274, Loss: 2.157832, 12 norm of gradients: 0.163494, 12 norm of weights: 1.973085\n",
            "Iteration Number: 2275, Loss: 2.157762, 12 norm of gradients: 0.163253, 12 norm of weights: 1.973050\n",
            "Iteration Number: 2276, Loss: 2.157692, 12 norm of gradients: 0.163011, 12 norm of weights: 1.973016\n",
            "Iteration Number: 2277, Loss: 2.157622, 12 norm of gradients: 0.162771, 12 norm of weights: 1.972981\n",
            "Iteration Number: 2278, Loss: 2.157552, 12 norm of gradients: 0.162530, 12 norm of weights: 1.972946\n",
            "Iteration Number: 2279, Loss: 2.157482, 12 norm of gradients: 0.162290, 12 norm of weights: 1.972911\n",
            "Iteration Number: 2280, Loss: 2.157413, 12 norm of gradients: 0.162051, 12 norm of weights: 1.972876\n",
            "Iteration Number: 2281, Loss: 2.157343, 12 norm of gradients: 0.161812, 12 norm of weights: 1.972842\n",
            "Iteration Number: 2282, Loss: 2.157274, 12 norm of gradients: 0.161573, 12 norm of weights: 1.972807\n",
            "Iteration Number: 2283, Loss: 2.157205, 12 norm of gradients: 0.161335, 12 norm of weights: 1.972773\n",
            "Iteration Number: 2284, Loss: 2.157136, 12 norm of gradients: 0.161097, 12 norm of weights: 1.972738\n",
            "Iteration Number: 2285, Loss: 2.157067, 12 norm of gradients: 0.160860, 12 norm of weights: 1.972704\n",
            "Iteration Number: 2286, Loss: 2.156998, 12 norm of gradients: 0.160623, 12 norm of weights: 1.972670\n",
            "Iteration Number: 2287, Loss: 2.156930, 12 norm of gradients: 0.160386, 12 norm of weights: 1.972635\n",
            "Iteration Number: 2288, Loss: 2.156861, 12 norm of gradients: 0.160150, 12 norm of weights: 1.972601\n",
            "Iteration Number: 2289, Loss: 2.156793, 12 norm of gradients: 0.159915, 12 norm of weights: 1.972567\n",
            "Iteration Number: 2290, Loss: 2.156724, 12 norm of gradients: 0.159679, 12 norm of weights: 1.972533\n",
            "Iteration Number: 2291, Loss: 2.156656, 12 norm of gradients: 0.159445, 12 norm of weights: 1.972499\n",
            "Iteration Number: 2292, Loss: 2.156588, 12 norm of gradients: 0.159210, 12 norm of weights: 1.972465\n",
            "Iteration Number: 2293, Loss: 2.156520, 12 norm of gradients: 0.158976, 12 norm of weights: 1.972431\n",
            "Iteration Number: 2294, Loss: 2.156452, 12 norm of gradients: 0.158743, 12 norm of weights: 1.972397\n",
            "Iteration Number: 2295, Loss: 2.156384, 12 norm of gradients: 0.158510, 12 norm of weights: 1.972363\n",
            "Iteration Number: 2296, Loss: 2.156317, 12 norm of gradients: 0.158277, 12 norm of weights: 1.972330\n",
            "Iteration Number: 2297, Loss: 2.156249, 12 norm of gradients: 0.158045, 12 norm of weights: 1.972296\n",
            "Iteration Number: 2298, Loss: 2.156182, 12 norm of gradients: 0.157813, 12 norm of weights: 1.972262\n",
            "Iteration Number: 2299, Loss: 2.156115, 12 norm of gradients: 0.157581, 12 norm of weights: 1.972229\n",
            "Iteration Number: 2300, Loss: 2.156047, 12 norm of gradients: 0.157350, 12 norm of weights: 1.972195\n",
            "Iteration Number: 2301, Loss: 2.155980, 12 norm of gradients: 0.157120, 12 norm of weights: 1.972162\n",
            "Iteration Number: 2302, Loss: 2.155914, 12 norm of gradients: 0.156889, 12 norm of weights: 1.972128\n",
            "Iteration Number: 2303, Loss: 2.155847, 12 norm of gradients: 0.156660, 12 norm of weights: 1.972095\n",
            "Iteration Number: 2304, Loss: 2.155780, 12 norm of gradients: 0.156430, 12 norm of weights: 1.972062\n",
            "Iteration Number: 2305, Loss: 2.155713, 12 norm of gradients: 0.156201, 12 norm of weights: 1.972029\n",
            "Iteration Number: 2306, Loss: 2.155647, 12 norm of gradients: 0.155973, 12 norm of weights: 1.971995\n",
            "Iteration Number: 2307, Loss: 2.155581, 12 norm of gradients: 0.155745, 12 norm of weights: 1.971962\n",
            "Iteration Number: 2308, Loss: 2.155514, 12 norm of gradients: 0.155517, 12 norm of weights: 1.971929\n",
            "Iteration Number: 2309, Loss: 2.155448, 12 norm of gradients: 0.155289, 12 norm of weights: 1.971896\n",
            "Iteration Number: 2310, Loss: 2.155382, 12 norm of gradients: 0.155063, 12 norm of weights: 1.971863\n",
            "Iteration Number: 2311, Loss: 2.155316, 12 norm of gradients: 0.154836, 12 norm of weights: 1.971830\n",
            "Iteration Number: 2312, Loss: 2.155250, 12 norm of gradients: 0.154610, 12 norm of weights: 1.971798\n",
            "Iteration Number: 2313, Loss: 2.155185, 12 norm of gradients: 0.154384, 12 norm of weights: 1.971765\n",
            "Iteration Number: 2314, Loss: 2.155119, 12 norm of gradients: 0.154159, 12 norm of weights: 1.971732\n",
            "Iteration Number: 2315, Loss: 2.155054, 12 norm of gradients: 0.153934, 12 norm of weights: 1.971699\n",
            "Iteration Number: 2316, Loss: 2.154988, 12 norm of gradients: 0.153709, 12 norm of weights: 1.971667\n",
            "Iteration Number: 2317, Loss: 2.154923, 12 norm of gradients: 0.153485, 12 norm of weights: 1.971634\n",
            "Iteration Number: 2318, Loss: 2.154858, 12 norm of gradients: 0.153262, 12 norm of weights: 1.971602\n",
            "Iteration Number: 2319, Loss: 2.154793, 12 norm of gradients: 0.153038, 12 norm of weights: 1.971569\n",
            "Iteration Number: 2320, Loss: 2.154728, 12 norm of gradients: 0.152815, 12 norm of weights: 1.971537\n",
            "Iteration Number: 2321, Loss: 2.154663, 12 norm of gradients: 0.152593, 12 norm of weights: 1.971505\n",
            "Iteration Number: 2322, Loss: 2.154598, 12 norm of gradients: 0.152371, 12 norm of weights: 1.971472\n",
            "Iteration Number: 2323, Loss: 2.154534, 12 norm of gradients: 0.152149, 12 norm of weights: 1.971440\n",
            "Iteration Number: 2324, Loss: 2.154469, 12 norm of gradients: 0.151928, 12 norm of weights: 1.971408\n",
            "Iteration Number: 2325, Loss: 2.154405, 12 norm of gradients: 0.151707, 12 norm of weights: 1.971376\n",
            "Iteration Number: 2326, Loss: 2.154340, 12 norm of gradients: 0.151486, 12 norm of weights: 1.971344\n",
            "Iteration Number: 2327, Loss: 2.154276, 12 norm of gradients: 0.151266, 12 norm of weights: 1.971312\n",
            "Iteration Number: 2328, Loss: 2.154212, 12 norm of gradients: 0.151046, 12 norm of weights: 1.971280\n",
            "Iteration Number: 2329, Loss: 2.154148, 12 norm of gradients: 0.150827, 12 norm of weights: 1.971248\n",
            "Iteration Number: 2330, Loss: 2.154084, 12 norm of gradients: 0.150608, 12 norm of weights: 1.971216\n",
            "Iteration Number: 2331, Loss: 2.154020, 12 norm of gradients: 0.150389, 12 norm of weights: 1.971184\n",
            "Iteration Number: 2332, Loss: 2.153957, 12 norm of gradients: 0.150171, 12 norm of weights: 1.971153\n",
            "Iteration Number: 2333, Loss: 2.153893, 12 norm of gradients: 0.149953, 12 norm of weights: 1.971121\n",
            "Iteration Number: 2334, Loss: 2.153830, 12 norm of gradients: 0.149736, 12 norm of weights: 1.971089\n",
            "Iteration Number: 2335, Loss: 2.153766, 12 norm of gradients: 0.149519, 12 norm of weights: 1.971058\n",
            "Iteration Number: 2336, Loss: 2.153703, 12 norm of gradients: 0.149302, 12 norm of weights: 1.971026\n",
            "Iteration Number: 2337, Loss: 2.153640, 12 norm of gradients: 0.149086, 12 norm of weights: 1.970995\n",
            "Iteration Number: 2338, Loss: 2.153577, 12 norm of gradients: 0.148870, 12 norm of weights: 1.970963\n",
            "Iteration Number: 2339, Loss: 2.153514, 12 norm of gradients: 0.148654, 12 norm of weights: 1.970932\n",
            "Iteration Number: 2340, Loss: 2.153451, 12 norm of gradients: 0.148439, 12 norm of weights: 1.970901\n",
            "Iteration Number: 2341, Loss: 2.153388, 12 norm of gradients: 0.148225, 12 norm of weights: 1.970869\n",
            "Iteration Number: 2342, Loss: 2.153326, 12 norm of gradients: 0.148010, 12 norm of weights: 1.970838\n",
            "Iteration Number: 2343, Loss: 2.153263, 12 norm of gradients: 0.147796, 12 norm of weights: 1.970807\n",
            "Iteration Number: 2344, Loss: 2.153201, 12 norm of gradients: 0.147583, 12 norm of weights: 1.970776\n",
            "Iteration Number: 2345, Loss: 2.153138, 12 norm of gradients: 0.147369, 12 norm of weights: 1.970745\n",
            "Iteration Number: 2346, Loss: 2.153076, 12 norm of gradients: 0.147157, 12 norm of weights: 1.970714\n",
            "Iteration Number: 2347, Loss: 2.153014, 12 norm of gradients: 0.146944, 12 norm of weights: 1.970683\n",
            "Iteration Number: 2348, Loss: 2.152952, 12 norm of gradients: 0.146732, 12 norm of weights: 1.970652\n",
            "Iteration Number: 2349, Loss: 2.152890, 12 norm of gradients: 0.146520, 12 norm of weights: 1.970621\n",
            "Iteration Number: 2350, Loss: 2.152828, 12 norm of gradients: 0.146309, 12 norm of weights: 1.970590\n",
            "Iteration Number: 2351, Loss: 2.152766, 12 norm of gradients: 0.146098, 12 norm of weights: 1.970559\n",
            "Iteration Number: 2352, Loss: 2.152705, 12 norm of gradients: 0.145887, 12 norm of weights: 1.970529\n",
            "Iteration Number: 2353, Loss: 2.152643, 12 norm of gradients: 0.145677, 12 norm of weights: 1.970498\n",
            "Iteration Number: 2354, Loss: 2.152582, 12 norm of gradients: 0.145467, 12 norm of weights: 1.970467\n",
            "Iteration Number: 2355, Loss: 2.152520, 12 norm of gradients: 0.145258, 12 norm of weights: 1.970437\n",
            "Iteration Number: 2356, Loss: 2.152459, 12 norm of gradients: 0.145049, 12 norm of weights: 1.970406\n",
            "Iteration Number: 2357, Loss: 2.152398, 12 norm of gradients: 0.144840, 12 norm of weights: 1.970376\n",
            "Iteration Number: 2358, Loss: 2.152337, 12 norm of gradients: 0.144631, 12 norm of weights: 1.970345\n",
            "Iteration Number: 2359, Loss: 2.152276, 12 norm of gradients: 0.144423, 12 norm of weights: 1.970315\n",
            "Iteration Number: 2360, Loss: 2.152215, 12 norm of gradients: 0.144216, 12 norm of weights: 1.970285\n",
            "Iteration Number: 2361, Loss: 2.152154, 12 norm of gradients: 0.144009, 12 norm of weights: 1.970254\n",
            "Iteration Number: 2362, Loss: 2.152094, 12 norm of gradients: 0.143802, 12 norm of weights: 1.970224\n",
            "Iteration Number: 2363, Loss: 2.152033, 12 norm of gradients: 0.143595, 12 norm of weights: 1.970194\n",
            "Iteration Number: 2364, Loss: 2.151972, 12 norm of gradients: 0.143389, 12 norm of weights: 1.970164\n",
            "Iteration Number: 2365, Loss: 2.151912, 12 norm of gradients: 0.143183, 12 norm of weights: 1.970134\n",
            "Iteration Number: 2366, Loss: 2.151852, 12 norm of gradients: 0.142978, 12 norm of weights: 1.970104\n",
            "Iteration Number: 2367, Loss: 2.151792, 12 norm of gradients: 0.142772, 12 norm of weights: 1.970074\n",
            "Iteration Number: 2368, Loss: 2.151731, 12 norm of gradients: 0.142568, 12 norm of weights: 1.970044\n",
            "Iteration Number: 2369, Loss: 2.151671, 12 norm of gradients: 0.142363, 12 norm of weights: 1.970014\n",
            "Iteration Number: 2370, Loss: 2.151612, 12 norm of gradients: 0.142159, 12 norm of weights: 1.969984\n",
            "Iteration Number: 2371, Loss: 2.151552, 12 norm of gradients: 0.141956, 12 norm of weights: 1.969954\n",
            "Iteration Number: 2372, Loss: 2.151492, 12 norm of gradients: 0.141752, 12 norm of weights: 1.969925\n",
            "Iteration Number: 2373, Loss: 2.151432, 12 norm of gradients: 0.141549, 12 norm of weights: 1.969895\n",
            "Iteration Number: 2374, Loss: 2.151373, 12 norm of gradients: 0.141347, 12 norm of weights: 1.969865\n",
            "Iteration Number: 2375, Loss: 2.151313, 12 norm of gradients: 0.141145, 12 norm of weights: 1.969836\n",
            "Iteration Number: 2376, Loss: 2.151254, 12 norm of gradients: 0.140943, 12 norm of weights: 1.969806\n",
            "Iteration Number: 2377, Loss: 2.151195, 12 norm of gradients: 0.140741, 12 norm of weights: 1.969776\n",
            "Iteration Number: 2378, Loss: 2.151135, 12 norm of gradients: 0.140540, 12 norm of weights: 1.969747\n",
            "Iteration Number: 2379, Loss: 2.151076, 12 norm of gradients: 0.140339, 12 norm of weights: 1.969718\n",
            "Iteration Number: 2380, Loss: 2.151017, 12 norm of gradients: 0.140139, 12 norm of weights: 1.969688\n",
            "Iteration Number: 2381, Loss: 2.150958, 12 norm of gradients: 0.139939, 12 norm of weights: 1.969659\n",
            "Iteration Number: 2382, Loss: 2.150900, 12 norm of gradients: 0.139739, 12 norm of weights: 1.969630\n",
            "Iteration Number: 2383, Loss: 2.150841, 12 norm of gradients: 0.139539, 12 norm of weights: 1.969600\n",
            "Iteration Number: 2384, Loss: 2.150782, 12 norm of gradients: 0.139340, 12 norm of weights: 1.969571\n",
            "Iteration Number: 2385, Loss: 2.150724, 12 norm of gradients: 0.139142, 12 norm of weights: 1.969542\n",
            "Iteration Number: 2386, Loss: 2.150665, 12 norm of gradients: 0.138943, 12 norm of weights: 1.969513\n",
            "Iteration Number: 2387, Loss: 2.150607, 12 norm of gradients: 0.138745, 12 norm of weights: 1.969484\n",
            "Iteration Number: 2388, Loss: 2.150548, 12 norm of gradients: 0.138548, 12 norm of weights: 1.969455\n",
            "Iteration Number: 2389, Loss: 2.150490, 12 norm of gradients: 0.138350, 12 norm of weights: 1.969426\n",
            "Iteration Number: 2390, Loss: 2.150432, 12 norm of gradients: 0.138153, 12 norm of weights: 1.969397\n",
            "Iteration Number: 2391, Loss: 2.150374, 12 norm of gradients: 0.137957, 12 norm of weights: 1.969368\n",
            "Iteration Number: 2392, Loss: 2.150316, 12 norm of gradients: 0.137760, 12 norm of weights: 1.969339\n",
            "Iteration Number: 2393, Loss: 2.150258, 12 norm of gradients: 0.137564, 12 norm of weights: 1.969310\n",
            "Iteration Number: 2394, Loss: 2.150201, 12 norm of gradients: 0.137369, 12 norm of weights: 1.969282\n",
            "Iteration Number: 2395, Loss: 2.150143, 12 norm of gradients: 0.137174, 12 norm of weights: 1.969253\n",
            "Iteration Number: 2396, Loss: 2.150085, 12 norm of gradients: 0.136979, 12 norm of weights: 1.969224\n",
            "Iteration Number: 2397, Loss: 2.150028, 12 norm of gradients: 0.136784, 12 norm of weights: 1.969196\n",
            "Iteration Number: 2398, Loss: 2.149970, 12 norm of gradients: 0.136590, 12 norm of weights: 1.969167\n",
            "Iteration Number: 2399, Loss: 2.149913, 12 norm of gradients: 0.136396, 12 norm of weights: 1.969138\n",
            "Iteration Number: 2400, Loss: 2.149856, 12 norm of gradients: 0.136202, 12 norm of weights: 1.969110\n",
            "Iteration Number: 2401, Loss: 2.149799, 12 norm of gradients: 0.136009, 12 norm of weights: 1.969082\n",
            "Iteration Number: 2402, Loss: 2.149741, 12 norm of gradients: 0.135816, 12 norm of weights: 1.969053\n",
            "Iteration Number: 2403, Loss: 2.149684, 12 norm of gradients: 0.135624, 12 norm of weights: 1.969025\n",
            "Iteration Number: 2404, Loss: 2.149628, 12 norm of gradients: 0.135431, 12 norm of weights: 1.968996\n",
            "Iteration Number: 2405, Loss: 2.149571, 12 norm of gradients: 0.135240, 12 norm of weights: 1.968968\n",
            "Iteration Number: 2406, Loss: 2.149514, 12 norm of gradients: 0.135048, 12 norm of weights: 1.968940\n",
            "Iteration Number: 2407, Loss: 2.149457, 12 norm of gradients: 0.134857, 12 norm of weights: 1.968912\n",
            "Iteration Number: 2408, Loss: 2.149401, 12 norm of gradients: 0.134666, 12 norm of weights: 1.968884\n",
            "Iteration Number: 2409, Loss: 2.149344, 12 norm of gradients: 0.134475, 12 norm of weights: 1.968855\n",
            "Iteration Number: 2410, Loss: 2.149288, 12 norm of gradients: 0.134285, 12 norm of weights: 1.968827\n",
            "Iteration Number: 2411, Loss: 2.149232, 12 norm of gradients: 0.134095, 12 norm of weights: 1.968799\n",
            "Iteration Number: 2412, Loss: 2.149175, 12 norm of gradients: 0.133906, 12 norm of weights: 1.968771\n",
            "Iteration Number: 2413, Loss: 2.149119, 12 norm of gradients: 0.133717, 12 norm of weights: 1.968743\n",
            "Iteration Number: 2414, Loss: 2.149063, 12 norm of gradients: 0.133528, 12 norm of weights: 1.968716\n",
            "Iteration Number: 2415, Loss: 2.149007, 12 norm of gradients: 0.133339, 12 norm of weights: 1.968688\n",
            "Iteration Number: 2416, Loss: 2.148951, 12 norm of gradients: 0.133151, 12 norm of weights: 1.968660\n",
            "Iteration Number: 2417, Loss: 2.148895, 12 norm of gradients: 0.132963, 12 norm of weights: 1.968632\n",
            "Iteration Number: 2418, Loss: 2.148840, 12 norm of gradients: 0.132775, 12 norm of weights: 1.968604\n",
            "Iteration Number: 2419, Loss: 2.148784, 12 norm of gradients: 0.132588, 12 norm of weights: 1.968577\n",
            "Iteration Number: 2420, Loss: 2.148728, 12 norm of gradients: 0.132401, 12 norm of weights: 1.968549\n",
            "Iteration Number: 2421, Loss: 2.148673, 12 norm of gradients: 0.132215, 12 norm of weights: 1.968521\n",
            "Iteration Number: 2422, Loss: 2.148617, 12 norm of gradients: 0.132028, 12 norm of weights: 1.968494\n",
            "Iteration Number: 2423, Loss: 2.148562, 12 norm of gradients: 0.131842, 12 norm of weights: 1.968466\n",
            "Iteration Number: 2424, Loss: 2.148507, 12 norm of gradients: 0.131657, 12 norm of weights: 1.968439\n",
            "Iteration Number: 2425, Loss: 2.148452, 12 norm of gradients: 0.131471, 12 norm of weights: 1.968411\n",
            "Iteration Number: 2426, Loss: 2.148396, 12 norm of gradients: 0.131286, 12 norm of weights: 1.968384\n",
            "Iteration Number: 2427, Loss: 2.148341, 12 norm of gradients: 0.131102, 12 norm of weights: 1.968357\n",
            "Iteration Number: 2428, Loss: 2.148286, 12 norm of gradients: 0.130917, 12 norm of weights: 1.968329\n",
            "Iteration Number: 2429, Loss: 2.148232, 12 norm of gradients: 0.130733, 12 norm of weights: 1.968302\n",
            "Iteration Number: 2430, Loss: 2.148177, 12 norm of gradients: 0.130550, 12 norm of weights: 1.968275\n",
            "Iteration Number: 2431, Loss: 2.148122, 12 norm of gradients: 0.130366, 12 norm of weights: 1.968248\n",
            "Iteration Number: 2432, Loss: 2.148067, 12 norm of gradients: 0.130183, 12 norm of weights: 1.968220\n",
            "Iteration Number: 2433, Loss: 2.148013, 12 norm of gradients: 0.130000, 12 norm of weights: 1.968193\n",
            "Iteration Number: 2434, Loss: 2.147958, 12 norm of gradients: 0.129818, 12 norm of weights: 1.968166\n",
            "Iteration Number: 2435, Loss: 2.147904, 12 norm of gradients: 0.129636, 12 norm of weights: 1.968139\n",
            "Iteration Number: 2436, Loss: 2.147850, 12 norm of gradients: 0.129454, 12 norm of weights: 1.968112\n",
            "Iteration Number: 2437, Loss: 2.147795, 12 norm of gradients: 0.129273, 12 norm of weights: 1.968085\n",
            "Iteration Number: 2438, Loss: 2.147741, 12 norm of gradients: 0.129091, 12 norm of weights: 1.968058\n",
            "Iteration Number: 2439, Loss: 2.147687, 12 norm of gradients: 0.128910, 12 norm of weights: 1.968031\n",
            "Iteration Number: 2440, Loss: 2.147633, 12 norm of gradients: 0.128730, 12 norm of weights: 1.968004\n",
            "Iteration Number: 2441, Loss: 2.147579, 12 norm of gradients: 0.128550, 12 norm of weights: 1.967978\n",
            "Iteration Number: 2442, Loss: 2.147525, 12 norm of gradients: 0.128370, 12 norm of weights: 1.967951\n",
            "Iteration Number: 2443, Loss: 2.147471, 12 norm of gradients: 0.128190, 12 norm of weights: 1.967924\n",
            "Iteration Number: 2444, Loss: 2.147418, 12 norm of gradients: 0.128011, 12 norm of weights: 1.967897\n",
            "Iteration Number: 2445, Loss: 2.147364, 12 norm of gradients: 0.127832, 12 norm of weights: 1.967871\n",
            "Iteration Number: 2446, Loss: 2.147311, 12 norm of gradients: 0.127653, 12 norm of weights: 1.967844\n",
            "Iteration Number: 2447, Loss: 2.147257, 12 norm of gradients: 0.127475, 12 norm of weights: 1.967818\n",
            "Iteration Number: 2448, Loss: 2.147204, 12 norm of gradients: 0.127297, 12 norm of weights: 1.967791\n",
            "Iteration Number: 2449, Loss: 2.147150, 12 norm of gradients: 0.127119, 12 norm of weights: 1.967765\n",
            "Iteration Number: 2450, Loss: 2.147097, 12 norm of gradients: 0.126941, 12 norm of weights: 1.967738\n",
            "Iteration Number: 2451, Loss: 2.147044, 12 norm of gradients: 0.126764, 12 norm of weights: 1.967712\n",
            "Iteration Number: 2452, Loss: 2.146991, 12 norm of gradients: 0.126587, 12 norm of weights: 1.967685\n",
            "Iteration Number: 2453, Loss: 2.146938, 12 norm of gradients: 0.126411, 12 norm of weights: 1.967659\n",
            "Iteration Number: 2454, Loss: 2.146885, 12 norm of gradients: 0.126235, 12 norm of weights: 1.967632\n",
            "Iteration Number: 2455, Loss: 2.146832, 12 norm of gradients: 0.126059, 12 norm of weights: 1.967606\n",
            "Iteration Number: 2456, Loss: 2.146779, 12 norm of gradients: 0.125883, 12 norm of weights: 1.967580\n",
            "Iteration Number: 2457, Loss: 2.146726, 12 norm of gradients: 0.125708, 12 norm of weights: 1.967554\n",
            "Iteration Number: 2458, Loss: 2.146674, 12 norm of gradients: 0.125533, 12 norm of weights: 1.967528\n",
            "Iteration Number: 2459, Loss: 2.146621, 12 norm of gradients: 0.125358, 12 norm of weights: 1.967501\n",
            "Iteration Number: 2460, Loss: 2.146568, 12 norm of gradients: 0.125184, 12 norm of weights: 1.967475\n",
            "Iteration Number: 2461, Loss: 2.146516, 12 norm of gradients: 0.125009, 12 norm of weights: 1.967449\n",
            "Iteration Number: 2462, Loss: 2.146464, 12 norm of gradients: 0.124836, 12 norm of weights: 1.967423\n",
            "Iteration Number: 2463, Loss: 2.146411, 12 norm of gradients: 0.124662, 12 norm of weights: 1.967397\n",
            "Iteration Number: 2464, Loss: 2.146359, 12 norm of gradients: 0.124489, 12 norm of weights: 1.967371\n",
            "Iteration Number: 2465, Loss: 2.146307, 12 norm of gradients: 0.124316, 12 norm of weights: 1.967345\n",
            "Iteration Number: 2466, Loss: 2.146255, 12 norm of gradients: 0.124143, 12 norm of weights: 1.967319\n",
            "Iteration Number: 2467, Loss: 2.146203, 12 norm of gradients: 0.123971, 12 norm of weights: 1.967294\n",
            "Iteration Number: 2468, Loss: 2.146151, 12 norm of gradients: 0.123799, 12 norm of weights: 1.967268\n",
            "Iteration Number: 2469, Loss: 2.146099, 12 norm of gradients: 0.123627, 12 norm of weights: 1.967242\n",
            "Iteration Number: 2470, Loss: 2.146047, 12 norm of gradients: 0.123456, 12 norm of weights: 1.967216\n",
            "Iteration Number: 2471, Loss: 2.145995, 12 norm of gradients: 0.123285, 12 norm of weights: 1.967191\n",
            "Iteration Number: 2472, Loss: 2.145943, 12 norm of gradients: 0.123114, 12 norm of weights: 1.967165\n",
            "Iteration Number: 2473, Loss: 2.145892, 12 norm of gradients: 0.122943, 12 norm of weights: 1.967139\n",
            "Iteration Number: 2474, Loss: 2.145840, 12 norm of gradients: 0.122773, 12 norm of weights: 1.967114\n",
            "Iteration Number: 2475, Loss: 2.145789, 12 norm of gradients: 0.122603, 12 norm of weights: 1.967088\n",
            "Iteration Number: 2476, Loss: 2.145737, 12 norm of gradients: 0.122433, 12 norm of weights: 1.967063\n",
            "Iteration Number: 2477, Loss: 2.145686, 12 norm of gradients: 0.122264, 12 norm of weights: 1.967037\n",
            "Iteration Number: 2478, Loss: 2.145635, 12 norm of gradients: 0.122095, 12 norm of weights: 1.967012\n",
            "Iteration Number: 2479, Loss: 2.145584, 12 norm of gradients: 0.121926, 12 norm of weights: 1.966986\n",
            "Iteration Number: 2480, Loss: 2.145532, 12 norm of gradients: 0.121757, 12 norm of weights: 1.966961\n",
            "Iteration Number: 2481, Loss: 2.145481, 12 norm of gradients: 0.121589, 12 norm of weights: 1.966935\n",
            "Iteration Number: 2482, Loss: 2.145430, 12 norm of gradients: 0.121421, 12 norm of weights: 1.966910\n",
            "Iteration Number: 2483, Loss: 2.145379, 12 norm of gradients: 0.121253, 12 norm of weights: 1.966885\n",
            "Iteration Number: 2484, Loss: 2.145329, 12 norm of gradients: 0.121086, 12 norm of weights: 1.966860\n",
            "Iteration Number: 2485, Loss: 2.145278, 12 norm of gradients: 0.120919, 12 norm of weights: 1.966834\n",
            "Iteration Number: 2486, Loss: 2.145227, 12 norm of gradients: 0.120752, 12 norm of weights: 1.966809\n",
            "Iteration Number: 2487, Loss: 2.145176, 12 norm of gradients: 0.120586, 12 norm of weights: 1.966784\n",
            "Iteration Number: 2488, Loss: 2.145126, 12 norm of gradients: 0.120419, 12 norm of weights: 1.966759\n",
            "Iteration Number: 2489, Loss: 2.145075, 12 norm of gradients: 0.120253, 12 norm of weights: 1.966734\n",
            "Iteration Number: 2490, Loss: 2.145025, 12 norm of gradients: 0.120088, 12 norm of weights: 1.966709\n",
            "Iteration Number: 2491, Loss: 2.144974, 12 norm of gradients: 0.119922, 12 norm of weights: 1.966684\n",
            "Iteration Number: 2492, Loss: 2.144924, 12 norm of gradients: 0.119757, 12 norm of weights: 1.966659\n",
            "Iteration Number: 2493, Loss: 2.144874, 12 norm of gradients: 0.119593, 12 norm of weights: 1.966634\n",
            "Iteration Number: 2494, Loss: 2.144824, 12 norm of gradients: 0.119428, 12 norm of weights: 1.966609\n",
            "Iteration Number: 2495, Loss: 2.144773, 12 norm of gradients: 0.119264, 12 norm of weights: 1.966584\n",
            "Iteration Number: 2496, Loss: 2.144723, 12 norm of gradients: 0.119100, 12 norm of weights: 1.966559\n",
            "Iteration Number: 2497, Loss: 2.144673, 12 norm of gradients: 0.118936, 12 norm of weights: 1.966534\n",
            "Iteration Number: 2498, Loss: 2.144623, 12 norm of gradients: 0.118773, 12 norm of weights: 1.966509\n",
            "Iteration Number: 2499, Loss: 2.144573, 12 norm of gradients: 0.118609, 12 norm of weights: 1.966485\n",
            "Iteration Number: 2500, Loss: 2.144524, 12 norm of gradients: 0.118447, 12 norm of weights: 1.966460\n",
            "Iteration Number: 2501, Loss: 2.144474, 12 norm of gradients: 0.118284, 12 norm of weights: 1.966435\n",
            "Iteration Number: 2502, Loss: 2.144424, 12 norm of gradients: 0.118122, 12 norm of weights: 1.966411\n",
            "Iteration Number: 2503, Loss: 2.144375, 12 norm of gradients: 0.117960, 12 norm of weights: 1.966386\n",
            "Iteration Number: 2504, Loss: 2.144325, 12 norm of gradients: 0.117798, 12 norm of weights: 1.966361\n",
            "Iteration Number: 2505, Loss: 2.144276, 12 norm of gradients: 0.117636, 12 norm of weights: 1.966337\n",
            "Iteration Number: 2506, Loss: 2.144226, 12 norm of gradients: 0.117475, 12 norm of weights: 1.966312\n",
            "Iteration Number: 2507, Loss: 2.144177, 12 norm of gradients: 0.117314, 12 norm of weights: 1.966288\n",
            "Iteration Number: 2508, Loss: 2.144127, 12 norm of gradients: 0.117154, 12 norm of weights: 1.966263\n",
            "Iteration Number: 2509, Loss: 2.144078, 12 norm of gradients: 0.116993, 12 norm of weights: 1.966239\n",
            "Iteration Number: 2510, Loss: 2.144029, 12 norm of gradients: 0.116833, 12 norm of weights: 1.966214\n",
            "Iteration Number: 2511, Loss: 2.143980, 12 norm of gradients: 0.116673, 12 norm of weights: 1.966190\n",
            "Iteration Number: 2512, Loss: 2.143931, 12 norm of gradients: 0.116514, 12 norm of weights: 1.966166\n",
            "Iteration Number: 2513, Loss: 2.143882, 12 norm of gradients: 0.116355, 12 norm of weights: 1.966141\n",
            "Iteration Number: 2514, Loss: 2.143833, 12 norm of gradients: 0.116196, 12 norm of weights: 1.966117\n",
            "Iteration Number: 2515, Loss: 2.143784, 12 norm of gradients: 0.116037, 12 norm of weights: 1.966093\n",
            "Iteration Number: 2516, Loss: 2.143735, 12 norm of gradients: 0.115878, 12 norm of weights: 1.966069\n",
            "Iteration Number: 2517, Loss: 2.143687, 12 norm of gradients: 0.115720, 12 norm of weights: 1.966045\n",
            "Iteration Number: 2518, Loss: 2.143638, 12 norm of gradients: 0.115562, 12 norm of weights: 1.966020\n",
            "Iteration Number: 2519, Loss: 2.143589, 12 norm of gradients: 0.115405, 12 norm of weights: 1.965996\n",
            "Iteration Number: 2520, Loss: 2.143541, 12 norm of gradients: 0.115247, 12 norm of weights: 1.965972\n",
            "Iteration Number: 2521, Loss: 2.143492, 12 norm of gradients: 0.115090, 12 norm of weights: 1.965948\n",
            "Iteration Number: 2522, Loss: 2.143444, 12 norm of gradients: 0.114933, 12 norm of weights: 1.965924\n",
            "Iteration Number: 2523, Loss: 2.143395, 12 norm of gradients: 0.114777, 12 norm of weights: 1.965900\n",
            "Iteration Number: 2524, Loss: 2.143347, 12 norm of gradients: 0.114620, 12 norm of weights: 1.965876\n",
            "Iteration Number: 2525, Loss: 2.143299, 12 norm of gradients: 0.114464, 12 norm of weights: 1.965852\n",
            "Iteration Number: 2526, Loss: 2.143251, 12 norm of gradients: 0.114308, 12 norm of weights: 1.965828\n",
            "Iteration Number: 2527, Loss: 2.143202, 12 norm of gradients: 0.114153, 12 norm of weights: 1.965804\n",
            "Iteration Number: 2528, Loss: 2.143154, 12 norm of gradients: 0.113998, 12 norm of weights: 1.965781\n",
            "Iteration Number: 2529, Loss: 2.143106, 12 norm of gradients: 0.113843, 12 norm of weights: 1.965757\n",
            "Iteration Number: 2530, Loss: 2.143058, 12 norm of gradients: 0.113688, 12 norm of weights: 1.965733\n",
            "Iteration Number: 2531, Loss: 2.143010, 12 norm of gradients: 0.113533, 12 norm of weights: 1.965709\n",
            "Iteration Number: 2532, Loss: 2.142963, 12 norm of gradients: 0.113379, 12 norm of weights: 1.965685\n",
            "Iteration Number: 2533, Loss: 2.142915, 12 norm of gradients: 0.113225, 12 norm of weights: 1.965662\n",
            "Iteration Number: 2534, Loss: 2.142867, 12 norm of gradients: 0.113071, 12 norm of weights: 1.965638\n",
            "Iteration Number: 2535, Loss: 2.142819, 12 norm of gradients: 0.112918, 12 norm of weights: 1.965614\n",
            "Iteration Number: 2536, Loss: 2.142772, 12 norm of gradients: 0.112765, 12 norm of weights: 1.965591\n",
            "Iteration Number: 2537, Loss: 2.142724, 12 norm of gradients: 0.112612, 12 norm of weights: 1.965567\n",
            "Iteration Number: 2538, Loss: 2.142677, 12 norm of gradients: 0.112459, 12 norm of weights: 1.965544\n",
            "Iteration Number: 2539, Loss: 2.142629, 12 norm of gradients: 0.112307, 12 norm of weights: 1.965520\n",
            "Iteration Number: 2540, Loss: 2.142582, 12 norm of gradients: 0.112155, 12 norm of weights: 1.965497\n",
            "Iteration Number: 2541, Loss: 2.142535, 12 norm of gradients: 0.112003, 12 norm of weights: 1.965473\n",
            "Iteration Number: 2542, Loss: 2.142487, 12 norm of gradients: 0.111851, 12 norm of weights: 1.965450\n",
            "Iteration Number: 2543, Loss: 2.142440, 12 norm of gradients: 0.111700, 12 norm of weights: 1.965426\n",
            "Iteration Number: 2544, Loss: 2.142393, 12 norm of gradients: 0.111549, 12 norm of weights: 1.965403\n",
            "Iteration Number: 2545, Loss: 2.142346, 12 norm of gradients: 0.111398, 12 norm of weights: 1.965380\n",
            "Iteration Number: 2546, Loss: 2.142299, 12 norm of gradients: 0.111247, 12 norm of weights: 1.965356\n",
            "Iteration Number: 2547, Loss: 2.142252, 12 norm of gradients: 0.111097, 12 norm of weights: 1.965333\n",
            "Iteration Number: 2548, Loss: 2.142205, 12 norm of gradients: 0.110947, 12 norm of weights: 1.965310\n",
            "Iteration Number: 2549, Loss: 2.142158, 12 norm of gradients: 0.110797, 12 norm of weights: 1.965287\n",
            "Iteration Number: 2550, Loss: 2.142111, 12 norm of gradients: 0.110647, 12 norm of weights: 1.965263\n",
            "Iteration Number: 2551, Loss: 2.142065, 12 norm of gradients: 0.110498, 12 norm of weights: 1.965240\n",
            "Iteration Number: 2552, Loss: 2.142018, 12 norm of gradients: 0.110349, 12 norm of weights: 1.965217\n",
            "Iteration Number: 2553, Loss: 2.141971, 12 norm of gradients: 0.110200, 12 norm of weights: 1.965194\n",
            "Iteration Number: 2554, Loss: 2.141925, 12 norm of gradients: 0.110051, 12 norm of weights: 1.965171\n",
            "Iteration Number: 2555, Loss: 2.141878, 12 norm of gradients: 0.109903, 12 norm of weights: 1.965148\n",
            "Iteration Number: 2556, Loss: 2.141832, 12 norm of gradients: 0.109755, 12 norm of weights: 1.965125\n",
            "Iteration Number: 2557, Loss: 2.141785, 12 norm of gradients: 0.109607, 12 norm of weights: 1.965102\n",
            "Iteration Number: 2558, Loss: 2.141739, 12 norm of gradients: 0.109460, 12 norm of weights: 1.965079\n",
            "Iteration Number: 2559, Loss: 2.141692, 12 norm of gradients: 0.109312, 12 norm of weights: 1.965056\n",
            "Iteration Number: 2560, Loss: 2.141646, 12 norm of gradients: 0.109165, 12 norm of weights: 1.965033\n",
            "Iteration Number: 2561, Loss: 2.141600, 12 norm of gradients: 0.109018, 12 norm of weights: 1.965010\n",
            "Iteration Number: 2562, Loss: 2.141554, 12 norm of gradients: 0.108872, 12 norm of weights: 1.964987\n",
            "Iteration Number: 2563, Loss: 2.141508, 12 norm of gradients: 0.108725, 12 norm of weights: 1.964964\n",
            "Iteration Number: 2564, Loss: 2.141462, 12 norm of gradients: 0.108579, 12 norm of weights: 1.964942\n",
            "Iteration Number: 2565, Loss: 2.141416, 12 norm of gradients: 0.108434, 12 norm of weights: 1.964919\n",
            "Iteration Number: 2566, Loss: 2.141370, 12 norm of gradients: 0.108288, 12 norm of weights: 1.964896\n",
            "Iteration Number: 2567, Loss: 2.141324, 12 norm of gradients: 0.108143, 12 norm of weights: 1.964873\n",
            "Iteration Number: 2568, Loss: 2.141278, 12 norm of gradients: 0.107997, 12 norm of weights: 1.964851\n",
            "Iteration Number: 2569, Loss: 2.141232, 12 norm of gradients: 0.107853, 12 norm of weights: 1.964828\n",
            "Iteration Number: 2570, Loss: 2.141187, 12 norm of gradients: 0.107708, 12 norm of weights: 1.964805\n",
            "Iteration Number: 2571, Loss: 2.141141, 12 norm of gradients: 0.107564, 12 norm of weights: 1.964783\n",
            "Iteration Number: 2572, Loss: 2.141095, 12 norm of gradients: 0.107419, 12 norm of weights: 1.964760\n",
            "Iteration Number: 2573, Loss: 2.141050, 12 norm of gradients: 0.107276, 12 norm of weights: 1.964737\n",
            "Iteration Number: 2574, Loss: 2.141004, 12 norm of gradients: 0.107132, 12 norm of weights: 1.964715\n",
            "Iteration Number: 2575, Loss: 2.140959, 12 norm of gradients: 0.106988, 12 norm of weights: 1.964692\n",
            "Iteration Number: 2576, Loss: 2.140913, 12 norm of gradients: 0.106845, 12 norm of weights: 1.964670\n",
            "Iteration Number: 2577, Loss: 2.140868, 12 norm of gradients: 0.106702, 12 norm of weights: 1.964647\n",
            "Iteration Number: 2578, Loss: 2.140823, 12 norm of gradients: 0.106560, 12 norm of weights: 1.964625\n",
            "Iteration Number: 2579, Loss: 2.140778, 12 norm of gradients: 0.106417, 12 norm of weights: 1.964603\n",
            "Iteration Number: 2580, Loss: 2.140732, 12 norm of gradients: 0.106275, 12 norm of weights: 1.964580\n",
            "Iteration Number: 2581, Loss: 2.140687, 12 norm of gradients: 0.106133, 12 norm of weights: 1.964558\n",
            "Iteration Number: 2582, Loss: 2.140642, 12 norm of gradients: 0.105991, 12 norm of weights: 1.964536\n",
            "Iteration Number: 2583, Loss: 2.140597, 12 norm of gradients: 0.105850, 12 norm of weights: 1.964513\n",
            "Iteration Number: 2584, Loss: 2.140552, 12 norm of gradients: 0.105709, 12 norm of weights: 1.964491\n",
            "Iteration Number: 2585, Loss: 2.140507, 12 norm of gradients: 0.105568, 12 norm of weights: 1.964469\n",
            "Iteration Number: 2586, Loss: 2.140462, 12 norm of gradients: 0.105427, 12 norm of weights: 1.964446\n",
            "Iteration Number: 2587, Loss: 2.140417, 12 norm of gradients: 0.105286, 12 norm of weights: 1.964424\n",
            "Iteration Number: 2588, Loss: 2.140373, 12 norm of gradients: 0.105146, 12 norm of weights: 1.964402\n",
            "Iteration Number: 2589, Loss: 2.140328, 12 norm of gradients: 0.105006, 12 norm of weights: 1.964380\n",
            "Iteration Number: 2590, Loss: 2.140283, 12 norm of gradients: 0.104866, 12 norm of weights: 1.964358\n",
            "Iteration Number: 2591, Loss: 2.140238, 12 norm of gradients: 0.104726, 12 norm of weights: 1.964336\n",
            "Iteration Number: 2592, Loss: 2.140194, 12 norm of gradients: 0.104587, 12 norm of weights: 1.964314\n",
            "Iteration Number: 2593, Loss: 2.140149, 12 norm of gradients: 0.104448, 12 norm of weights: 1.964292\n",
            "Iteration Number: 2594, Loss: 2.140105, 12 norm of gradients: 0.104309, 12 norm of weights: 1.964270\n",
            "Iteration Number: 2595, Loss: 2.140060, 12 norm of gradients: 0.104170, 12 norm of weights: 1.964248\n",
            "Iteration Number: 2596, Loss: 2.140016, 12 norm of gradients: 0.104032, 12 norm of weights: 1.964226\n",
            "Iteration Number: 2597, Loss: 2.139972, 12 norm of gradients: 0.103894, 12 norm of weights: 1.964204\n",
            "Iteration Number: 2598, Loss: 2.139927, 12 norm of gradients: 0.103756, 12 norm of weights: 1.964182\n",
            "Iteration Number: 2599, Loss: 2.139883, 12 norm of gradients: 0.103618, 12 norm of weights: 1.964160\n",
            "Iteration Number: 2600, Loss: 2.139839, 12 norm of gradients: 0.103480, 12 norm of weights: 1.964138\n",
            "Iteration Number: 2601, Loss: 2.139795, 12 norm of gradients: 0.103343, 12 norm of weights: 1.964116\n",
            "Iteration Number: 2602, Loss: 2.139751, 12 norm of gradients: 0.103206, 12 norm of weights: 1.964094\n",
            "Iteration Number: 2603, Loss: 2.139707, 12 norm of gradients: 0.103069, 12 norm of weights: 1.964072\n",
            "Iteration Number: 2604, Loss: 2.139663, 12 norm of gradients: 0.102933, 12 norm of weights: 1.964051\n",
            "Iteration Number: 2605, Loss: 2.139619, 12 norm of gradients: 0.102796, 12 norm of weights: 1.964029\n",
            "Iteration Number: 2606, Loss: 2.139575, 12 norm of gradients: 0.102660, 12 norm of weights: 1.964007\n",
            "Iteration Number: 2607, Loss: 2.139531, 12 norm of gradients: 0.102524, 12 norm of weights: 1.963986\n",
            "Iteration Number: 2608, Loss: 2.139487, 12 norm of gradients: 0.102389, 12 norm of weights: 1.963964\n",
            "Iteration Number: 2609, Loss: 2.139443, 12 norm of gradients: 0.102253, 12 norm of weights: 1.963942\n",
            "Iteration Number: 2610, Loss: 2.139400, 12 norm of gradients: 0.102118, 12 norm of weights: 1.963921\n",
            "Iteration Number: 2611, Loss: 2.139356, 12 norm of gradients: 0.101983, 12 norm of weights: 1.963899\n",
            "Iteration Number: 2612, Loss: 2.139312, 12 norm of gradients: 0.101848, 12 norm of weights: 1.963877\n",
            "Iteration Number: 2613, Loss: 2.139269, 12 norm of gradients: 0.101713, 12 norm of weights: 1.963856\n",
            "Iteration Number: 2614, Loss: 2.139225, 12 norm of gradients: 0.101579, 12 norm of weights: 1.963834\n",
            "Iteration Number: 2615, Loss: 2.139182, 12 norm of gradients: 0.101445, 12 norm of weights: 1.963813\n",
            "Iteration Number: 2616, Loss: 2.139138, 12 norm of gradients: 0.101311, 12 norm of weights: 1.963791\n",
            "Iteration Number: 2617, Loss: 2.139095, 12 norm of gradients: 0.101178, 12 norm of weights: 1.963770\n",
            "Iteration Number: 2618, Loss: 2.139052, 12 norm of gradients: 0.101044, 12 norm of weights: 1.963748\n",
            "Iteration Number: 2619, Loss: 2.139008, 12 norm of gradients: 0.100911, 12 norm of weights: 1.963727\n",
            "Iteration Number: 2620, Loss: 2.138965, 12 norm of gradients: 0.100778, 12 norm of weights: 1.963706\n",
            "Iteration Number: 2621, Loss: 2.138922, 12 norm of gradients: 0.100645, 12 norm of weights: 1.963684\n",
            "Iteration Number: 2622, Loss: 2.138879, 12 norm of gradients: 0.100513, 12 norm of weights: 1.963663\n",
            "Iteration Number: 2623, Loss: 2.138836, 12 norm of gradients: 0.100380, 12 norm of weights: 1.963642\n",
            "Iteration Number: 2624, Loss: 2.138793, 12 norm of gradients: 0.100248, 12 norm of weights: 1.963620\n",
            "Iteration Number: 2625, Loss: 2.138750, 12 norm of gradients: 0.100116, 12 norm of weights: 1.963599\n",
            "Iteration Number: 2626, Loss: 2.138707, 12 norm of gradients: 0.099985, 12 norm of weights: 1.963578\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "lambda_ = 1.0\n",
        "\n",
        "model = fit(xtrain_normal, ytrain, learning_rate, lambda_, 10000, verbose=1) #keep the verbose on here for your submissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "-AiarpzOhIvE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6761ab5c-c71d-4422-bea4-dc5a87fc3d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy:  0.9192307692307692\n"
          ]
        }
      ],
      "source": [
        "print(\"Train accuracy: \", accuracy(xtrain_normal, ytrain, model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "nrU6Tr7mhIvE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3c4998f1-2364-46ac-9d8c-c4a7544f0bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01 5 0.8621378621378621\n",
            "0.01 2 0.9110889110889111\n",
            "0.01 1 0.929070929070929\n",
            "0.01 0.1 0.9240759240759241\n",
            "0.01 0.01 0.9280719280719281\n",
            "0.001 5 0.9090909090909091\n",
            "0.001 2 0.919080919080919\n",
            "0.001 1 0.9140859140859141\n",
            "0.001 0.1 0.9330669330669331\n",
            "0.001 0.01 0.9240759240759241\n",
            "0.0001 5 0.9110889110889111\n",
            "0.0001 2 0.913086913086913\n",
            "0.0001 1 0.9240759240759241\n",
            "0.0001 0.1 0.919080919080919\n",
            "0.0001 0.01 0.9070929070929071\n",
            "1e-05 5 0.8161838161838162\n",
            "1e-05 2 0.7452547452547452\n",
            "1e-05 1 0.7442557442557443\n",
            "1e-05 0.1 0.7902097902097902\n",
            "1e-05 0.01 0.7572427572427572\n"
          ]
        }
      ],
      "source": [
        "#grid search for finding the best hyperparams and model\n",
        "\n",
        "best_model = None\n",
        "best_val = -1\n",
        "for lr in [0.01, 0.001, 0.0001, 0.00001]:\n",
        "    for la in [5, 2, 1, 0.1, 0.01]:\n",
        "        model = fit(xtrain_normal, ytrain, lr, la, 10000, verbose=0)\n",
        "        val_acc = accuracy(xval_normal, yval, model)\n",
        "        print(lr, la, val_acc)\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_model = model\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "n5zNZCNVhIvE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "50e716fc-c361-47a8-8148-77843636b0b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy:  0.941\n"
          ]
        }
      ],
      "source": [
        "print(\"Test accuracy: \", accuracy(xtest_normal, ytest, best_model))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}